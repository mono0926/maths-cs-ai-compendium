# 確率の基本概念 (Probability Concepts)

- 確率は、ある事象（イベント）に対して 0 から 1 の間の数値を割り当て、それがどれくらい起こりやすいかを測定します。

- 確率が 0 なら不可能、1 なら確実、0.5 ならコイン投げの表裏と同じです。

- 確率には主に2つの解釈があります。**頻度主義 (frequentist)** の見方では、確率は長期間の相対的な頻度であると考えます。例えば、公平なコインを 10,000 回投げれば、表が出る回数はほぼ 50% になるという考え方です。

- **ベイズ主義 (Bayesian)** の見方では、確率は「確信の度合い (degree of belief)」であると考えます。明日が一度しか来ないとしても、「明日雨が降る確率は 70% だ」と言うことができます。

- どちらの解釈も、数学的には同じルールを使用します。違いは哲学的なものですが、機械学習 (ML) においては重要です。頻度主義的な手法では点推定が得られますが、ベイズ主義的な手法ではパラメータに対する完全な分布が得られます。

- **標本空間 (sample space)** $S$ は、ある試行のすべての起こりうる結果の集合です。コイン投げなら $S = \{H, T\}$、サイコロ投げなら $S = \{1, 2, 3, 4, 5, 6\}$ です。

- **事象 (event)** は、標本空間の任意の部分集合です。「偶数が出る」という事象は $A = \{2, 4, 6\}$ となり、これは $S$ の部分集合です。

- すべての結果が同様に確からしい場合、ある事象の確率は単純な数え上げ（ファイル 01）で求められます：

$$P(A) = \frac{|A|}{|S|} = \frac{\text{該当する結果の数}}{\text{すべての結果の数}}$$

- 偶数の例：$P(\text{even}) = \frac{3}{6} = 0.5$。

![標本空間 S 内の事象 A と B、その交差と補集合を示すベン図](../../images/venn_diagram.svg)

- 事象 $A$ の **補集合（余事象）** は $A'$ または $A^c$ と書かれ、$S$ の中で $A$ に含まれないすべての要素を指します。すべての結果は $A$ に含まれるか含まれないかのどちらかであるため：

$$P(A') = 1 - P(A)$$

- 余事象を利用したほうが簡単な場合もしばしばあります。例えば「コインを5回投げて少なくとも1回表が出る」確率を計算する場合、表が出るすべてのパターンを数える代わりに、「一度も表が出ない（すべて裏）」パターンを1から引きます：$P(\text{at least one head}) = 1 - P(\text{all tails}) = 1 - (0.5)^5 = 0.969$。

- 2つの事象が同時に起こり得ない場合、それらは **互いに排他的 (mutually exclusive)** または **互いに素 (disjoint)** であると言います（$A \cap B = \emptyset$）。サイコロを1回投げて2が出ることと5が出ることは、互いに排他的です。

- **互いに排他的な事象の加法定理** はシンプルです：

$$P(A \cup B) = P(A) + P(B) \quad \text{($A \cap B = \emptyset$ の場合)}$$

- 事象が重なる可能性がある場合は、共通部分（交差）の二重カウントを避けるために **加法定理（一般形）** を使用します：

$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

- これは計数における包含排除の原理と同じです。上のベン図がその理由を示しています。紫色の領域（共通部分）は $P(A)$ で1回、$P(B)$ で1回、計2回カウントされているため、1回分を差し引く必要があります。

- **同時確率 (Joint probability)** $P(A \cap B)$ は、事象 $A$ と $B$ が両方とも起こる確率です。トランプのデッキにおいて、$P(\text{赤} \cap \text{キング}) = \frac{2}{52}$ です。赤のキングは2枚あるからです。

- **周辺確率 (Marginal probability)** は、他の事象に関わらず、ある単一の事象が起こる確率です。$P(\text{赤}) = \frac{26}{52} = 0.5$ は周辺確率です。2つの変数の同時分布がある場合、もう一方の変数について和をとる（または積分する）ことで周辺確率が得られます。

- **条件付き確率 (Conditional probability)** は、「事象 $B$ がすでに起こったという条件下で、事象 $A$ が起こる確率はどれくらいか？」という問いに答えます。標本空間を $S$ から $B$ へと縮小し、$B$ のうちどれだけの割合が $A$ に属しているかを求めます：

$$P(A | B) = \frac{P(A \cap B)}{P(B)}, \quad P(B) > 0$$

![標本空間を S から B に縮小するものとしての条件付き確率](../../images/conditional_probability.svg)

- 例：トランプを1枚引き、それが「赤である」と告げられたとします。それがキングである確率は？ 赤いカードは 26 枚あり、そのうちキングは 2 枚なので、$P(\text{king} | \text{red}) = \frac{2}{26} = \frac{1}{13}$ です。公式を使うと：$P(\text{king} \cap \text{red}) / P(\text{red}) = \frac{2/52}{26/52} = \frac{1}{13}$。

- 2つの事象が、一方が起こったことを知ってももう一方の確率に影響を与えない場合、それらは **独立 (independent)** であると言います。形式的には：

$$P(A \cap B) = P(A) \cdot P(B)$$

- 同等に、$P(A | B) = P(A)$ です。2つの別々のコインを投げることは独立した事象です。一方で、トランプを戻さずに（非復元抽出で）2枚引くことは、最初の1枚が残りのカードに影響を与えるため、独立ではありません。

- 独立性は、計算を大幅に簡略化してくれます。独立な事象であれば同時確率は積に分解でき、計算が扱いやすくなります。多くの ML モデル（例：ナイーブベイズ）が特徴量間の独立性を仮定するのは、まさにこの簡略化のためです。

- 任意の2つの事象に対する **乗法定理 (multiplication rule)** は、条件付き確率の公式を変形したものです：

$$P(A \cap B) = P(A | B) \cdot P(B) = P(B | A) \cdot P(A)$$

- 独立な事象の場合、条件付き確率は周辺確率と等しくなるため、これは $P(A \cap B) = P(A) \cdot P(B)$ に簡略化されます。

- **ベイズの定理 (Bayes' theorem)** は、確率論における最も重要な結果の一つであり、ベイズ主義機械学習の基礎です。これを用いると、条件付き確率の向きを逆転させることができます：

$$P(A | B) = \frac{P(B | A) \cdot P(A)}{P(B)}$$

- この定理は、$P(A \cap B)$ を2通りの方法（$P(B|A) \cdot P(A) = P(A|B) \cdot P(B)$）で書き、それを $P(A|B)$ について解くことで直接導かれます。

![ベイズの定理の構成要素：事後確率、尤度、事前確率、エビデンス](../../images/bayes_components.svg)

- 各コンポーネントには名前があります：
  - **事前確率 (Prior)** $P(A)$：証拠を見る前の初期の確信度。
  - **尤度 / ゆうど (Likelihood)** $P(B|A)$：$A$ が正しいと仮定したとき、その証拠がどれくらい得られやすいか。
  - **証拠 / エビデンス (Evidence)** $P(B)$：その証拠が得られる全確率。正規化因子として機能します。
  - **事後確率 (Posterior)** $P(A|B)$：証拠を見た後の、更新された確信度。

- 古典的な医療診断の例を解いてみましょう。ある病気が人口の 1% に影響を与えているとします。その病気の検査の精度は 95% です。つまり、病気の人を正しく特定できる確率（感度 / sensitivity）が 95% で、健康な人を正しく特定できる確率（特異度 / specificity）が 90% です。

- あなたが陽性（＋）と判定されました。実際にその病気にかかっている確率はどれくらいでしょうか？

- $D$ = 病気である、 $+$ = 陽性と判定される、とします。
  - 事前確率：$P(D) = 0.01$
  - 尤度：$P(+ | D) = 0.95$
  - 偽陽性率：$P(+ | D') = 0.10$

- まず全陽性率 $P(+)$ が必要です。**全確率の法則 (law of total probability)** を使います：

$$P(+) = P(+ | D) \cdot P(D) + P(+ | D') \cdot P(D')$$
$$= 0.95 \times 0.01 + 0.10 \times 0.99 = 0.0095 + 0.099 = 0.1085$$

- ここで、ベイズの定理を適用します：

$$P(D | +) = \frac{P(+ | D) \cdot P(D)}{P(+)} = \frac{0.95 \times 0.01}{0.1085} \approx 0.088$$

- 検査が「95% 正確」であるにもかかわらず、陽性結果が出ても実際にかかっている確率はわずか約 8.8% です。事前確率きわめて重要であることがわかります。病気自体が稀であるため、陽性結果のほとんどは偽陽性となってしまいます。これは、ML のあらゆる分類問題において重要な洞察です。クラスが不均衡（imbalanced）な場合、精度（Accuracy）だけで判断するのは非常に危険です。

- **全確率の法則 (law of total probability)** は、標本空間を互いに排他的で網羅的な事象 $B_1, B_2, \ldots, B_n$ に分割し、任意の事象 $A$ を次のように表現します：

$$P(A) = \sum_{i=1}^{n} P(A | B_i) \cdot P(B_i)$$

- これは、医療診断の例で $P(+)$ を計算する際に使用したものです。母集団を「病気である」と「病気でない」の2つに分けました。

- **確率の連鎖律 (chain rule of probability)** は、乗法定理を任意の数の事象へ一般化したものです：

$$P(A_1 \cap A_2 \cap \cdots \cap A_n) = P(A_1) \cdot P(A_2 | A_1) \cdot P(A_3 | A_1 \cap A_2) \cdots P(A_n | A_1 \cap \cdots \cap A_{n-1})$$

- 各要素は、それ以前のすべての事象を条件としています。これは、言語モデル（自己回帰型モデル）の背骨となる考え方です。ある文章の確率は、各単語が「それまでのすべての単語」の後に現れる条件付き確率の積として表されます。

- **条件付き独立 (Conditional independence)** とは、第3の事象が与えられたときに、2つの事象が独立になることを意味します。$C$ が与えられたとき、$A$ と $B$ が条件付き独立であるとは：

$$P(A \cap B | C) = P(A | C) \cdot P(B | C)$$

- 事象は、単体では依存していても条件付きでは独立であったり、あるいはその逆であったりします。例えば、2人の生徒のテストの点数が相関していても（どちらもテストの難易度に依存する）、テストの難易度がわかってしまえば、2人の点数は独立であるとみなせます。

- 条件付き独立は、ベイジアンネットワークのようなグラフィカルモデルにおいて鍵となる仮定です。これにより、複雑な同時分布を扱いやすい小さな断片に分解（ファクタライズ）することができ、推論を計算可能にします。

## コーディングタスク (CoLab または notebook を使用)

1. 医療診断問題をシミュレートしてください。100,000 人の人口を生成し、病気の有病率と検査精度を適用して、ベイズの定理が正しい事後確率を導き出すことを確認してください。

```python {cmd=true}
import jax
import jax.numpy as jnp

key = jax.random.PRNGKey(42)
n = 100_000

# 母集団の生成
k1, k2 = jax.random.split(key)
has_disease = jax.random.bernoulli(k1, p=0.01, shape=(n,))

# 検査結果の生成
k3, k4 = jax.random.split(k2)
# 感度: P(+|D) = 0.95, 特異度: P(-|D') = 0.90
test_positive = jnp.where(
    has_disease,
    jax.random.bernoulli(k3, p=0.95, shape=(n,)),
    jax.random.bernoulli(k4, p=0.10, shape=(n,))
)

# 陽性と判定された人のうち、実際に病気である人の割合は？
positives = test_positive.astype(bool)
true_positives = (has_disease & positives).sum()
total_positives = positives.sum()

print(f"全陽性判定数: {total_positives}")
print(f"真の陽性数:   {true_positives}")
print(f"P(病気 | 陽性) シミュレーション値: {true_positives / total_positives:.4f}")
print(f"ベイズの定理による理論値:        {0.95 * 0.01 / 0.1085:.4f}")
```

2. 加法定理をシミュレーションで検証してください。既知の確率と重なり（オーバーラップ）を持つランダムな事象 A と B を生成し、$P(A \cup B) = P(A) + P(B) - P(A \cap B)$ が成り立つことを確認してください。

```python {cmd=true}
import jax
import jax.numpy as jnp

key = jax.random.PRNGKey(0)
n = 200_000
k1, k2 = jax.random.split(key)

# 事象：A = 値 < 0.4, B = 値 < 0.6 (共通部分は < 0.4)
vals_a = jax.random.uniform(k1, shape=(n,))
vals_b = jax.random.uniform(k2, shape=(n,))

A = vals_a < 0.4
B = vals_b < 0.6

p_a = A.mean()
p_b = B.mean()
p_a_and_b = (A & B).mean()
p_a_or_b = (A | B).mean()

print(f"P(A) = {p_a:.4f}")
print(f"P(B) = {p_b:.4f}")
print(f"P(A ∩ B) = {p_a_and_b:.4f}")
print(f"P(A ∪ B) シミュレーション値: {p_a_or_b:.4f}")
print(f"P(A) + P(B) - P(A∩B) 論理値: {p_a + p_b - p_a_and_b:.4f}")
```

3. 条件の変化によって条件付き確率が変わることを示してください。2つのサイコロを投げるシミュレーションを行い、$P(\text{合計} = 7)$ を計算し、次に $P(\text{合計} = 7 | \text{1つ目の出目} = 3)$ を計算してください。

```python {cmd=true}
import jax
import jax.numpy as jnp

key = jax.random.PRNGKey(1)
n = 500_000
k1, k2 = jax.random.split(key)

d1 = jax.random.randint(k1, shape=(n,), minval=1, maxval=7)
d2 = jax.random.randint(k2, shape=(n,), minval=1, maxval=7)
total = d1 + d2

# 非条件付き
p_sum7 = (total == 7).mean()
print(f"P(合計=7) = {p_sum7:.4f} (理論値: {6/36:.4f})")

# 条件付き（1つ目が3だった場合）
mask = d1 == 3
p_sum7_given_d1_3 = (total[mask] == 7).mean()
print(f"P(合計=7 | 1つ目が3) = {p_sum7_given_d1_3:.4f} (理論値: {1/6:.4f})")
```

4. ベイズの定理を関数として実装し、確信度を反復的に更新（逐次更新）してください。コインのバイアス（偏り）に対する一様事前分布から始め、各フリップ（投擲）の結果を観察するたびに分布を更新してください。

```python {cmd=true}
import jax.numpy as jnp
import matplotlib.pyplot as plt

def bayes_update(prior, likelihood):
    """事前分布に尤度を掛け合わせ、正規化する"""
    posterior = prior * likelihood
    return posterior / posterior.sum()

# 可能なバイアス値を離散化する
theta = jnp.linspace(0, 1, 200)
prior = jnp.ones_like(theta)  # 一様事前分布
prior = prior / prior.sum()

# 観察されたフリップ結果: 1=表, 0=裏
flips = [1, 1, 0, 1, 1, 1, 0, 1, 0, 1]

plt.figure(figsize=(10, 5))
plt.plot(theta, prior, "--", color="#999", label="初期事前分布")

for i, flip in enumerate(flips):
    likelihood = theta if flip == 1 else (1 - theta)
    prior = bayes_update(prior, likelihood)
    if i in [0, 2, 4, 9]:
        plt.plot(theta, prior, label=f"{i+1} 回のフリップ後", linewidth=2)

plt.xlabel("コインのバイアス θ")
plt.ylabel("確信度 (正規化済み)")
plt.title("ベイズ更新：コインのバイアスに対する確信度の変化")
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```
