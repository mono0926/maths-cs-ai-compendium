# 情報理論

- 1948年にクロード・シャノンによって創始された**情報理論 (Information theory)** は、情報を定量化するための数学的枠組みを提供します。それは、「ある出来事にどれくらい驚くべきか？」「メッセージにはどれだけの情報が含まれているか？」「2つの確率分布はどれくらい異なっているか？」といった問いに答えてくれます。

- これらの問いは抽象的に聞こえるかもしれませんが、機械学習の損失関数、データ圧縮、そして通信システムの根幹をなしています。分類タスクで最も一般的な損失関数である交差エントロピー損失は、情報理論から直接導き出されたものです。

- まずは最も単純な問いから始めましょう。単一の出来事はどれだけの情報を持っているでしょうか？

- **驚き (Surprisal)**（自己情報量とも呼ばれます）は、ある出来事がどれほど意外であるかを測定します。非常に高い確率で起こる出来事からは、得られる情報はほとんどありません。逆に、稀な出来事が起これば、そこから多くのことを学ぶことができます。

- 砂漠に住んでいて、誰かが「今日は晴れだ」と言ったとしても、それはあまり有益な情報ではありません。しかし、「今日は雪だ」と言われれば、それは極めて有益な情報です。この直感を形式化したのが「驚き」です。

$$I(x) = \log_2 \frac{1}{p(x)} = -\log_2 p(x)$$

- $\log_2$ を使用する場合、単位は**ビット (bits)** となります。公平なコイン投げの「驚き」は $-\log_2(0.5) = 1$ ビットです。確率が $1/8$ の出来事の「驚き」は $\log_2(8) = 3$ ビットになります。

- なぜ単なる $1/p$ ではなく対数（ログ）を使うのでしょうか？理由は3つあります。
  - 確実な出来事 ($p = 1$) の情報はゼロであるべきです。$\log(1) = 0$ ですが、$1/1 = 1$ になってしまいます。
  - 独立した出来事の情報量は加算的であるべきです。$\log(1/p_1 p_2) = \log(1/p_1) + \log(1/p_2)$ が成り立ちます。
  - 滑らかで扱いやすい関数が望ましいです。$1/p$ は爆発的に増加しますが、$\log(1/p)$ は緩やかに増加します。

- **エントロピー (Entropy)** は「驚き」の期待値であり、ある分布からサンプリングされた出来事1回あたりに得られる情報の平均量です。これは分布の不確実性や「予測不可能性」を測定します。

$$H(X) = E[I(X)] = -\sum_{x} p(x) \log_2 p(x)$$

![高確率のイベントは驚きが少なく、低確率のイベントは驚きが大きいことを示す棒グラフ。エントロピーはそれらの加重平均である](../../images/surprisal_entropy.svg)

- 公平なコインのエントロピーは $H = -0.5\log_2(0.5) - 0.5\log_2(0.5) = 1$ ビット。不確実性が最大の状態です。
- $p = 0.9$ と偏ったコインのエントロピーは $H = -0.9\log_2(0.9) - 0.1\log_2(0.1) \approx 0.469$ ビット。不確実性が低いため、エントロピーも低くなります。
- 決定論的な出来事 ($p = 1$) のエントロピーは $H = 0$ です。不確実性は全くありません。

- すべての結果が等確率であるときにエントロピーは最大になります。$n$ 個の結果が等確率なら $H = \log_2 n$ です。公平なサイコロのエントロピーは $\log_2 6 \approx 2.585$ ビットです。

- エントロピーの実用的な意味は**圧縮**にあります。シャノンの情報源符号化定理によれば、情報を失うことなくデータをそのエントロピー率以下に圧縮することはできません。すべてのピクセルが等確率である画像（最大エントロピー）は圧縮できません。ほとんどが白の画像（低エントロピー）はよく圧縮されます。

- 規模感の目安として、256階調のグレースケールピクセル1つは最大8ビットのエントロピーを持ちます。1080pのグレースケール画像は最大で $1920 \times 1080 \times 8 \approx 1660$ 万ビットになります。実際の画像では隣接するピクセル間に相関があるためエントロピーはもっと低く、だからこそJPEG圧縮が機能するのです。

- 連続確率変数の場合、離散的な和は積分になります。**微分エントロピー (Differential entropy)** は次のように定義されます。

$$h(X) = -\int_{-\infty}^{\infty} f(x) \log f(x)\, dx$$

- 分散 $\sigma^2$ のガウス分布の微分エントロピーは $h = \frac{1}{2}\log_2(2\pi e \sigma^2)$ です。同じ分散を持つすべての分布の中で、ガウス分布が最大のエントロピーを持ちます。これがガウス分布がモデリングで非常によく使われる理由の一つです。指定された平均と分散以外に、最小限の仮定しか置いていないからです。

- **相互情報量 (Mutual information)** は、ある変数を知ることが別の変数についてどれだけの情報を与えるかを測定します。これは、$Y$ を観測したときに得られる $X$ の不確実性の減少量です。

$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$

- 同等に、次のようにも書けます。

$$I(X; Y) = \sum_{x,y} p(x,y) \log_2 \frac{p(x,y)}{p(x) p(y)}$$

- $X$ と $Y$ が独立であれば $p(x,y) = p(x)p(y)$ となり、相互情報量はゼロになります。依存関係が強いほど、相互情報量は高くなります。

- 機械学習において、相互情報量は特徴量選択（ターゲットとのMIが高い特徴量を選ぶ）、情報ボトルネック法、クラスタリング精度の評価などに使われます。

- **交差エントロピー (Cross-entropy)** は、分布 $q$ 用に最適化された符号を使用して、分布 $p$ からの出来事をエンコードするのに必要な平均ビット数を測定します。

$$H(p, q) = -\sum_{x} p(x) \log_2 q(x)$$

- $q$ が $p$ と完全に一致すれば、交差エントロピーはエントロピーと等しくなります：$H(p, p) = H(p)$。$q$ が不適切な近似であれば、交差エントロピーは高くなります。この「余分な」ビットが不一致から生じるものです。

- これこそ、交差エントロピーが機械学習の分類における標準的な損失関数である理由です。正解ラベルが $p$（ワンホット分布）を定義し、モデルの予測確率が $q$ を定義します。交差エントロピーを最小化することは、$q$ を $p$ に近づけることになります。

$$\mathcal{L} = -\sum_{c} y_c \log \hat{y}_c$$

- 正解クラスが $c$ である単一のサンプルでは、これは $\mathcal{L} = -\log \hat{y}_c$ と単純化されます。この損失は、モデルの予測における正解クラスの「驚き」です。モデルが正解に高い確率を割り当てていれば、損失は低くなります。

- **KLダイバージェンス (KL divergence)**（カルバック・ライブラー・ダイバージェンス。相対エントロピーとも呼ばれる）は、ある分布が別の分布とどれくらい異なっているかを測定します。

$$D_{\text{KL}}(p \| q) = \sum_{x} p(x) \log \frac{p(x)}{q(x)} = H(p, q) - H(p)$$

- KLダイバージェンスは、真の分布 $p$ の代わりに分布 $q$ を使用することによる「追加コスト」です。常に非負 ($D_{\text{KL}} \ge 0$) であり、$p = q$ のときにのみゼロになります。

![2つの分布 p と q の間のギャップが KL ダイバージェンスを表していることを示す図](../../images/kl_divergence.svg)

- KLダイバージェンスは対称ではありません：$D_{\text{KL}}(p \| q) \ne D_{\text{KL}}(q \| p)$。この非対称性は重要です。$D_{\text{KL}}(p \| q)$ は、$p$ の確率が高いところで $q$ の確率を低く見積もることに対してペナルティを与えます（$\log(p/q)$ が発散するため）。$D_{\text{KL}}(q \| p)$ はその逆に対してペナルティを与えます。

- この非対称性は、2つの近似スタイルをもたらします。
  - $D_{\text{KL}}(p \| q)$ の最小化は**モーメントマッチング (moment-matching)** 的な挙動を示します：$q$ は $p$ のすべての最頻値（モード）をカバーしようとしますが、広がりすぎてしまう可能性があります。
  - $D_{\text{KL}}(q \| p)$ の最小化は**モードシーク (mode-seeking)** 的な挙動を示します：$q$ は $p$ の一つのモードに集中しますが、他のモードを見逃す可能性があります。これは変分推論で使用される手法です。

- モデルにとって $H(p)$ は定数であるため、交差エントロピー $H(p, q)$ の最小化は $D_{\text{KL}}(p \| q)$ の最小化と等価です。だからこそ、交差エントロピー損失を使うことで、予測分布と真の分布の間のKLダイバージェンスも最小化していると断言できるのです。

- KLダイバージェンスは**ベイズ更新 (Bayesian updating)** において中心的な役割を果たします。事後分布 $P(\theta | D)$ は、観測されたデータと矛盾せず、（KLダイバージェンスの意味で）事前分布 $P(\theta)$ に最も近い分布です。新しい観測のたびに事後分布が更新され、$\theta$ に関する不確実性が減少していきます。

- 変分オートエンコーダー (VAE) では、損失関数は「再構成損失（交差エントロピー）」と「潜在空間を標準正規分布に近づけるためのKLダイバージェンス項」の2つから構成されます。

- まとめると、エントロピーは分布固有の不確実性、交差エントロピーはモデルがいかに現実を近似できているか、そしてKLダイバージェンスはその2つの間のギャップを示しています。これら3つの量は、現代の機械学習における最適化の根幹をなしています。

## コーディングタスク (CoLab または notebook を使用)

1. 様々な分布のエントロピーを計算し、与えられた結果の数に対して一様分布が最大のエントロピーを持つことを確認してください。

```python {cmd=true}
import jax.numpy as jnp

def entropy(p):
    """ビット単位のエントロピーを計算。確率ゼロの事象を除外。"""
    p = p[p > 0]
    return -jnp.sum(p * jnp.log2(p))

# 公平なサイコロ
fair = jnp.ones(6) / 6
print(f"公平なサイコロのエントロピー:   {entropy(fair):.4f} ビット (最大 = log2(6) = {jnp.log2(6.):.4f})")

# いかさまサイコロ
loaded = jnp.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.5])
print(f"いかさまサイコロのエントロピー: {entropy(loaded):.4f} ビット")

# 決定論的
det = jnp.array([0.0, 0.0, 0.0, 0.0, 0.0, 1.0])
print(f"決定論的:      {entropy(det):.4f} ビット")

# 公平なコイン
coin = jnp.array([0.5, 0.5])
print(f"公平なコインのエントロピー:  {entropy(coin):.4f} ビット")
```

2. 真の分布といくつかの近似分布の間の交差エントロピーと KL ダイバージェンスを計算してください。$D_{\text{KL}}(p \| q) = H(p, q) - H(p)$ であることを確認してください。

```python {cmd=true}
import jax.numpy as jnp

def cross_entropy(p, q):
    return -jnp.sum(p * jnp.log2(jnp.clip(q, 1e-10, 1.0)))

def kl_divergence(p, q):
    mask = p > 0
    return jnp.sum(jnp.where(mask, p * jnp.log2(p / jnp.clip(q, 1e-10, 1.0)), 0.0))

def entropy(p):
    p = p[p > 0]
    return -jnp.sum(p * jnp.log2(p))

p = jnp.array([0.4, 0.3, 0.2, 0.1])  # 真の分布

for name, q in [("完全一致", p),
                ("わずかな不一致", jnp.array([0.35, 0.30, 0.25, 0.10])),
                ("大きな不一致", jnp.array([0.1, 0.1, 0.1, 0.7]))]:
    h_p = entropy(p)
    h_pq = cross_entropy(p, q)
    kl = kl_divergence(p, q)
    print(f"{name:20s}: H(p)={h_p:.4f}, H(p,q)={h_pq:.4f}, "
          f"KL={kl:.4f}, H(p,q)-H(p)={h_pq-h_p:.4f}")
```

3. 2つの異なる分布に対して $D_{\text{KL}}(p \| q)$ と $D_{\text{KL}}(q \| p)$ を計算し、KL ダイバージェンスが対称でないことを示してください。

```python {cmd=true}
import jax.numpy as jnp

def kl_div(p, q):
    mask = p > 0
    return float(jnp.sum(jnp.where(mask, p * jnp.log2(p / jnp.clip(q, 1e-10, 1.0)), 0.0)))

p = jnp.array([0.9, 0.1])
q = jnp.array([0.5, 0.5])

print(f"D_KL(p || q) = {kl_div(p, q):.4f}")
print(f"D_KL(q || p) = {kl_div(q, p):.4f}")
print(f"一致しません。KLダイバージェンスは非対称です。")
```

4. トレーニング中の交差エントロピー損失をシミュレートしてください。正解の「ワンホット」ラベルを作成し、モデルの予測確率が改善するにつれて損失が減少する様子を示してください。

```python {cmd=true}
import jax.numpy as jnp
import matplotlib.pyplot as plt

# 正解ラベル: 4クラス中の2番目のクラス
true_label = jnp.array([0, 0, 1, 0])

# 予測が改善していく様子をシミュレート
steps = []
losses = []
for confidence in jnp.linspace(0.25, 0.99, 50):
    # モデルがクラス2に対してより確信を持つようになる
    remaining = (1 - confidence) / 3
    pred = jnp.array([remaining, remaining, confidence, remaining])
    loss = -jnp.sum(true_label * jnp.log(jnp.clip(pred, 1e-10, 1.0)))
    steps.append(float(confidence))
    losses.append(float(loss))

plt.figure(figsize=(8, 4))
plt.plot(steps, losses, color="#e74c3c", linewidth=2)
plt.xlabel("Model confidence in true class")
plt.ylabel("Cross-entropy loss")
plt.title("Cross-entropy loss decreases as predictions improve")
plt.grid(alpha=0.3)
plt.show()
```
