# 視覚・言語・アクション (VLA) モデル

- **視覚・言語からアクションへ**: 言語指示を物理的なアクションにグラウンディング（紐付け）する
- **VLA モデル**: アーキテクチャ (視覚エンコーダ + LLM + アクションヘッド)、RT-2、Octo、OpenVLA
- **アクションのトークン化**: 連続的なアクションの離散化、アクション・チャンキング
- **事前学習のレシピ**: ウェブスケールの視覚・言語データ → ロボット操作データ
- **汎用性**: 未知の物体、環境、指示への対応
- **インターネットデータとロボットデータの共同訓練 (Co-training)**
- **機体（Embodiment）に依存しないモデル**: 複数の異なるロボット形状に対応する単一モデル
- **ベンチマーク**: SIMPLER、実世界での評価プロトコル
