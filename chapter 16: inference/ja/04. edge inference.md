# エッジ推論 (Edge Inference)

- **エッジの制約**: 限られたメモリ、電力バジェット、ネットワークへの依存の禁止
- **モデル圧縮パイプライン**: プルーニング (枝刈り) → 量子化 → コンパイル
- **デバイス上のランタイム**: TensorFlow Lite、ONNX Runtime、Core ML、TensorRT、ExecuTorch
- **コンパイラスタック**: グラフの最適化、演算子の結合 (Operator fusion)、メモリプランニング、タイリング
- **ハードウェアターゲット**: モバイル GPU (Adreno、Mali)、NPU (Qualcomm Hexagon、Apple Neural Engine、Google Edge TPU)
- **デバイス上の LLM**: 10億〜30億パラメータ規模の Phi、Gemma、Llama、4 ビット推論
- **フェデレーション学習**: デバイス上での学習、プライバシーを保護した集約、通信効率
- **レイテンシの最適化**: モデルの分割、アーリーエグジット (Early exit)、キャッシュ戦略
