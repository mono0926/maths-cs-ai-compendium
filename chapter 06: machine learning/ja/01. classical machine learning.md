# 古典的な機械学習

- 機械学習とは、明示的にルールをプログラミングするのではなく、データから学ぶことによって、あるタスクにおけるパフォーマンスを向上させるアルゴリズムの研究です。「もし収入が5万ドル以上で年齢が30歳未満ならローンを承認する」というルールを書く代わりに、アルゴリズムに数千件の過去のローン決定データを与え、そこに隠れたパターンを自ら発見させます。

- 機械学習には大きく分けて3つのパラダイムがあります。**教師あり学習 (Supervised learning)** は、各入力に対して既知の正解出力が紐付いている「ラベル付きデータ」を使用します。アルゴリズムは、入力から出力へのマッピング（写像）を学習します。**教師なし学習 (Unsupervised learning)** はラベルのないデータを扱い、クラスタ（塊）や圧縮された表現など、隠れた構造を発見しようとします。**強化学習 (Reinforcement learning)** は、環境内での行動に対して報酬や罰則を受け取ることで、試行錯誤を通じて学習します（詳細はファイル04で扱います）。

- 教師あり学習の中で、**分類 (classification)** は離散的なカテゴリ（スパムか否か、猫か犬か）を予測し、**回帰 (regression)** は連続的な値（住宅価格、明日の気温）を予測します。この境界は必ずしも厳密ではありません。例えばロジスティック回帰は名前に「回帰」と付いていますが、実際には分類を行います。

- 確率モデルにおける重要な区別は、**生成モデル vs 判別モデル (generative vs discriminative)** です。生成モデルは結合分布 $P(x, y)$ を学習します。これは、データそのものがどのように生成されるかを理解することを意味し、新しいサンプルを生成することも可能です。判別モデルは $P(y \mid x)$ を直接学習し、クラス間の境界にのみ焦点を当てます。ナイーブベイズは生成モデルであり、ロジスティック回帰（ファイル02）は判別モデルです。生成モデルは柔軟性が高いですが学習が難しく、判別モデルは十分なデータがある場合に高い分類精度を示すことが多いです。

- **ナイーブベイズ (Naive Bayes)** は、最もシンプルで効果的な分類器の一つです。第5章で学んだベイズの定理を直接応用します。

$$P(C_k \mid x) = \frac{P(x \mid C_k) \, P(C_k)}{P(x)}$$

- 「ナイーブ（素朴）」と呼ばれる理由は、非常に強い独立性の仮定にあります。つまり、クラスが与えられたとき、すべての特徴量は互いに独立しているとみなします。例えばメールをスパムかどうか分類する場合、ナイーブベイズは「free（無料）」という単語が含まれていることが、そのメールがスパムだと分かった後の「winner（当選者）」という単語の出現確率に影響を与えないと仮定します。これは現実にはまずあり得ない仮定ですが、それでもこの分類器は驚くほど上手く機能します。

- $P(x)$ はすべてのクラスで共通であるため、分類は単に分子を最大化するクラスを選択することに簡略化されます。

$$\hat{y} = \arg\max_{k} \; P(C_k) \prod_{i=1}^{n} P(x_i \mid C_k)$$

- 事前確率 $P(C_k)$ は、単に訓練データにおける各クラスの割合です。尤度 $P(x_i \mid C_k)$ は扱う特徴量の種類に依存し、それによって3つの主要なバリエーションが生まれます。

- **多項ナイーブベイズ (Multinomial Naive Bayes)** は、文書内の単語頻度のようなカウントデータ向けに設計されています。各特徴量 $x_i$ は単語 $i$ が出現した回数を表し、尤度は多項分布に従います。これはテキスト分類、感情分析、スパムフィルタリングの標準的な選択肢です。

- **ガウスナイーブベイズ (Gaussian Naive Bayes)** は、各クラス内において各特徴量が正規分布（ガウス分布）に従うと仮定します。訓練データからクラス $k$ ごとの特徴量 $i$ の平均 $\mu_{ik}$ と分散 $\sigma_{ik}^2$ を推定し、次のように計算します。

$$P(x_i \mid C_k) = \frac{1}{\sqrt{2\pi\sigma_{ik}^2}} \exp\!\left(-\frac{(x_i - \mu_{ik})^2}{2\sigma_{ik}^2}\right)$$

- 身長、体重、センサーの読み取り値のような連続的な測定値を特徴量とする場合に自然な選択となります。

![2つの重なり合うガウス分布に基づくクラス条件付き分布と、事後確率が交差する位置にある決定境界](../../images/naive_bayes_classify.svg)

- **ベルヌーイナイーブベイズ (Bernoulli Naive Bayes)** は、バイナリ（二値）特徴量をモデル化します。各特徴量は「あり(1)」か「なし(0)」のいずれかです。単語が何回出現したかではなく、単に出現したかどうかだけを追跡します。短いテキストやバイナリ特徴量ベクトルに適しています。

- 実用上の問題として、ある特徴量の値が訓練データの特定のクラスで一度も出現しない場合があります。この場合、尤度がゼロになり、すべてを掛け合わせると事後確率全体がゼロになってしまいます。**ラプラススムージング (Laplace smoothing)** は、すべての「特徴量とクラスの組み合わせ」に小さなカウント（通常は1）を加えることで、この問題を解決します。

$$P(x_i \mid C_k) = \frac{\text{count}(x_i, C_k) + \alpha}{\text{count}(C_k) + \alpha \cdot V}$$

- ここで $\alpha$ はスムージングパラメータ（通常は1）、$V$ はその特徴量が取り得る値の数です。これにより、確率が厳密にゼロになることを防ぎます。

- **決定木 (Decision trees)** は、全く異なるアプローチを取ります。確率を計算する代わりに、一連の「はい/いいえ」の質問を通じて特徴空間を分割（パーティション）します。「20の質問」というゲームを想像してください。各ステップで、可能性を最も絞り込める質問を投げかけていきます。

- 木はすべての訓練データを含む「根（ルート）」から始まります。各「内部ノード」では、分割のための特徴量と閾値（例：「年齢 < 30？」）を選択します。データはその回答に基づいて左右に流れます。これを「葉（リーフ）」に到達するまで再帰的に繰り返します。葉ノードには予測値が保持されます。分類なら多数決によるクラス、回帰なら平均値です。

![特徴量による深さ2の分割、はい/いいえの分岐、クラス予測を示す色の付いた葉ノードを持つ決定木](../../images/decision_tree_split.svg)

- 重要な問いは「どの特徴量で分割すべきか？」です。分割後の「子ノード」ができるだけ「純粋（ピュア）」になるような分割が理想的です。つまり、ノード内のデータの多くが同じクラスに属するようにします。不純さを測る指標として、**ジニ不純度 (Gini impurity)** と**エントロピー (entropy)** がよく使われます。

- **ジニ不純度 (Gini impurity)** は、ノード内の分布に従ってランダムにラベルを付けたときに、誤分類される確率を測定します。

$$\text{Gini}(S) = 1 - \sum_{k=1}^{K} p_k^2$$

- ノードが完全に純粋（すべて同一クラス）ならジニ不純度は0になります。2クラスが50/50で完全に混ざっている場合、最大値の0.5になります。

- **エントロピー**（第5章の情報理論のセクションを参照）は「情報の驚き」の平均を測定します。

$$H(S) = -\sum_{k=1}^{K} p_k \log_2 p_k$$

- 純粋なノードのエントロピーは0です。完全にバランスの取れたバイナリノードのエントロピーは1ビットです。実際には、ジニ不純度とエントロピーは非常に似た木を生成します。ジニ不純度は対数計算を避けるため、計算がわずかに高速です。

- **情報利得 (Information gain)** は、分割によって達成される「不純度の減少」です。集合 $S$ を部分集合 $S_L$ と $S_R$ に分ける分割の場合：

$$\text{IG}(S, \text{split}) = H(S) - \frac{|S_L|}{|S|} H(S_L) - \frac{|S_R|}{|S|} H(S_R)$$

- アルゴリズムは各ノードで情報利得が最大になる分割を、どん欲（グリーディ）に選択します。これは局所的な最適戦略であり、大域的な最適性は保証されませんが、実用上は上手く機能します。

- **回帰木 (Regression trees)** も仕組みは同じですが、葉ノードでは連続値（そのリーフに到達したデータの平均値）を予測し、分割基準としてはジニ不純度やエントロピーの代わりに「分散の減少」を使用します。

- そのままにしておくと、決定木はすべての葉が純粋になるまで分割を続け、訓練データを丸暗記してしまいます。これが深刻な過学習（オーバーフィッティング）です。**枝刈り (Pruning)** はこれに対抗する手段です。「事前枝刈り」は、木の成長前に制限（最大深さ、リーフあたりの最小サンプル数、分割に必要な最小情報利得など）を設けます。「事後枝刈り」は一旦木を完全に成長させた後、バリデーションセットでのパフォーマンスを向上させない枝を除去します。

- 単一の決定木は解釈が容易ですが、データの小さな変化で全く異なる木が生成されるなど、不安定になりやすい傾向があります。**アンサンブル学習 (Ensemble methods)** は、多くのモデルを組み合わせることで、単一のモデルよりも優れた予測を実現します。

- 核心となるアイデアは「群衆の知恵」です。100人の平凡な分類器に多数決を取らせれば、個々の分類器の誤りが独立している限り、アンサンブル全体としては極めて優れた結果を出すことができます。

- **バギング (Bagging: bootstrap aggregating)** は、データのランダムな部分集合（重複を許す「ブートストラップサンプリング」）を使用して複数のモデルを訓練します。各モデルは元のデータの約63%を目にします。予測時には、出力を平均化（回帰）したり、多数決（分類）を取ったりします。各モデルが見るデータが異なるため、犯す間違いも異なり、平均化することでバリアンス（分散）の多くが打ち消されます。

- **ランダムフォレスト (Random Forests)** は、バギングを決定木に応用し、さらに一工夫加えたものです。各分割において、ランダムに選ばれた一部の特徴量（全 $d$ 個のうち通常は $\sqrt{d}$ 個）のみを検討します。これにより決定木同士の相関がさらに抑制され、アンサンブルとしての強力さが増します。ランダムフォレストは、あらゆる機械学習において最も信頼性の高い、即戦力の分類器の一つです。

![サイドバイサイドの比較: バギングは並列にモデルを訓練して平均化し、ブースティングは前のモデルの誤りを修正するように逐次的にモデルを訓練する](../../images/ensemble_methods.svg)

- **ブースティング (Boosting)** は対照的な哲学に基づいています。モデルを独立に訓練するのではなく、前のモデルが間違えたデータに焦点を当てて、逐次的に訓練を進めます。

- **AdaBoost** (Adaptive Boosting) は、各訓練データに「重み」を持たせます。最初はすべての重みは均一です。弱い学習器（多くの場合、非常に浅い決定木で「スタンプ」と呼ばれるもの）を訓練した後、誤分類されたデータの重みを大きくし、次の学習器がそれらに注意を払うようにします。最終的な予測はすべての学習器による加重投票となり、パフォーマンスの高い学習器ほど大きな発言権を持ちます。

$$H(x) = \text{sign}\!\left(\sum_{t=1}^{T} \alpha_t \, h_t(x)\right)$$

- 学習器 $t$ の重み $\alpha_t$ は、そのエラー率 $\epsilon_t$ に依存します。

$$\alpha_t = \frac{1}{2} \ln\!\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)$$

- エラー率が低い学習器には大きな正の重みが与えられ、ランダムな予測 ($\epsilon = 0.5$) に近い学習器の重みはゼロになります。

- **勾配ブースティング (Gradient Boosting)** はこの考え方を一般化したものです。データの重みを変える代わりに、新しいモデルがそれまでのアンサンブルの「残差エラー」（損失関数の負の勾配）を予測するように訓練されます。二乗誤差損失の場合、残差は文字通り予測値と正解値の差になります。決定木を用いた勾配ブースティング (GBDT) は、構造化データのコンペティションで多くの勝利を収めてきました (XGBoost, LightGBM, CatBoost などが有名な実装です)。

- 重要な対比：バギングは**バリアンス**を減少させ（ノイズを平均化する）、ブースティングは**バイアス**を減少させます（体系的な誤りを修正する）。バギングは個々のモデルが過学習しているときに、ブースティングは個々のモデルが未学習（アンダーフィッティング）のときに最も効果を発揮します。

- 教師なし学習に目を向けると、**K-Meansクラスタリング**は最もシンプルで広く使われているクラスタリングアルゴリズムです。$n$ 個のデータと目標のクラスタ数 $K$ が与えられたとき、各点からそのクラスタ中心（セントロイド）までの距離の合計を最小化するように、データを $K$ 個のグループに割り当てます。

- アルゴリズムは2つのステップを交互に繰り返します。まず「**割り当て (assign)**」：各点を最も近いセントロイドに割り当てます。次に「**更新 (update)**」：各セントロイドを、そこに割り当てられたすべての点の平均値に移動させます。割り当てが変化しなくなるまで繰り返します。各ステップでクラスタ内距離の合計が減少（または維持）するため、必ず収束することが保証されています。

![色分けされた3つのクラスタの点、セントロイドのマーカー、破線のクラスタ境界を示す2D散布図](../../images/kmeans_clustering.svg)

- 形式的には、K-Meansは**慣性 (inertia)** と呼ばれるクラスタ内の平方和を最小化します。

$$J = \sum_{k=1}^{K} \sum_{x \in C_k} \|x - \mu_k\|^2$$

- ここで $\mu_k$ はクラスタ $C_k$ のセントロイドです。

- K-Meansは初期値に敏感です。初期セントロイドの選択が悪いと、不適切な局所解に陥ることがあります。**K-Means++** 初期化戦略は、最初のセントロイドをランダムに選び、2つ目以降は既存のセントロイドからの距離の2乗に比例する確率で選択します。これにより初期中心が分散され、ほぼ常に優れた結果が得られます。

- $K$ をどう選ぶか？2つの一般的なツールがあります。**エルボー法 (elbow method)** は、慣性を $K$ に対してプロットし、クラスタを増やしても効果が薄れ始める「肘（エルボー）」のような曲がり角を探します。**シルエット係数 (silhouette score)** は、ある点が自クラスタとどれだけ似ていて、隣接する他のクラスタとどれだけ異なっているかを、-1（誤ったクラスタ）から+1（適切に分類）までの範囲で測定します。全データの平均シルエット係数が、クラスタリングの品質の全体的な指標になります。

- K-Meansには限界もあります。クラスタが球状でサイズがほぼ均一であることを前提としており、また「ハードな割り当て」（各点はちょうど1つのクラスタに属する）を行います。**混合ガウスモデル (Gaussian Mixture Models: GMM)** はこれら両方の制限を緩和します。

- GMMは、データを $K$ 個のガウス分布（それぞれが平均 $\mu_k$、共分散 $\Sigma_k$、混合重み $\pi_k$ を持つ）の混合としてモデル化します（重みの合計は1になります）。

$$P(x) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(x \mid \mu_k, \Sigma_k)$$

- ハードな割り当ての代わりに、各点は「**ソフトな割り当て**」を受け取ります。つまり、各クラスタに属する確率（負担率: responsibility）が計算されます。2つのガウス分布の境界付近にある点は、「60%はクラスタA、40%はクラスタB」のように扱われることがあります。

- GMMの学習には **EMアルゴリズム (Expectation-Maximisation algorithm)** が使われます。これはK-Meansのように2つのステップを繰り返します。**Eステップ**（期待値）では負担率を計算します：各点について、各ガウス分布から生成された確率を求めます。**Mステップ**（最大化）ではパラメータを更新します：与えられた負担率に基づいて、最適な平均、共分散、混合重みを求めます。EMアルゴリズムは各反復でデータの尤度を増加させることが保証されており、局所最大値に収束します。

- 実はK-Meansは、GMMのEMアルゴリズムの特殊なケースです。共分散が等しい球状ガウス分布を用い、負担率をハード（0または1）にしたものに相当します。

- **サポートベクターマシン (Support Vector Machines: SVM)** は、幾何学的な観点から分類にアプローチします。2つのクラスが線形分離可能な場合、それらを分けるハイパープレーン（超平面）は無数に存在します。SVMは、ハイパープレーンと各クラスの最も近いデータ点との間の距離が最大となる、**最大マージン (maximum margin)** を持つプレーンを見つけます。

- マージンの境界線上に位置する最も近い点たちは、**サポートベクター (support vectors)** と呼ばれます。境界を定義する上で重要なのはこれらの点だけであり、他の訓練データを取り除いても描かれるハイパープレーンは変わりません。

![最大マージンのハイパープレーンによって隔てられた2つのクラス。マージンの帯と丸で囲まれたサポートベクターが示されている](../../images/svm_margin.svg)

- 線形分類器 $f(x) = w \cdot x + b$ において、最大マージンを見つけることは次の問題を解くことに相当します。

$$\min_{w, b} \; \frac{1}{2}\|w\|^2 \quad \text{条件：} \quad y_i(w \cdot x_i + b) \geq 1 \; \text{（すべての } i \text{ に対して）}$$

- これは凸二次計画問題であるため、一意なグローバル解が存在します（局所解に悩まされる心配はありません）。

- 現実のデータが完璧に分離できることは稀です。**ソフトマージンSVM** は、スラック変数 $\xi_i \geq 0$ を導入することで、マージンの違反を許容します。

$$\min_{w, b, \xi} \; \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{n} \xi_i \quad \text{条件：} \quad y_i(w \cdot x_i + b) \geq 1 - \xi_i$$

- ハイパーパラメータ $C$ がトレードオフを制御します。$C$ が大きいと誤分類に厳しく（タイトな適合、過学習のリスク）、$C$ が小さいとより多くの違反を許容します（広いマージン、より正則化されたモデル）。

- SVMの最も強力な機能は**カーネルトリック (kernel trick)** です。元の特徴空間では線形分離できないデータセットの多くも、高次元空間にマッピングすると分離可能になります。カーネルトリックを使うと、その高次元空間への変換を明示的に計算することなく、高次元空間での内積を計算できます。

- カーネル関数 $K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$ が、SVMの最適化におけるすべてのドット積を置き換えます。最も一般的なカーネルは **RBF（動径基底関数: Radial Basis Function）** カーネルです。

$$K(x_i, x_j) = \exp\!\left(-\gamma \|x_i - x_j\|^2\right)$$

- RBFカーネルは、データを暗黙的に無限次元空間へとマッピングします。パラメータ $\gamma$ は単一の訓練データの個別の影響範囲を制御します：$\gamma$ が大きいと各点が直近の近傍にしか影響を与えず（過学習のリスク）、$\gamma$ が小さいとより滑らかな境界が生成されます。

- 他にも、多項式カーネル $K(x_i, x_j) = (x_i \cdot x_j + c)^d$ や、線形カーネル $K(x_i, x_j) = x_i \cdot x_j$（変換を行わない標準的なSVM）などがあります。

- 実際、ディープラーニングが普及する前は、RBFカーネルを用いたSVMが主流の分類器でした。サンプル数に対して特徴量が多い場合など、小中規模のデータセットでは現在でも有効です。

- SVMと第2章（行列）の結びつきは深いです。最適化は通常、訓練データ間のドット積のみに依存する「双対形式」で解かれます。これこそがカーネルトリックを可能にしている仕組みです。アルゴリズム全体が内積と線形代数の言葉で記述されています。

- 古典的な機械学習のツールキットをまとめると以下の通りです。

| アルゴリズム       | 種類                       | 主な長所                         | 主な短所             |
| ------------------ | -------------------------- | -------------------------------- | -------------------- |
| ナイーブベイズ     | 教師あり（生成）           | 高速、少量のデータで機能         | 独立性の仮定         |
| 決定木             | 教師あり                   | 解釈性が高い                     | 過学習しやすい       |
| ランダムフォレスト | 教師あり（アンサンブル）   | 堅牢、ハイパーパラメータが少ない | 解釈性がやや低い     |
| 勾配ブースティング | 教師あり（アンサンブル）   | 表データで最高水準               | 低速、調整が大変     |
| K-Means            | 教師なし（クラスタリング） | シンプル、拡張性が高い           | 球状クラスタを想定   |
| GMM                | 教師なし（クラスタリング） | ソフト割り当て、柔軟な形状       | 初期値に敏感         |
| SVM                | 教師あり                   | 高次元で効果的                   | 大規模データでは低速 |

## コーディングタスク (CoLab または notebook を使用)

1. ガウスナイーブベイズをスクラッチで実装してください。2クラスの合成2Dデータで訓練し、判定境界を可視化してください。scikit-learn の実装と比較してみてください。

```python {cmd=true}
import jax.numpy as jnp
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification

# 合成データの生成
X, y = make_classification(n_samples=300, n_features=2, n_redundant=0,
                           n_informative=2, n_clusters_per_class=1, random_state=42)
X, y = jnp.array(X), jnp.array(y)

# ガウスナイーブベイズをスクラッチで学習
classes = jnp.unique(y)
params = {}
for c in classes:
    c = int(c)
    mask = y == c
    X_c = X[mask]
    params[c] = {
        'mean': jnp.mean(X_c, axis=0),
        'var': jnp.var(X_c, axis=0),
        'prior': jnp.sum(mask) / len(y)
    }

def gaussian_log_likelihood(x, mean, var):
    return -0.5 * jnp.sum(jnp.log(2 * jnp.pi * var) + (x - mean)**2 / var)

def predict(X):
    preds = []
    for x in X:
        log_posts = []
        for c in [0, 1]:
            log_post = jnp.log(params[c]['prior']) + gaussian_log_likelihood(
                x, params[c]['mean'], params[c]['var'])
            log_posts.append(log_post)
        preds.append(jnp.argmax(jnp.array(log_posts)))
    return jnp.array(preds)

# 決定境界の可視化
xx, yy = jnp.meshgrid(jnp.linspace(X[:,0].min()-1, X[:,0].max()+1, 200),
                       jnp.linspace(X[:,1].min()-1, X[:,1].max()+1, 200))
grid = jnp.column_stack([xx.ravel(), yy.ravel()])
zz = predict(grid).reshape(xx.shape)

plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, zz, alpha=0.3, cmap='coolwarm')
plt.scatter(X[y==0, 0], X[y==0, 1], c='#3498db', label='Class 0', edgecolors='k', s=20)
plt.scatter(X[y==1, 0], X[y==1, 1], c='#e74c3c', label='Class 1', edgecolors='k', s=20)
plt.title("Gaussian Naive Bayes Decision Boundary")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

accuracy = jnp.mean(predict(X) == y)
print(f"訓練精度: {accuracy:.2%}")
```

2. ジニ不純度を使用して分割を行う決定木を構築してください。単一ノードの分割ロジックを実装し、情報利得によって最適な特徴量と閾値がどのように選ばれるかを示してください。

```python {cmd=true}
import jax.numpy as jnp

def gini_impurity(y):
    """ラベル配列のジニ不純度を計算"""
    classes, counts = jnp.unique(y, return_counts=True)
    probs = counts / len(y)
    return 1.0 - jnp.sum(probs ** 2)

def information_gain(y, left_mask):
    """booleanマスクによる y の分割前後での情報利得を計算"""
    parent_gini = gini_impurity(y)
    left_y, right_y = y[left_mask], y[~left_mask]
    n = len(y)
    if len(left_y) == 0 or len(right_y) == 0:
        return 0.0
    child_gini = (len(left_y)/n) * gini_impurity(left_y) + \
                 (len(right_y)/n) * gini_impurity(right_y)
    return float(parent_gini - child_gini)

def best_split(X, y):
    """情報利得を最大化する特徴量と閾値を見つける"""
    best_ig, best_feat, best_thresh = -1, None, None
    for feat in range(X.shape[1]):
        thresholds = jnp.unique(X[:, feat])
        for thresh in thresholds:
            mask = X[:, feat] <= float(thresh)
            ig = information_gain(y, mask)
            if ig > best_ig:
                best_ig, best_feat, best_thresh = ig, feat, float(thresh)
    return best_feat, best_thresh, best_ig

# 例: 合成データ
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=100, n_features=4, n_redundant=0, random_state=0)
X, y = jnp.array(X), jnp.array(y)

feat, thresh, ig = best_split(X, y)
print(f"最適な分割: 特徴量 {feat}, 閾値 {thresh:.3f}, 情報利得 {ig:.4f}")
print(f"親のジニ不純度: {gini_impurity(y):.4f}")
mask = X[:, feat] <= thresh
print(f"左ノードのジニ不純度:   {gini_impurity(y[mask]):.4f} ({int(jnp.sum(mask))} サンプル)")
print(f"右ノードのジニ不純度:  {gini_impurity(y[~mask]):.4f} ({int(jnp.sum(~mask))} サンプル)")
```

3. K-Means++ 初期化を用いた K-Means をスクラッチで実装してください。合成データセットをクラスタリングし、最終的なクラスタを可視化してください。

```python {cmd=true}
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# 合成クラスタの生成
X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=42)
X = jnp.array(X)

def kmeans_plus_plus_init(X, K, key):
    """K-Means++ 初期化"""
    n = X.shape[0]
    idx = jax.random.randint(key, (), 0, n)
    centroids = [X[idx]]
    for _ in range(1, K):
        dists = jnp.min(jnp.stack([jnp.sum((X - c)**2, axis=1) for c in centroids]), axis=0)
        probs = dists / jnp.sum(dists)
        key, subkey = jax.random.split(key)
        idx = jax.random.choice(subkey, n, p=probs)
        centroids.append(X[idx])
    return jnp.stack(centroids)

def kmeans(X, K, max_iters=20, key=jax.random.PRNGKey(0)):
    centroids = kmeans_plus_plus_init(X, K, key)
    history = [centroids]
    for _ in range(max_iters):
        # 割り当てステップ
        dists = jnp.stack([jnp.sum((X - c)**2, axis=1) for c in centroids])
        labels = jnp.argmin(dists, axis=0)
        # 更新ステップ
        new_centroids = jnp.stack([
            jnp.mean(X[labels == k], axis=0) for k in range(K)
        ])
        history.append(new_centroids)
        if jnp.allclose(centroids, new_centroids):
            break
        centroids = new_centroids
    return labels, centroids, history

K = 4
labels, centroids, history = kmeans(X, K)

# 最終結果のプロット
colors = ['#3498db', '#e74c3c', '#27ae60', '#9b59b6']
plt.figure(figsize=(8, 6))
for k in range(K):
    mask = labels == k
    plt.scatter(X[mask, 0], X[mask, 1], c=colors[k], s=20, alpha=0.6)
    plt.scatter(centroids[k, 0], centroids[k, 1], c=colors[k], marker='X',
                s=200, edgecolors='k', linewidths=1.5)
plt.title(f"K-Means クラスタリング (K={K}, {len(history)-1} 反復)")
plt.grid(alpha=0.3)
plt.show()

# 慣性 (inertia) を計算
inertia = sum(jnp.sum((X[labels == k] - centroids[k])**2) for k in range(K))
print(f"最終的な慣性: {inertia:.2f}")
```

4. カーネルトリックの実演。多項式カーネルを例に、カーネル関数の値と「高次元へ写像した後のドット積」が一致することを確認してください。

```python {cmd=true}
import jax.numpy as jnp

# シンプルな 2D データ
X = jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])

# 多項式カーネル: K(x,y) = (x·y + 1)^2
def poly_kernel(X, degree=2, c=1.0):
    return (X @ X.T + c) ** degree

# 2Dデータに対する明示的な2次特徴マッピング: (1, sqrt(2)*x1, sqrt(2)*x2, x1^2, x2^2, sqrt(2)*x1*x2)
def poly_features(X):
    x1, x2 = X[:, 0], X[:, 1]
    return jnp.column_stack([
        jnp.ones(len(X)),
        jnp.sqrt(2) * x1,
        jnp.sqrt(2) * x2,
        x1 ** 2,
        x2 ** 2,
        jnp.sqrt(2) * x1 * x2
    ])

K_trick = poly_kernel(X)
phi = poly_features(X)
K_explicit = phi @ phi.T

print("カーネルトリック (次数2の多項式):")
print(K_trick)
print("\n明示的な特徴マッピングによるドット積:")
print(K_explicit)
print(f"\n行列が一致: {jnp.allclose(K_trick, K_explicit)}")

# RBFカーネル: 有限次元の明示的な写像は存在しません
def rbf_kernel(X, gamma=0.5):
    sq_dists = jnp.sum(X**2, axis=1, keepdims=True) + \
               jnp.sum(X**2, axis=1) - 2 * X @ X.T
    return jnp.exp(-gamma * sq_dists)

K_rbf = rbf_kernel(X)
print("\nRBFカーネル行列:")
print(K_rbf)
print("対角成分は常に 1 (自分自身との類似度)")
print("非対角成分は距離に応じて減衰します")
```
