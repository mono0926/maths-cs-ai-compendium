# 勾配ベースの機械学習

- ファイル01で扱った古典的な手法は、巧妙なヒューリスティクスや解析的な解法（閉形式解）を使用していました。このファイルでは、勾配（グラディエント）に従って学習するアルゴリズムを扱います。これは、損失曲面上で低い方へと小さな一歩を踏み出し続け、最適なパラメータを見つけ出す手法です。勾配ベースの学習は、線形回帰から世界最大のニューラルネットワークに至るまで、あらゆるものの原動力となっています。

- **線形回帰 (Linear regression)** は最も単純な勾配ベースのモデルですが、解析的な解（正規方程式）も持っているため、学習の出発点として最適です。モデルは直線（または高次元では超平面）です。

$$\hat{y} = w \cdot x + b = \sum_{i=1}^{d} w_i x_i + b$$

- 第2章の行列の記法を用いると、すべての訓練入力を行列 $X$ の行として積み上げ、バイアスを $w$ の一部として処理（$X$ に1の列を追加）することで、$\hat{y} = Xw$ と書けます。

- 目標は、予測値と実際の値の差の2乗の平均である**平均二乗誤差 (MSE)** を最小化することです。

$$\mathcal{L}(w) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} \|y - Xw\|^2$$

- なぜ2乗誤差を使うのでしょうか？それには統計学的な正当化があります。もしターゲット $y$ が $y = Xw + \epsilon$（ただし $\epsilon \sim \mathcal{N}(0, \sigma^2)$）として生成されると仮定するなら、データのガウス尤度（第5章）を最大化することは、MSEを最小化することと等価になります。また、2乗誤差は小さなミスよりも大きなミスに重いペナルティを与えるという、実用上の利点もあります。

![データ点の散布図と最適フィットの直線、および誤差を示す破線の垂直な残差線](../../images/linear_regression_fit.svg)

- MSEは $w$ の二次関数であるため、解析的に求めることができる単一のグローバルな最小値を持ちます。微分してゼロと置き、解くことで**正規方程式 (normal equation)** が得られます。

$$w^{*} = (X^T X)^{-1} X^T y$$

- これは第2章で学んだ行列の逆行列を直接的に使用しています。$X^T X$ は $d \times d$ 行列（$d$ は特徴量の数）、$X^T y$ は $d$ 次元ベクトルです。正規方程式を使えば、一度の計算で厳密に最適な重みを得ることができます。

- 正規方程式が使えないのはどのような場合でしょうか？それは $X^T X$ が特異行列（逆行列を持たない）のときです。特徴量が線形従属である場合や、サンプル数よりも特徴量の方が多い場合 ($d > n$) に起こります。このような場合は、後述する正則化や勾配降下法が必要になります。

- **ロジスティック回帰 (Logistic regression)** は、線形モデルをバイナリ分類（二値分類）に適応させたものです。連続的な値を予測する代わりに、0から1の間の確率を予測したいと考えます。**シグモイド関数 (sigmoid function)** は、あらゆる実数をこの範囲に押し込めます。

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

- モデルは、線形回帰と同じように線形スコア $z = w \cdot x + b$ を計算し、それをシグモイド関数に通します：$\hat{y} = \sigma(w \cdot x + b)$。出力 $\hat{y}$ は、$P(y = 1 \mid x)$（入力 $x$ が与えられたときにクラス1である確率）として解釈されます。

![0.5の閾値が記されたシグモイド曲線。クラス0とクラス1を予測するそれぞれの領域が示されている](../../images/sigmoid_logistic.svg)

- シグモイド関数には優れた性質があります：$\sigma(0) = 0.5$ であり、$z \to \infty$ で $\sigma(z) \to 1$、$z \to -\infty$ で $\sigma(z) \to 0$ となります。また、その微分は $\sigma'(z) = \sigma(z)(1 - \sigma(z))$ という優美な形をしています。

- ロジスティック回帰の損失関数は**バイナリ交差エントロピー (Binary cross-entropy: BCE)** であり、ベルヌーイ尤度（第5章）から直接導かれます。

$$\mathcal{L} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]$$

- 正解ラベル $y_i$ が1のときは第1項のみが有効になり、予測値が低い場合にペナルティを与えます。正解ラベルが0のときは第2項のみが有効になり、予測値が高い場合にペナルティを与えます。対数（ログ）をとることで、「自信満々な間違い」に対して極めて厳しいペナルティが課されます。例えば正解が1のときに0.01と予測する方が、0.4と予測するよりもはるかに高いコスト（損失）になります。

- 線形回帰のMSEとは異なり、BCEを最小化する重みには閉形式解が存在しません。そこで反復的なアプローチである**勾配降下法 (gradient descent)** が必要になります。

- 勾配降下法の直感的な理解は簡単です。霧の中、起伏のある地形（損失曲面）に立っていると想像してください。どこが一番低い場所（グローバルな最小値）かは見えませんが、足元の傾斜（勾配）は感じることができます。傾斜を下る方向に一歩踏み出し、また傾斜を感じて進む。これを繰り返すことで、最終的に谷底（最小値）に到達します。

$$w \leftarrow w - \eta \frac{\partial \mathcal{L}}{\partial w}$$

- **学習率 (learning rate)** $\eta$ は、一歩の大きさを制御します。大きすぎると谷を飛び越えてしまい、収束せずに飛び跳ねてしまいます。小さすぎると進みが遅すぎて、浅い局所解（ローカルミニマム）から抜け出せなくなる可能性があります。

![1Dの損失曲線上の3つのボール。高すぎる学習率は飛び越え、適切な学習率は収束し、低すぎる学習率は途中で止まってしまう様子](../../images/gradient_descent_landscape.svg)

- 勾配 $\frac{\partial \mathcal{L}}{\partial w}$ は、最も急な「上り坂」の方向を指すベクトルです。私たちは下り坂に行きたいので、この値を引き算します。これは第3章で学んだ微分（連鎖律）を損失関数に適用したものです。

- **バッチ勾配降下法 (Batch gradient descent)** は、ステップごとに訓練データ全体を使って勾配を計算します。これは正確な勾配が得られますが、データの数 $n$ が大きい場合には計算コストが非常に高くなります。

- **確率的勾配降下法 (Stochastic gradient descent: SGD)** は、ステップごとにランダムに選んだ1つのサンプルのみを使用します。勾配はノイズを含みますが（1つのサンプルからの推定であるため）、各ステップは極めて高速です。このノイズは、むしろ浅い局所解から抜け出す助けになることもあります。

- **ミニバッチ勾配降下法 (Mini-batch gradient descent)** は、その中間を取ります。ステップごとに $B$ 個（通常は32, 64, 256など）のサンプルを使用します。これは、バッチ全体での計算効率（行列演算による高速化）と勾配の精度のバランスをとる手法で、現代のディープラーニングのほとんどで採用されています。

- **誤差逆伝播法 (Backpropagation)** は、ニューラルネットワークのような多くのパラメータを持つモデルで、実際に勾配を計算する方法です。これは、第3章の連鎖律を計算グラフ全体に体系的に適用したものです。

- あらゆるモデルは、操作の有向非巡回グラフ (DAG) として表現できます。入力が流れ込み、重みが掛けられ、足し合わされ、非線形関数を通って、最終的に損失値を出力します。**順伝播 (forward pass)** は、データを入力から出力へ流すことで出力を計算します。

- **逆伝播 (backward pass)** は、逆に勾配を流します。損失からスタートして、連鎖律をすべてのノードに適用し、各パラメータに対する損失の変化量を計算します。$L$ が $z$ に依存し、$z$ が $w$ に依存しているなら次のように計算できます。

$$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial w}$$

- 各ノードは、自身の局所的な微分と、上から流れてきた勾配さえ分かれば計算が可能です。これにより、逆伝播は非常にモジュール化され、効率的になります。計算コストは順伝播の約2倍程度です。

- 単純なSGDには問題があります。急なカーブを持つ方向では振動し、平坦な方向では進みが極めて遅くなります。**オプティマイザ (Optimisers)** は、過去の勾配の履歴に基づいてステップを調整することで、これを改善します。

- **モーメンタム (Momentum)** を伴うSGDは、過去の勾配の移動平均（指数移動平均、第4章参照）を保持します。これにより振動が抑制され、一貫した方向への進みが加速されます。

$$v_t = \beta v_{t-1} + (1 - \beta) \nabla \mathcal{L}$$
$$w \leftarrow w - \eta \, v_t$$

- 坂を転がるボールを想像してください。モーメンタム（慣性）があることで、一方向には加速し、左右の小刻みな揺れは打ち消されます。通常、$\beta = 0.9$ が使われます。

- **ネステロフの加速勾配法 (Nesterov Accelerated Gradient: NAG)** は、賢い修正案です。現在の位置ではなく、慣性で進んだ先の「先読み」位置 $w - \eta \beta v_{t-1}$ で勾配を計算します。この修正ステップにより、行き過ぎを防ぐことができます。

$$v_t = \beta \, v_{t-1} + \nabla \mathcal{L}(w - \eta \beta \, v_{t-1})$$
$$w \leftarrow w - \eta \, v_t$$

- **Adagrad** は、パラメータごとに学習率を調整します。大きな勾配を受けたパラメータの学習率を小さくし、その逆も行います。勾配の2乗を累積していきます。

$$G_t = G_{t-1} + g_t^2, \quad w \leftarrow w - \frac{\eta}{\sqrt{G_t + \epsilon}} g_t$$

- 問題点：$G_t$ は増え続ける一方なので、実効的な学習率は単調に減少し続け、最終的には何も学習できなくなるほど小さくなってしまいます。

- **RMSprop** は、勾配の2乗を累積する代わりに「指数移動平均」を使用することでこれを解決し、古い勾配よりも新しい勾配を重視するようにします。

$$s_t = \beta \, s_{t-1} + (1 - \beta) g_t^2, \quad w \leftarrow w - \frac{\eta}{\sqrt{s_t + \epsilon}} g_t$$

- **Adam (Adaptive Moment Estimation)** は、モーメンタムとRMSpropを組み合わせたものです。1次のモーメント推定（勾配の平均。モーメンタムに対応）と2次のモーメント推定（勾配の2乗の平均。RMSpropに対応）の両方を保持します。

$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$
$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$

- $m_t$ と $v_t$ はゼロで初期化されるため、初期のステップではゼロに偏ってしまいます。これを補正するのがバイアス補正です。

$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$

$$w \leftarrow w - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

- 2Dの等高線図において、SGDがジグザグに進むのに対し、Momentumは滑らかな経路を辿り、Adamは最小値へ向かって最も直接的なルートを突き進みます。

- デフォルトのハイパーパラメータ ($\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$) は幅広い問題で良好に機能するため、Adamは多くのディープラーニングにおいてデフォルトのオプティマイザとなっています。

- **AdamW** は、重み減衰（Weight Decay）を勾配の更新から切り離したものです。標準的な L2 正則化と Weight Decay は、SGDでは等価ですが、Adamでは異なります。AdamW は $\lambda w$ を勾配に足すのではなく、パラメータに対して個別に重み減衰を適用します。これにより汎化性能が向上し、現在では Transformer の学習などの標準となっています。

$$w \leftarrow w - \eta \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \, w \right)$$

- **LION (EvoLved Sign Momentum)** は、プログラム探索によって発見された新しいオプティマイザです。モーメンタムの「符号 (sign)」のみを使用（大きさは無視）することで、各更新を均一なスケールにします。Adamよりもメモリ使用量が少なく（2次モーメントの保持が不要）、多くのタスクでAdamを上回ることもあります。

$$w \leftarrow w - \eta \cdot \text{sign}(\beta_1 \, m_{t-1} + (1 - \beta_1) \, g_t)$$
$$m_t = \beta_2 \, m_{t-1} + (1 - \beta_2) \, g_t$$

- **Muon (Momentum + Orthogonalisation)** は、ネステロフ・モーメンタムを適用した後、更新行列を直交化する手法です。直交化には polar decomposition（極分解）を近似する Newton-Schulz 反復を使用します。これにより更新方向が Stiefel多様体上に乗り、すべての特異方向に対して更新の大きさが均一になります。Adamのような適応的な2次モーメント推定（$v_t$ のバッファ）が不要になるため、メモリも節約できます。Transformer の学習において、AdamW に匹敵する品質でより速い収束を実現することが示されています。特に Attention や MLP の重み行列に適用され、埋め込み層や出力層には引き続き AdamW が使われるのが一般的です。

$$G_t = \text{NesterovMomentum}(\nabla \mathcal{L})$$
$$U_t = \text{NewtonSchulz}(G_t) \approx G_t (G_t^T G_t)^{-1/2}$$
$$W \leftarrow W - \eta \, U_t$$

- Newton-Schulz 反復は、$X_{k+1} = \frac{1}{2} X_k (3I - X_k^T X_k)$ を数回（通常5〜10回）繰り返すことで計算されます。これにより、完全な SVD（特異値分解）を行うコストを避けつつ、良好な近似を得ることができます。

![Muonの直交化: モーメンタムによる更新は特異値が偏っていますが、Newton-Schulz反復はそれらを均等化し、すべての方向に一様に更新を適用します](../../images/optimizer_muon.svg)

![オプティマイザのメモリ比較: 各オプティマイザがパラメータごとに保存する情報](../../images/optimizer_memory.svg)

- MSE や BCE 以外にも、いくつかの**損失関数 (loss functions)** がよく使われます。

- **平均絶対誤差 (Mean Absolute Error: MAE)**（L1損失）は、差の絶対値の平均をとります：$\frac{1}{n}\sum|y_i - \hat{y}_i|$。大きなエラーを2乗しないため、MSEよりも外れ値に対して堅牢（ロバスト）です。

- **Huber損失** は、両者のいいとこ取りをしたものです。エラーが小さいときはMSEのように振る舞い（滑らかで最適化しやすい）、エラーが大きいときはMAEのように振る舞います（外れ値に強い）。切り替わりのポイントは閾値 $\delta$ で制御されます。

- **カテゴリカル交差エントロピー (Categorical cross-entropy: CCE)** は、BCEを多クラスに一般化したものです。正解クラスを $c$、その予測確率を $\hat{y}_c$ とすると：

$$\mathcal{L} = -\log(\hat{y}_c)$$

- これは正解クラスの負の対数尤度そのものです。交差エントロピーの最小化は尤度の最大化と同義であり、第5章の情報理論とも繋がっています：交差エントロピーは、真の分布の代わりに予測分布を使用することによる「追加のビット数」を測定しているのです。

- **ヒンジ損失 (Hinge loss)** は SVM で使用されます：$\mathcal{L} = \max(0, 1 - y \cdot f(x))$。マージンの誤った側にある点や、マージン内にある点にのみペナルティを課します。十分に自信を持って正しく分類された点については、損失はゼロになります。

- **正則化 (Regularisation)** は、複雑すぎるモデルにペナルティを加えることで、過学習を防ぎます。正則化された損失は次のようになります。

$$\mathcal{L}_{\text{reg}} = \mathcal{L}_{\text{data}} + \lambda \, R(w)$$

- **L2正則化**（リッジ、重み減衰）は、重みの2乗和をペナルティとします：$R(w) = \|w\|^2 = \sum w_i^2$。特定の重みが極端に大きくなることを防ぎ、すべての重みをゼロの方向へと引き寄せますが、完全にゼロにすることは稀です。

- **L1正則化**（ラッソ）は、重みの絶対値の和をペナルティとします：$R(w) = \|w\|_1 = \sum |w_i|$。重みをスパース（疎）にする性質があり、多くの重みを完全にゼロにします。これによって自動的な特徴量選択が行われます。

- **Elastic Net** はこれらを組み合わせたものです：$R(w) = \alpha \|w\|_1 + (1 - \alpha) \|w\|^2$。スパース性と重みの縮小をブレンドします。

- これには美しいベイズ的な解釈があります（第5章）。L2正則化は、重みにガウス分布の事前分布を置いてMAP推定を行うことと等価です。L1正則化は、ラプラス分布の事前分布に相当します。正則化の強さ $\lambda$ は、データに対してどれだけ事前分布を信じるかをコントロールしています。

- **評価指標 (Evaluation metrics)** は、モデルが実際に機能しているかを教えてくれます。回帰では MSE や MAE が標準的ですが、分類ではさらに細かな視点が必要です。

- **混同行列 (Confusion matrix)** は、二値分類における4つのカウントをまとめた表です。
  - 真陽性 (TP: True Positive): 正解も予測もポジティブ
  - 偽陽性 (FP: False Positive): 予測はポジティブだが、実際はネガティブ
  - 真陰性 (TN: True Negative): 正解も予測もネガティブ
  - 偽陰性 (FN: False Negative): 予測はネガティブだが、実際はポジティブ

- **正解率 (Accuracy)** = $\frac{TP + TN}{TP + TN + FP + FN}$ は、クラスのバランスが悪い場合には誤解を招くことがあります。メールの99%がスパムでない場合、常に「スパムでない」と答えるモデルは正解率99%ですが、全く役に立ちません。

- **適合率 (Precision)** = $\frac{TP}{TP + FP}$ は、「ポジティブと予測したものの中で、実際にポジティブだったのはどれくらいか？」に答えます。高い適合率は、誤報（オオカミ少年）が少ないことを意味します。

- **再現率 (Recall: 感度)** = $\frac{TP}{TP + FN}$ は、「実際のポジティブの中で、どれだけ見つけることができたか？」に答えます。高い再現率は、見逃しが少ないことを意味します。

- **F1スコア** = $\frac{2 \cdot \text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}$ は、適合率と再現率の調和平均であり、両者のバランスを評価します。

- **ROC曲線** は、分類の閾値を0から1まで変化させたときの、真陽性率（再現率）と偽陽性率 ($\frac{FP}{FP + TN}$) をプロットしたものです。完璧な分類器は左上の角に張り付きます。**AUC** (ROC曲線の下側の面積) は、これを1つの数値で表したもので、1.0が完璧、0.5はランダムな予測を意味します。

- **交差検証 (Cross-validation)** は、汎化性能のより信頼できる推定値を提供します。**k-分割交差検証** では、データを $k$ 個に分割し、そのうち $k-1$ 個で訓練し、残りの1個でテストします。これを入れ替えながら繰り返し、全 $k$ 回の平均テストスコアを算出します。すべてのデータを訓練とテストの両方に活用できるため、データが限られている場合に特に有効です。

- **バイアス・バリアンスのトレードオフ**（第4章）は、機械学習における根本的な課題です。モデルの期待される誤差は、次の3つに分解されます。

$$\text{誤差} = \text{バイアス}^2 + \text{バリアンス} + \text{削減不可能なノイズ}$$

- **バイアス** は誤った仮定による体系的な誤差です（曲がったデータに直線でフィットさせるなど）。**バリアンス** は訓練データの変動に対する敏感さです（ノイズにまでフィットしてしまう高次の多項式など）。単純すぎるモデルは高バイアス・低バリアンスになり、複雑すぎるモデルは低バイアス・高バリアンスになります。最適なポイントを見極めて、トータルの誤差を最小化する必要があります。

- **学習率スケジューリング (Learning rate scheduling)** は、訓練中に $\eta$ を調整します。
  - ステップ減衰：$N$ エポックごとに $\eta$ を一定倍（例：0.1倍）にする。
  - コサインアニーリング：コサイン曲線に沿って、初期値からゼロ付近まで滑らかに減少させる。
  - ウォームアップ：最初の数千ステップで非常に小さな $\eta$ から始めて線形に増加させ、その後減衰させる。これは初期の大きな勾配で学習が不安定になるのを防ぎます。
  - 1cycleポリシー：コサイン曲線で一度上げ、その後下げる手法。より速い収束が期待できます。

- **ハイパーパラメータチューニング** は、学習率、バッチサイズ、正則化の強さなど、勾配降下法では直接学べない値を最適化するプロセスです。
  - グリッドサーチ：事前に定義したグリッド上のすべての組み合わせを試す（網羅的だがコストがかかる）。
  - ランダムサーチ：ランダムに組み合わせを選んで試す。重要でないハイパーパラメータにリソースを割かずに済むため、グリッドサーチより効率的なことが多いです。
  - ベイズ最適化：目的関数のモデルを自ら構築し、次にどのハイパーパラメータを試すべきかを賢く決定します。
  - **ASHA (Asynchronous Successive Halving Algorithm)**: 多数の試行を並列かつ低予算（エポック数）で走らせ、見込みのあるものだけに追加予算（さらなるエポック数）を与え、ダメなものは早期終了させます。100回フルで回す代わりに、安く100回始めて、勝ち残った一握りだけを最後まで完走させるもので、Ray Tune などの現代の大規模チューニングのバックボーンとなっています。

- **スケジュライフリー学習 (Schedule-free learning)** は、学習率スケジュールの必要性そのものを排除しようとするアプローチです。$\eta$ を固定のカーブで減衰させる代わりに、2つの系列を維持します。一つは緩やかに変化する平均の系列 $z_t$（これが最適解に収束します）、もう一つは高速に探索する系列 $y_t$（ここで勾配が計算されます）です。最終的に平均化された系列を出力することで、事後的に決定される最適なスケジュールに匹敵する収束速度を理論的に保証します。これによりスケジュールをハイパーパラメータとして考慮する必要がなくなり、ベースとなる学習率を設定するだけでよくなります。SGD や Adam のスケジュライフリー版は、調整済みのスケジュールと同等以上の性能を示すことが確認されています。

## コーディングタスク (CoLab または notebook を使用)

1. 線形回帰を、正規方程式と勾配降下法の両方で実装してください。両者の解を比較し、勾配降下法における損失が反復とともに収束していく様子をプロットしてください。

```python {cmd=true}
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt

# 合成データの生成: y = 3x + 2 + ノイズ
key = jax.random.PRNGKey(42)
n = 100
X = jax.random.uniform(key, (n, 1), minval=0, maxval=10)
y = 3 * X[:, 0] + 2 + jax.random.normal(key, (n,)) * 1.5

# バイアス項（定数項）の列を追加
X_b = jnp.column_stack([X, jnp.ones(n)])

# 正規方程式による解析解
w_exact = jnp.linalg.solve(X_b.T @ X_b, X_b.T @ y)
print(f"正規方程式: w={w_exact[0]:.4f}, b={w_exact[1]:.4f}")

# 勾配降下法
w_gd = jnp.zeros(2)
lr = 0.005
losses = []
for step in range(500):
    pred = X_b @ w_gd
    error = pred - y
    loss = jnp.mean(error ** 2)
    losses.append(float(loss))
    grad = (2 / n) * X_b.T @ error
    w_gd = w_gd - lr * grad

print(f"勾配降下法: w={w_gd[0]:.4f}, b={w_gd[1]:.4f}")

fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].scatter(X[:, 0], y, s=15, alpha=0.5, color='#3498db')
axes[0].plot([0, 10], [w_exact[1], w_exact[0]*10 + w_exact[1]], color='#e74c3c', linewidth=2)
axes[0].set_title("線形回帰のフィット結果")
axes[0].set_xlabel("x"); axes[0].set_ylabel("y")

axes[1].plot(losses, color='#27ae60', linewidth=1.5)
axes[1].set_title("勾配降下法の損失収束")
axes[1].set_xlabel("Step"); axes[1].set_ylabel("MSE")
axes[1].set_yscale('log')
plt.tight_layout()
plt.show()
```

2. 勾配降下法を用いたロジスティック回帰をスクラッチで実装してください。2Dデータセットで訓練し、学習された決定境界を可視化してください。

```python {cmd=true}
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons

# データの生成
X, y = make_moons(n_samples=300, noise=0.2, random_state=42)
X, y = jnp.array(X), jnp.array(y, dtype=jnp.float32)

def sigmoid(z):
    return 1 / (1 + jnp.exp(-z))

# バイアス列を追加
X_b = jnp.column_stack([X, jnp.ones(len(X))])
w = jnp.zeros(3)
lr = 0.5
losses = []

for step in range(2000):
    z = X_b @ w
    pred = sigmoid(z)
    # BCE損失
    loss = -jnp.mean(y * jnp.log(pred + 1e-8) + (1 - y) * jnp.log(1 - pred + 1e-8))
    losses.append(float(loss))
    # 勾配の計算
    grad = X_b.T @ (pred - y) / len(y)
    w = w - lr * grad

# 決定境界の可視化
xx, yy = jnp.meshgrid(jnp.linspace(-2, 3, 200), jnp.linspace(-1.5, 2, 200))
grid = jnp.column_stack([xx.ravel(), yy.ravel(), jnp.ones(xx.size)])
zz = sigmoid(grid @ w).reshape(xx.shape)

plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, zz, levels=[0, 0.5, 1], alpha=0.3, colors=['#e74c3c', '#3498db'])
plt.contour(xx, yy, zz, levels=[0.5], colors='#9b59b6', linewidths=2)
plt.scatter(X[y==0, 0], X[y==0, 1], c='#e74c3c', s=15, label='クラス 0')
plt.scatter(X[y==1, 0], X[y==1, 1], c='#3498db', s=15, label='クラス 1')
plt.title("ロジスティック回帰の決定境界")
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

3. 2次元の二次関数曲面における複数のオプティマイザの軌跡を比較してください。同じ開始点から SGD、Momentum、Adam を実行し、その経路をプロットしてください。

```python {cmd=true}
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt

# 縦に長い二次関数: L(w1, w2) = 0.5*w1^2 + 10*w2^2
def loss_fn(w):
    return 0.5 * w[0]**2 + 10 * w[1]**2

grad_fn = jax.grad(loss_fn)

def run_sgd(w0, lr=0.05, steps=80):
    w = w0.copy()
    path = [w.copy()]
    for _ in range(steps):
        g = grad_fn(w)
        w = w - lr * g
        path.append(w.copy())
    return jnp.stack(path)

def run_momentum(w0, lr=0.05, beta=0.9, steps=80):
    w, v = w0.copy(), jnp.zeros(2)
    path = [w.copy()]
    for _ in range(steps):
        g = grad_fn(w)
        v = beta * v + (1 - beta) * g
        w = w - lr * v
        path.append(w.copy())
    return jnp.stack(path)

def run_adam(w0, lr=0.05, b1=0.9, b2=0.999, eps=1e-8, steps=80):
    w, m, v = w0.copy(), jnp.zeros(2), jnp.zeros(2)
    path = [w.copy()]
    for t in range(1, steps + 1):
        g = grad_fn(w)
        m = b1 * m + (1 - b1) * g
        v = b2 * v + (1 - b2) * g**2
        m_hat = m / (1 - b1**t)
        v_hat = v / (1 - b2**t)
        w = w - lr * m_hat / (jnp.sqrt(v_hat) + eps)
        path.append(w.copy())
    return jnp.stack(path)

w0 = jnp.array([8.0, 3.0])
sgd_path = run_sgd(w0)
mom_path = run_momentum(w0)
adam_path = run_adam(w0)

# プロット
fig, ax = plt.subplots(figsize=(8, 6))
w1 = jnp.linspace(-10, 10, 100)
w2 = jnp.linspace(-4, 4, 100)
W1, W2 = jnp.meshgrid(w1, w2)
L = 0.5 * W1**2 + 10 * W2**2
ax.contour(W1, W2, L, levels=20, cmap='Greys', alpha=0.4)
ax.plot(sgd_path[:,0], sgd_path[:,1], 'o-', color='#3498db', markersize=2, linewidth=1, label='SGD')
ax.plot(mom_path[:,0], mom_path[:,1], 'o-', color='#27ae60', markersize=2, linewidth=1, label='Momentum')
ax.plot(adam_path[:,0], adam_path[:,1], 'o-', color='#e74c3c', markersize=2, linewidth=1, label='Adam')
ax.plot(0, 0, 'k*', markersize=15, label='最小値')
ax.set_xlabel('w₁'); ax.set_ylabel('w₂')
ax.set_title("二次関数曲面におけるオプティマイザの軌道比較")
ax.legend()
plt.grid(alpha=0.3)
plt.show()
```

4. 重みのスパース性に対する L1 正則化と L2 正則化の効果を示してください。両方のペナルティで線形回帰を訓練し、得られた重みベクトルを比較してください。

```python {cmd=true}
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt

# 合成データ: 20ある特徴量のうち、最初の3つだけが有効
key = jax.random.PRNGKey(0)
n, d = 200, 20
w_true = jnp.zeros(d).at[:3].set(jnp.array([3.0, -2.0, 1.5]))
X = jax.random.normal(key, (n, d))
y = X @ w_true + 0.5 * jax.random.normal(key, (n,))

def train_ridge(X, y, lam=1.0, lr=0.01, steps=2000):
    """勾配降下法による L2 正則化（リッジ回帰）"""
    w = jnp.zeros(X.shape[1])
    for _ in range(steps):
        pred = X @ w
        grad = (2/len(y)) * X.T @ (pred - y) + 2 * lam * w
        w = w - lr * grad
    return w

def train_lasso(X, y, lam=1.0, lr=0.01, steps=2000):
    """近接勾配降下法 (proximal GD) による L1 正則化（ラッソ回帰）"""
    w = jnp.zeros(X.shape[1])
    for _ in range(steps):
        pred = X @ w
        grad = (2/len(y)) * X.T @ (pred - y)
        w = w - lr * grad
        # ソフト閾値演算 (L1 に対する近接作用素)
        w = jnp.sign(w) * jnp.maximum(jnp.abs(w) - lr * lam, 0)
    return w

w_l2 = train_ridge(X, y, lam=0.1)
w_l1 = train_lasso(X, y, lam=0.1)

fig, axes = plt.subplots(1, 3, figsize=(14, 4))
axes[0].bar(range(d), w_true, color='#333', alpha=0.7)
axes[0].set_title("正解の重み"); axes[0].set_xlabel("特徴量")
axes[1].bar(range(d), w_l2, color='#3498db', alpha=0.7)
axes[1].set_title("L2 (Ridge): 全体を縮小"); axes[1].set_xlabel("特徴量")
axes[2].bar(range(d), w_l1, color='#e74c3c', alpha=0.7)
axes[2].set_title("L1 (Lasso): 不要分をゼロに"); axes[2].set_xlabel("特徴量")
plt.tight_layout()
plt.show()

print(f"L2 非ゼロ重み数: {int(jnp.sum(jnp.abs(w_l2) > 0.01))}/{d}")
print(f"L1 非ゼロ重み数: {int(jnp.sum(jnp.abs(w_l1) > 0.01))}/{d}")
```
