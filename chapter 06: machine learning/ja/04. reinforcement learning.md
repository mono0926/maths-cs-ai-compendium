# 強化学習

- 教師あり学習にはラベル付きデータが必要です。教師なし学習はラベルのないデータからパターンを見つけます。**強化学習 (Reinforcement learning: RL)** はそのどちらとも異なります。エージェントが「環境」と対話し、「行動」を選択し、「報酬」を受け取ることによって学習します。正解ラベルは存在せず、エージェントは試行錯誤を通じて望ましい振る舞いを発見しなければなりません。

- 犬に新しい芸を教えることを想像してください。正解となる行動のデータセットを見せるわけではありません。代わりに犬が何かを試し、良い行動に対してはおやつをあげます。時間をかけて、犬はあなたが高い報酬（おやつ）をくれる行動が何であるかを理解します。強化学習はこのプロセスを定式化したものです。

- 強化学習の構成には5つの核となる要素があります。**エージェント**は学習者であり意思決定者です。**環境**はエージェント以外のすべてであり、相互作用の対象です。各時刻において、エージェントは**状態 (state)** $s_t$ を観測し、**行動 (action)** $a_t$ を選択し、**報酬 (reward)** $r_t$ を受け取り、新しい状態 $s_{t+1}$ へと遷移します。エージェントの目標は、時間の経過とともに獲得する報酬の総計を最大化することです。

![エージェントと環境のループ：エージェントが状態を観測し、行動を決定し、報酬を受け取る。環境は次の状態へ遷移する](../../images/mdp_agent_loop.svg)

- **方策 (Policy)** $\pi$ はエージェントの戦略であり、状態から行動へのマッピングです。決定的な方策は、1つの状態に対して1つの行動を導きます：$a = \pi(s)$。確率的な方策は、行動の確率分布を提供します：$\pi(a \mid s)$。強化学習の目的は、期待される累積報酬を最大化する「最適方策」を見つけることです。

- 強化学習の数学的な枠組みは**マルコフ決定過程 (Markov Decision Process: MDP)** であり、タプル $(S, A, P, R, \gamma)$ で定義されます：状態の集合 $S$、行動の集合 $A$、遷移確率 $P(s' \mid s, a)$、報酬関数 $R(s, a)$、および割引率 $\gamma$ です。

- **マルコフ性**（第5章参照）とは、未来は現在の状態にのみ依存し、そこに至るまでの履歴には依存しないという性質です：$P(s_{t+1} \mid s_t, a_t, s_{t-1}, \ldots) = P(s_{t+1} \mid s_t, a_t)$。これは、現在の状態が意思決定に必要なすべての情報を保持していることを意味します。

- **割引率 (Discount factor)** $\gamma \in [0, 1)$ は、エージェントが即時の報酬に対して将来の報酬をどれだけ重視するかを決定します。時刻 $t$ からの割引収益（return）は次のようになります。

$$G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots = \sum_{k=0}^{\infty} \gamma^k r_{t+k}$$

- $\gamma = 0$ の場合、エージェントは極めて短期的（近視眼的）になり、次の報酬のことしか考えません。$\gamma$ が1に近いほど、エージェントは長期的になります。また、割引率は（報酬が有限であれば）無限和が収束することを保証し、数学的な定義を可能にします。

- **価値関数 (Value functions)** は、ある状態にいること（あるいはある状態である行動をとること）がどれだけ「良い」かを推定します。**状態価値関数** $V^\pi(s)$ は、状態 $s$ で開始し、方策 $\pi$ に従った場合の期待収益です。

$$V^\pi(s) = \mathbb{E}_\pi \left[ G_t \mid s_t = s \right]$$

- **行動価値関数** $Q^\pi(s, a)$ は、状態 $s$ で開始し、行動 $a$ をとり、その後は方策 $\pi$ に従った場合の期待収益です。

$$Q^\pi(s, a) = \mathbb{E}_\pi \left[ G_t \mid s_t = s, a_t = a \right]$$

- 両者の関係：$V^\pi(s) = \sum_a \pi(a \mid s) \, Q^\pi(s, a)$。状態価値は、方策によって重み付けされた行動価値の平均です。

- **ベルマン方程式 (Bellman equation)** は、再帰的な関係を表します：ある状態の価値は、即時の報酬と、割引かれた次の状態の価値の和に等しいというものです。状態価値関数の場合：

$$V^\pi(s) = \sum_a \pi(a \mid s) \sum_{s'} P(s' \mid s, a) \left[ R(s, a) + \gamma \, V^\pi(s') \right]$$

- 最適価値関数 $V^{*}(s)$ においては、エージェントは常に最善の行動を選択します。

$$V^{*}(s) = \max_a \sum_{s'} P(s' \mid s, a) \left[ R(s, a) + \gamma \, V^{*}(s') \right]$$

- 同様に、$Q^{*}$ に対する**ベルマン最適方程式**は次の通りです。

$$Q^{*}(s, a) = \sum_{s'} P(s' \mid s, a) \left[ R(s, a) + \gamma \max_{a'} Q^{*}(s', a') \right]$$

- $Q^{*}$ さえ分かれば、最適方策は単純です：常に最も高い $Q$ 値を持つ行動を選択するだけです：$\pi^{*}(s) = \arg\max_a Q^{*}(s, a)$。

- **動的計画法 (Dynamic programming)** は、遷移確率と報酬（すなわち環境の完全なモデル）が分かっている場合にMDPを解く手法です。**方策評価**は、与えられた方策 $\pi$ に対してベルマン方程式を収束するまで繰り返し適用することで $V^\pi$ を計算します。**方策改善**は、算出された価値関数に基づいて、「より良い」方策（より貪欲な方策）を構築します：$\pi'(s) = \arg\max_a \sum_{s'} P(s' \mid s, a)[R(s,a) + \gamma V^\pi(s')]$。

- **方策反復法 (Policy iteration)** は、方策が変化しなくなるまで評価と改善を交互に繰り返します。これは最適方策に収束することが保証されています。

- **価値反復法 (Value iteration)** は、これら2つのステップを1つに統合します。ベルマン最適方程式を $V^{*}$ が収束するまで繰り返し適用し、最後に方策を抽出します。

$$V(s) \leftarrow \max_a \sum_{s'} P(s' \mid s, a) \left[ R(s, a) + \gamma \, V(s') \right]$$

- 動的計画法には $P(s' \mid s, a)$ の知識が必要ですが、これは現実には不可能なことが多いです。ほとんどの現実の問題では、エージェントは環境のダイナミクスを知らず、相互作用することしかできません。ここで**モデルフリー (model-free)** な手法が登場します。

- **時間的差分学習 (Temporal Difference (TD) learning)** は、モデルを知らなくても経験から学びます。核となるアイデアは**ブートストラップ (bootstrapping)** です。エピソードの終了を待って実際の収益 $G_t$ を計算する代わりに、現在の価値関数を使用して期待収益を推定します。

$$V(s_t) \leftarrow V(s_t) + \alpha \left[ r_t + \gamma \, V(s_{t+1}) - V(s_t) \right]$$

- ブラケット内の項は **TD誤差 (TD error)** と呼ばれます。これは **TDターゲット** ($r_t + \gamma V(s_{t+1})$) と現在の推定値 $V(s_t)$ との差です。TD誤差が正であれば、その状態は期待よりも良かったことを意味するため、価値を高めます。負であれば下げます。

![TDターゲットを示す状態遷移図：現在の価値、報酬、ブートストラップされた次の価値を用いた更新式](../../images/td_update.svg)

- TD学習は、エピソード全体を待たずともステップが進むごとに更新を行うため、モンテカルロ法よりもはるかに効率的です。また、エピソードの終わりがない継続的な環境でも動作します。

- **SARSA** (State-Action-Reward-State-Action) は、Q値にTD学習を適用したものです。エージェントは状態 $s$ で行動 $a$ をとり、報酬 $r$ と次の状態 $s'$ を観測し、さらに自分の方策に従って次の行動 $a'$ を選択します。

$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \, Q(s', a') - Q(s, a) \right]$$

- SARSA は**方策内 (on-policy)** 手法です：エージェントが実際に取った行動（探索を含む）に基づいて更新を行います。これにより SARSA はより保守的になり、自身の探索のノイズを考慮した方策を学習します。

- **Q学習 (Q-learning)** は、最も有名な強化学習アルゴリズムです。SARSA に似ていますが、エージェントが実際に取った行動ではなく、「あり得る最善の行動」を用いて更新を行います。

$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]$$

- Q学習は**方策外 (off-policy)** 手法です：どの方策に従っていたとしても、真に最適な $Q$ 値を直接学習しようとします。エージェントがランダムに探索している間でも、最適な行動価値を学習できます。これにより Q学習はより積極的（アグレッシブ）で収束が速い傾向がありますが、価値を過大評価する懸念もあります。

- **探索と利用 (Exploration vs exploitation)** は、強化学習の根本的なジレンマです。エージェントは、すでに知っている知識を利用すべきか（高い推定価値を持つ行動をとる）、あるいは未知の行動を探索し、より良い行動がないか探すべきかを選択しなければなりません。

- 最もシンプルな戦略は **$\epsilon$-greedy法** です：確率 $\epsilon$ でランダムな行動をとり（探索）、確率 $1-\epsilon$ で現在のベストな行動をとります（利用）。最初は大きな $\epsilon$ で始め、徐々に小さくしていくのが一般的です。

- 状態と行動のペアごとに表（Qテーブル）に値を保存する「テーブル形式」の手法は、状態空間が小さくて離散的な場合には機能します。しかし、状態空間が巨大または連続的な場合は、関数近似が必要になります。**Deep Q-Network (DQN)** は、ニューラルネットワークを使用して $Q(s, a; \theta)$（$\theta$ はネットワークの重み）を近似します。

- DQN は学習を安定させるための、2つの画期的な手法を導入しました。**経験再生 (Experience replay)**: 連続した遷移データ（これらは高い相関を持っています）から直接学ぶ代わりに、遷移データを一旦バッファに保存し、そこからランダムにミニバッチを取り出して訓練します。これにより相関を断ち切り、データを効率的に再利用できます。

- **ターゲットネットワーク (Target network)**: TDターゲットを計算するために、メインのネットワークとは別に、更新を遅らせたコピーを使用します。これがないと、メインのネットワークを更新するたびにターゲットも動いてしまい、「自分の尻尾を追いかける」ような不安定な状態になります。ターゲットネットワークは定期的に一括更新（ハードアップデート）されるか、微量ずつ更新（ソフトアップデート: $\theta^{-} \leftarrow \tau\theta + (1-\tau)\theta^{-}$）されます。

- DQN の損失関数は、単純に Q値の予測値と TDターゲットとの間の MSE です。

$$\mathcal{L}(\theta) = \mathbb{E} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^{-}) - Q(s, a; \theta) \right)^2 \right]$$

- これまでの手法はどれも価値関数を学習し、そこから方策を導き出すものでした。**方策勾配法 (Policy gradient)** は異なるアプローチを取ります：方策そのものをパラメータ化 $\pi(a \mid s; \theta)$ し、期待収益に対する勾配上昇法によって直接最適化します。

- **方策勾配定理**は、期待収益の勾配を方策パラメータに関して導出します。

$$\nabla_\theta J(\theta) = \mathbb{E}_\pi \left[ \nabla_\theta \log \pi(a \mid s; \theta) \cdot G_t \right]$$

- これは、「高い収益に繋がった行動の確率を上げ、低い収益に繋がった行動の確率を下げる」ことを意味します。対数確率の勾配が方策を変化させる方向を示し、$G_t$ がどれだけ変化させるかのスケール（重み）となります。

- **REINFORCE** は最も単純な方策勾配アルゴリズムです。1つのエピソードを実行し、各ステップの収益 $G_t$ を計算して更新します。

$$\theta \leftarrow \theta + \alpha \, \nabla_\theta \log \pi(a_t \mid s_t; \theta) \cdot G_t$$

- REINFORCE はバリアンス（分散）が大きいという課題があります。$G_t$ が1回のエピソードから得られるノイズの多いサンプルに過ぎないためです。これを解決するため、バイアスを導入せずに分散を抑える手法として、期待収益の平均値などを**ベースライン**として差し引くことが行われます。

$$\theta \leftarrow \theta + \alpha \, \nabla_\theta \log \pi(a_t \mid s_t; \theta) \cdot (G_t - b)$$

- **Actor-Critic** 手法は2つのネットワークを使用します。**Actor（アクター）** は方策 $\pi(a \mid s; \theta)$ であり、**Critic（クリティック）** はベースラインとして機能する価値関数 $V(s; \phi)$ です。収益の代わりにアドバンテージ $A_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ が使われます。

$$\theta \leftarrow \theta + \alpha \, \nabla_\theta \log \pi(a_t \mid s_t; \theta) \cdot A_t$$

- クリティックは、価値ベースの手法と同様に TD誤差を最小化するように更新されます。アクターは、クリティックのアドバンテージ推定値によって分散が抑えられた状態で、方策勾配を用いて更新されます。これは両方の手法の良いとこ取りをしたものです。

![アクター・クリティックの2ヘッド構成：アクターが行動確率を出力し、クリティックが価値推定値を出力する。アドバンテージ信号がアクターの更新を導く。](../../images/actor_critic.svg)

- **PPO** (Proximal Policy Optimization) は、実用上最も広く使われている方策勾配アルゴリズムです。これは、方策の更新が大きすぎると学習が致命的に崩壊するという問題に対処しています。

- PPO は**クリップされた代理目的関数 (clipped surrogate objective)** を使用します。新旧の方策の確率比を $r_t(\theta) = \frac{\pi(a_t | s_t; \theta)}{\pi(a_t | s_t; \theta_{\text{old}})}$ とすると、損失関数は次の通りです。

$$\mathcal{L}^{\text{CLIP}}(\theta) = \mathbb{E} \left[ \min\!\left( r_t(\theta) A_t, \; \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]$$

- クリップ操作（通常 $\epsilon = 0.2$）により、比率が1から離れすぎることを防ぎ、更新を小さく安定させます。アドバンテージが正（その行動が良かった）なら上限を設け、負（悪かった）なら下限を設けます。これは、以前の信頼領域手法 (TRPO) よりもシンプルで安定しています。

- PPO は、ChatGPT のようなモデルの学習（**RLHF**: 人間のフィードバックによる強化学習）にも使われています。RLHF では、人間の好みに基づいて訓練された「報酬モデル」を構築し、PPO を使って言語モデルの方策がその報酬を最大化するように最適化します。

- **DPO** (Direct Preference Optimization) は、報酬モデルを介さずに RLHF を簡略化する手法です。報酬モデルの訓練と後の強化学習を分ける代わりに、好みの比較データから直接方策を最適化する閉形式の損失関数を導出します。

$$\mathcal{L}_{\text{DPO}}(\theta) = -\mathbb{E} \left[ \log \sigma\!\left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right]$$

- ここで $y_w$ は好まれた（勝利した）回答、$y_l$ は好まれなかった（敗北した）回答です。DPO は好まれた回答の相対的な確率を高める手法で、PPO ベースの RLHF よりも実装がはるかに簡単です。

- 強化学習アルゴリズムにおける重要な分類を2つ。**方策内 (On-policy) vs 方策外 (Off-policy)**: 方策内手法 (SARSA, PPO) は、現在の最新の方策によって生成されたデータから学習します。方策外手法 (Q学習, DQN) は、過去の方策によって生成されたデータからも学習できます。方策外手法はサンプル効率に優れますが、不安定になることもあります。

- **モデルベース vs モデルフリー**: モデルフリー手法（ここまでの議論すべて）は、経験から直接価値や方策を学習します。モデルベース手法は、環境のモデル（$P$ や $R$）そのものを学習し、それを用いてシミュレーション（プランニング）を行います。モデルベースはサンプル効率が高いですが、正確なモデルを学習する難しさが伴います。

- 強化学習手法のまとめ：

| 手法         | タイプ           | キーアイデア              | 強み                 |
| ------------ | ---------------- | ------------------------- | -------------------- |
| 価値反復法   | DP, モデルベース | ベルマン最適性            | 小規なMDPでの厳密解  |
| SARSA        | TD, 方策内       | 方策内でのQ学習           | 保守的で安全         |
| Q学習        | TD, 方策外       | 最適Q値、貪欲ターゲット   | シンプルで効果的     |
| DQN          | 深層, 方策外     | 神経Q + 再生 + ターゲット | 高次元な状態への対応 |
| REINFORCE    | 方策勾配         | 対数確率の勾配 \* 収益    | 直接的な方策最適化   |
| Actor-Critic | PG + 価値        | アクター+クリティック     | 低分散で柔軟         |
| PPO          | PG, クリップ     | 信頼領域のような安定性    | 業界標準の手法       |
| DPO          | 直接的な好み     | 報酬モデルのスキップ      | シンプルなRLHF       |

## コーディングタスク (CoLab または notebook を使用)

1. シンプルなグリッドワールドに対して価値反復法を実装してください。最適価値関数を計算し、最適方策を抽出してください。それらをヒートマップと矢印プロットで視覚化してください。

```python {cmd=true}
import jax.numpy as jnp
import matplotlib.pyplot as plt

# 4x4 グリッドワールド: ゴールは (3,3)、報酬は各ステップ -1、ゴールで 0
grid_size = 4
gamma = 0.99
goal = (3, 3)

# 行動: 上, 下, 左, 右
actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]
action_names = ['up', 'down', 'left', 'right']
action_arrows = ['\u2191', '\u2193', '\u2190', '\u2192']

def step(s, a):
    """決定的な遷移"""
    ns = (max(0, min(grid_size-1, s[0]+a[0])),
          max(0, min(grid_size-1, s[1]+a[1])))
    return ns

# 価値反復法
V = jnp.zeros((grid_size, grid_size))
for iteration in range(100):
    V_new = jnp.array(V)
    for i in range(grid_size):
        for j in range(grid_size):
            if (i, j) == goal:
                continue
            values = []
            for a in actions:
                ns = step((i, j), a)
                values.append(-1 + gamma * float(V[ns[0], ns[1]]))
            V_new = V_new.at[i, j].set(max(values))
    if jnp.max(jnp.abs(V_new - V)) < 1e-6:
        print(f"{iteration+1} 回の反復で収束しました")
        break
    V = V_new

# 方策の抽出
policy = [['' for _ in range(grid_size)] for _ in range(grid_size)]
for i in range(grid_size):
    for j in range(grid_size):
        if (i, j) == goal:
            policy[i][j] = 'G'
            continue
        best_a = max(range(4), key=lambda a: -1 + gamma * float(V[step((i,j), actions[a])[0], step((i,j), actions[a])[1]]))
        policy[i][j] = action_arrows[best_a]

fig, axes = plt.subplots(1, 2, figsize=(10, 4))
im = axes[0].imshow(V, cmap='YlOrRd_r')
axes[0].set_title("最適価値関数")
for i in range(grid_size):
    for j in range(grid_size):
        axes[0].text(j, i, f"{V[i,j]:.1f}", ha='center', va='center', fontsize=10)
plt.colorbar(im, ax=axes[0])

axes[1].imshow(jnp.ones((grid_size, grid_size)), cmap='Greys', vmin=0, vmax=2)
axes[1].set_title("最適方策")
for i in range(grid_size):
    for j in range(grid_size):
        axes[1].text(j, i, policy[i][j], ha='center', va='center', fontsize=18)
plt.tight_layout(); plt.show()
```

2. シンプルなグリッドワールドでテーブル形式の Q学習を実装してください。エージェントを訓練し、学習曲線をプロットし、学習された Q値を表示してください。

```python {cmd=true}
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt

grid_size = 5
goal = (4, 4)
actions = [(-1,0), (1,0), (0,-1), (0,1)]

# Qテーブルの初期化
Q = {}
for i in range(grid_size):
    for j in range(grid_size):
        Q[(i,j)] = [0.0] * 4

alpha = 0.1
gamma = 0.95
epsilon = 1.0
epsilon_decay = 0.995
min_epsilon = 0.01

def step(s, a_idx):
    a = actions[a_idx]
    ns = (max(0, min(grid_size-1, s[0]+a[0])),
          max(0, min(grid_size-1, s[1]+a[1])))
    r = 0.0 if ns == goal else -1.0
    done = ns == goal
    return ns, r, done

key = jax.random.PRNGKey(42)
rewards_per_episode = []

for ep in range(500):
    s = (0, 0)
    total_reward = 0
    for _ in range(100):
        key, subkey = jax.random.split(key)
        if float(jax.random.uniform(subkey)) < epsilon:
            key, subkey = jax.random.split(key)
            a = int(jax.random.randint(subkey, (), 0, 4))
        else:
            a = max(range(4), key=lambda i: Q[s][i])

        ns, r, done = step(s, a)
        total_reward += r
        # Q学習の更新
        Q[s][a] += alpha * (r + gamma * max(Q[ns]) - Q[s][a])
        s = ns
        if done:
            break
    rewards_per_episode.append(total_reward)
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

plt.figure(figsize=(8, 4))
# 曲線の平滑化
window = 20
smoothed = [sum(rewards_per_episode[max(0,i-window):i+1])/min(i+1, window)
            for i in range(len(rewards_per_episode))]
plt.plot(smoothed, color='#3498db', linewidth=1.5)
plt.xlabel("エピソード"); plt.ylabel("合計報酬 (平滑化済み)")
plt.title("グリッドワールドにおける Q学習")
plt.grid(alpha=0.3); plt.show()

# 学習された方策の表示
arrow = ['\u2191', '\u2193', '\u2190', '\u2192']
print("学習された方策:")
for i in range(grid_size):
    row = ""
    for j in range(grid_size):
        if (i,j) == goal:
            row += " G "
        else:
            row += f" {arrow[max(range(4), key=lambda a: Q[(i,j)][a])]} "
    print(row)
```

3. 多腕バンディット問題に対して REINFORCE を実装してください。訓練を通じて、方策がどのように最適な腕を好むように進化するかを示してください。

```python {cmd=true}
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt

# 5本の腕があり、それぞれ期待報酬が異なる
true_rewards = jnp.array([0.2, 0.5, 0.8, 0.3, 0.1])
n_arms = len(true_rewards)

# 方策: ロジットに対する softmax
logits = jnp.zeros(n_arms)
lr = 0.1
key = jax.random.PRNGKey(42)

policy_history = []
reward_history = []

for step in range(2000):
    probs = jax.nn.softmax(logits)
    policy_history.append(probs)

    # 行動のサンプリング
    key, subkey = jax.random.split(key)
    action = jax.random.choice(subkey, n_arms, p=probs)

    # 報酬の獲得 (ベルヌーイ分布)
    key, subkey = jax.random.split(key)
    reward = float(jax.random.uniform(subkey) < true_rewards[action])
    reward_history.append(reward)

    # REINFORCE による更新
    # grad log pi(a) = e_a - probs (softmax のパラメータ化の場合)
    grad_log_pi = -probs.at[action].add(1.0)  # one-hot(a) - probs
    logits = logits + lr * reward * grad_log_pi

policy_history = jnp.stack(policy_history)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))
colors = ['#3498db', '#e74c3c', '#27ae60', '#9b59b6', '#f39c12']
for i in range(n_arms):
    axes[0].plot(policy_history[:, i], color=colors[i],
                 label=f'腕 {i} (正解={true_rewards[i]:.1f})', linewidth=1.5)
axes[0].set_xlabel("ステップ"); axes[0].set_ylabel("P(腕)")
axes[0].set_title("方策の進化 (REINFORCE)")
axes[0].legend(fontsize=8, loc='upper left'); axes[0].grid(alpha=0.3)

# 平滑化された報酬
window = 50
smoothed = [sum(reward_history[max(0,i-window):i+1])/min(i+1,window)
            for i in range(len(reward_history))]
axes[1].plot(smoothed, color='#27ae60', linewidth=1.5)
axes[1].axhline(y=0.8, color='#e74c3c', linestyle='--', alpha=0.5, label='最高の腕')
axes[1].set_xlabel("ステップ"); axes[1].set_ylabel("平均報酬")
axes[1].set_title("報酬の推移"); axes[1].legend()
axes[1].grid(alpha=0.3)
plt.tight_layout(); plt.show()
```
