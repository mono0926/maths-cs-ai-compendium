# 分散ディープラーニング

- 巨大なニューラルネットワークを単一の GPU で訓練しようとすると、いずれ限界に突き当たります。モデルがメモリに収まらなかったり、訓練に数ヶ月かかったりするからです。分散訓練（Distributed training）は、複数のデバイス（GPU、TPU、あるいは複数台のマシン）に作業を分散させることで、より高速に、そしてより巨大なモデルを訓練することを可能にします。このファイルでは、それを実現するための技術について解説します。

- なぜ分散化が重要なのかを理解するために、訓練の**計算コスト**から見ていきましょう。バッチサイズ $B$ のサンプルに対して、入力サイズ $d_{\text{in}}$、出力サイズ $d_{\text{out}}$ の全結合層を1回順伝播（Forward pass）させるには、約 $2 \cdot B \cdot d_{\text{in}} \cdot d_{\text{out}}$ 回の FLOPs（浮動小数点演算）が必要です（出力行列の各要素に対して乗算1回と加算1回）。逆伝播（Backward pass）のコストは順伝播の約2倍（入力に対する勾配と重みに対する勾配の両方を計算するため）かかるため、全結合層の1回の訓練ステップは約 $6 \cdot B \cdot d_{\text{in}} \cdot d_{\text{out}}$ FLOPs となります。

- 隠れ層の次元 $d$ を持つ Transformer 層の場合、自己注意（Self-attention）ブロックには4つの射影（Q, K, V, 出力）が含まれ、それぞれに $O(B \cdot n \cdot d^2)$ FLOPs（$n$ は系列長）かかります。さらに、注意行列の計算に $O(B \cdot n^2 \cdot d)$ かかります。フィードフォワード（FFN）ブロックには、通常 $d$ から $4d$ に拡大して戻る2つの全結合層があり、$O(B \cdot n \cdot 8d^2)$ かかります。層あたりの合計は、およそ $O(B \cdot n \cdot 12d^2 + B \cdot n^2 \cdot d)$ です。これに層の数を掛ければ、GPT 規模のモデルを訓練するために数千の GPU 時間が必要な理由が分かります。

- 計算コストよりも制約が厳しいのが**メモリの壁**です。訓練中、GPU メモリは以下の4つを同時に保持しなければなりません。

![訓練時のメモリ内訳を示す積み上げ棒グラフ：パラメータ、勾配、オプティマイザの状態、活性化値](../images/training_memory_breakdown.svg)

- **パラメータ (Parameters)**: モデルの重み。70億（7B）パラメータのモデルを FP32（1パラメータあたり4バイト）で保持すると、重みだけで 28 GB 必要です。
- **勾配 (Gradients)**: パラメータと同じサイズです。さらに 28 GB。
- **オプティマイザの状態 (Optimizer states)**: Adam は2つの追加バッファ（1次および2次モーメントの推定値）を保持し、それぞれパラメータと同じサイズです。これらは、モデルが低精度で動作していても数値的安定性のために FP32 で保持されます。7B モデルの場合、$2 \times 28 = 56$ GB になります。
- **活性化値 (Activations)**: 逆伝播で使用するために順伝播中に保存される中間値。サイズはバッチサイズ、系列長、モデルの幅に依存します。これはしばしば最大のコンポーネントとなり、バッチサイズに比例して増大します。

- FP32 の Adam を使用した 7B モデルの場合：28 (パラメータ) + 28 (勾配) + 56 (オプティマイザ) = 112 GB となり、活性化値を数えるまでもなく、単一の 80 GB A100 GPU には収まりません。これが分散戦略が必要不可欠な理由です。

- **混合精度訓練 (Mixed precision training)** はメモリ節約の第一歩です。すべてを FP32（32ビット浮動小数点）で保持する代わりに、順伝播と逆伝播には FP16 または BF16（16ビット）を使用し、オプティマイザの更新用に FP32 のマスターコピーを保持します。

- **FP16** は高い精度（10ビットの仮数部）を持ちますが範囲が狭く、オーバーフローやアンダーフローが発生しやすいです。ロススケーリング（Loss scaling：逆伝播の前に損失に大きな係数を掛け、後で勾配を同じ係数で割る）によってこれを緩和します。

- **BF16** (Brain Float) は FP32 と同じ指数範囲（8ビット）を持ちますが、精度は低くなります（7ビットの仮数部）。オーバーフローがほとんど発生せず、ロススケーリングもほぼ不要なため、扱いが非常に簡単です。現代の Transformer 訓練では BF16 が標準です。

- 混合精度によって、活性化値と勾配（順伝播・逆伝播における主要コスト）のメモリ消費量はほぼ半分になり、安定性のためにオプティマイザの状態のみを FP32 で保持します。

- **データ並列 (Data parallelism)** は最もシンプルな分散戦略です。モデル全体を $N$ 個の GPU にコピーし、ミニバッチを $N$ 個に分割して各 GPU に送ります。各 GPU は自身のチャンクに対して独立して順伝播と逆伝播を実行します。その後、勾配が全 GPU 間で平均化され（All-reduce 演算）、各 GPU が自身のモデルを更新します。

- モデル側から見ると、これはバッチサイズを $N$ 倍にして訓練するのと等価です。各 GPU がサイズ $B$ のバッチを処理する場合、実効バッチサイズは $N \cdot B$ となります。

![データ並列（モデルを複製しデータを分割）とモデル並列（モデルを分割しデータを共有）の比較図](../images/data_model_parallelism.svg)

- 勾配の平均化は同期または非同期で行われます。**同期 SGD (Synchronous SGD)** は全 GPU の処理完了を待ってから平均化するため、大バッチでの単一 GPU 訓練と数学的に等価です。欠点は、最も遅い GPU（ストラグラー）に全体の速度が引きずられることです。

- **非同期 SGD (Asynchronous SGD)** は、各 GPU が待機せずに共有のパラメータサーバーを独立して更新します。ストラグラー問題は回避できますが、「古い勾配（Stale gradients）」問題が発生します。ある GPU が少し古いパラメータに基づいて計算した勾配を適用することになり、ノイズが増えて収束が遅れる可能性があります。実用的には、効率的な通信を伴う同期 SGD が好まれます。

- **勾配蓄積 (Gradient accumulation)** は、限られたハードウェアで大バッチをシミュレートするための手法です。1つのミニバッチごとに更新するのではなく、複数回の順伝播・逆伝播を実行して勾配を蓄積してから更新を行います。これにより、活性化値のためのメモリを増やさずに（一度に1ミニバッチ分の活性化値だけがメモリに乗ればよいため）、大きなバッチサイズと同じ結果を得られます。

- モデル自体が単一の GPU に収まらない場合は、**モデル並列 (Model parallelism)** が必要になります。主に2つの形態があります。

- **テンソル並列 (Tensor parallelism)** は、個々の層を複数の GPU に分割します。例えば、行列演算 $Y = XW$ は列方向に分割できます：$W$ を $[W_1, W_2]$ として2つの GPU に配置し、$Y_1 = XW_1$ と $Y_2 = XW_2$ を並列計算して後で結合します。これはアテンションの射影層や FFN 層で有効です。層ごとに部分的な結果を結合する必要があるため、GPU 間（通常はノード内の NVLink）の高速な通信が必要です。

- **パイプライン並列 (Pipeline parallelism)** は、異なる層を異なる GPU に割り当てます。GPU 0 が層 1-4 を担当し、GPU 1 が層 5-8 を担当する、という具合です。データは組立ラインのようにパイプラインを流れます。単純な方法では「パイプライン・バブル（空き時間）」が生じます。GPU 0 がマイクロバッチ 1 の順伝播をしている間、GPU 1-3 は何もしないからです。これを解決するため、ミニバッチをさらに小さな**マイクロバッチ**に分割してパイプラインに流し込み、常に全 GPU が稼働するようにします。

- **ハイブリッド並列 (Hybrid parallelism)** は、データ並列、テンソル並列、パイプライン並列を組み合わせます。典型的な超大規模モデルのセットアップでは、ノード内（高速な NVLink で接続された 8 GPU）でテンソル並列を行い、ノード間でパイプライン並列、そしてノードグループ間でデータ並列を行うといった構成が取られます。GPT-4 や Llama などのモデルはこのように訓練されています。

- 分散訓練の効率は**通信**に大きく依存します。重要な演算は **All-reduce** です：$N$ 個の GPU それぞれが持っている値を合計（または平均）し、その結果を全 GPU に配布します。

- 未熟な All-reduce は、全データを1つの GPU に集めて合計し、それをブロードキャストします。これは通信量が $O(N)$ になり、ルートとなる GPU がボトルネックになります。

- **リング All-reduce (Ring All-reduce)** ははるかに効率的です。$N$ 個の GPU をリング状に配置します。各 GPU はデータを $N$ 個のチャンクに分割します。$N-1$ ステップかけて、各 GPU がチャンクを隣に送り、別の隣から受け取って部分和を蓄積していきます。さらに $N-1$ ステップで完成した合計値を全 GPU に伝播させます。各 GPU が転送するデータ量はデータサイズの $2(N-1)/N$ 倍であり、$N$ が大きくなると約 2倍に収束します。重要なのは、これが $N$ に依存して増大しないことで、帯域幅を最適に利用できます。

![リング状に配置された4つのGPUが勾配のチャンクを隣に渡し、全員が完全な合計値を持つまでの図](../images/ring_allreduce.svg)

- **パラメータサーバー (Parameter servers)** は、パラメータを保持する専用のサーバーノードを置くアーキテクチャです。ワーカーが勾配を計算してサーバーに送り、サーバーがパラメータを更新して送り返します。構成は単純ですが、サーバー側で通信のボトルネックが生じやすいです。

- **NCCL** (NVIDIA Collective Communications Library) は、GPU 間通信の標準ライブラリです。All-reduce、All-gather、ブロードキャストなどの集団通信操作を最適化して提供し、ネットワークトポロジーに合わせて最適なアルゴリズムを自動選択します。

- **スケーリング則 (Scaling laws)** は、モデルの性能が計算量、データ量、モデルサイズに対してどのように向上するかを記述したものです。Kaplan ら (2020) の初期のスケーリング則では、損失はそれぞれに対して冪乗則（べき乗則）に従って減少することが示されました。

$$L(N) \propto N^{-\alpha_N}, \quad L(D) \propto D^{-\alpha_D}, \quad L(C) \propto C^{-\alpha_C}$$

- ここで $N$ はパラメータ数、$D$ はデータセットサイズ、$C$ は計算予算です。

- **Chinchilla スケーリング則** (Hoffmann et al., 2022) は、従来の多くのモデルが「訓練不足」であったことを指摘しました。ある計算予算に対しては、従来考えられていたよりも「小さなモデル」を「より多くのデータ」で訓練すべきであるという結論です。最適な比率はパラメータあたり約20トークンです。7B モデルであれば、Llama 1 が 65B モデルで使った 300B トークンではなく、約 1400B（1.4T）トークン程度を学習させるべきです。この発見により、業界は「計算最適（Compute-optimal）」な訓練へとシフトしました。

- **混合エキスパート (Mixture of Experts: MoE)** は、計算量を比例して増やさずにモデル容量を拡張するアーキテクチャです。Transformer 層ごとに1つの FFN を持つ代わりに、$N$ 個の「エキスパート」ネットワーク（標準的な FFN）を持ちます。**ゲートネットワーク**（ルーター）が各トークンを調べ、上位 $K$ 個のエキスパート（通常 $K=1$ または $2$）に送ります。

![ゲートネットワークを通じて選択されたエキスパートにトークンがルーティングされ、出力が重み付け結合される MoE の図](../images/moe_routing.svg)

- パラメータ総数は非常に多くなりますが（$N$ 個のエキスパートがあるため）、トークンごとに活性化されるのは $K$ 個だけなので、トークンあたりの FLOPs はほぼ一定に保たれます。例えば Mixtral 8x7B は合計 47B パラメータを持ちますが、順伝播1回で使用されるのは約 13B だけであり、巨大なモデルの性能を比較的小さな計算コストで実現しています。

- MoE には課題もあります。**ロードバランシング**: ルーターが特定の1つのエキスパートにばかりトークンを送ってしまうと、他が無駄になります。補助的な損失関数を用いて均等なルーティングを促します。**通信**: 異なるエキスパートが異なる GPU に配置されている場合、トークンのルーティングに All-to-all 通信が必要になり、これが高コストになります。

- 数週間に及ぶ数千 GPU での訓練では、**フォールトトレランス (Fault tolerance)** が極めて重要です。GPU が1つ壊れただけで、すべての進捗を失うわけにはいきません。**チェックポインティング**は、モデルの重み、オプティマイザの状態、訓練状態（学習率、ステップ数、データの位置）を定期的にストレージに保存します。故障が発生した場合は、最新のチェックポイントから再開します。

- **勾配チェックポインティング (Gradient checkpointing / activation recomputation)** は、フォールトトレランスではなくメモリ最適化の手法です。順伝播ですべての活性化値を保存する代わりに、特定のチェックポイントの活性化値だけを保存します。逆伝播の際、足りない活性化値はそのチェックポイントから再計算します。これは計算とメモリのトレードオフです：計算量は約 33% 増えますが、活性化値のメモリを $\sqrt{L}$（$L$ は層数）程度に削減できます。

- まとめると、最先端のフロンティアモデルの訓練には、これらすべての技術が組み合わされています。BF16 混合精度、数千の GPU でのリング All-reduce を用いたデータ並列、ノード内のテンソル並列、ノード間のパイプライン並列、メモリ削減のための勾配チェックポインティング、パラメータ効率のための MoE、そして故障対策の定期的なチェックポインティングです。システムエンジニアリングは、アルゴリズム設計と同じくらい挑戦的な課題となっています。

- 分散訓練ツールキットのまとめ：

| 技術                       | 役割                           | トレードオフ                           |
| -------------------------- | ------------------------------ | -------------------------------------- |
| 混合精度 (BF16)            | 活性化値・勾配のメモリを半減   | わずかな数値的差異                     |
| データ並列                 | GPU 間でバッチサイズを拡張     | 勾配同期の通信オーバーヘッド           |
| テンソル並列               | GPU 間で層を分割               | 高速な相互接続が必要                   |
| パイプライン並列           | GPU 間で処理ステージを分割     | パイプライン・バブル（無駄な時間）     |
| 勾配蓄積                   | 仮想的に大バッチをシミュレート | 速度低下（複数回の Fwd/Bwd）           |
| 勾配チェックポインティング | 活性化値のメモリを削減         | 計算量が約33%増加                      |
| リング All-reduce          | 効率的な勾配平均化             | 巨大モデルでは帯域幅が制限になる       |
| MoE                        | 同一 FLOPs でモデル容量を増大  | ロードバランス、ルーティングの複雑さ   |
| スケーリング則             | 計算予算の割り当てガイド       | 経験則であり、常に成立するとは限らない |

## コーディングタスク (CoLab または notebook を使用)

1. Transformer 層の FLOPs とメモリ要件を計算してください。隠れ層の次元 $d$、系列長 $n$、バッチサイズ $B$、層の数が与えられたとき、総訓練コストを見積もってください。

```python
import jax.numpy as jnp

def transformer_layer_flops(d, n, B):
    """Transformer 層の1回の順伝播における概算 FLOPs"""
    # QKV 射影: 3 * (B * n * d * d) * 2 (乗算と加算)
    qkv_flops = 3 * 2 * B * n * d * d
    # アテンション: QK^T で (B * n * n * d) * 2、attn*V で (B * n * n * d) * 2
    attn_flops = 2 * 2 * B * n * n * d
    # 出力射影: (B * n * d * d) * 2
    out_flops = 2 * B * n * d * d
    # FFN: 2層, d->4d および 4d->d: 2 * (B * n * d * 4d) * 2
    ffn_flops = 2 * 2 * B * n * d * 4 * d
    return qkv_flops + attn_flops + out_flops + ffn_flops

def transformer_layer_memory(d, n, B, dtype_bytes=2):
    """1層あたりの活性化値メモリ（バイト）の概算"""
    # QKV: 3 * B * n * d
    qkv_mem = 3 * B * n * d * dtype_bytes
    # アテンション重み: B * heads * n * n (おおよそ B * n * n * sizeof)
    attn_mem = B * n * n * dtype_bytes
    # FFN 中間状態: B * n * 4d
    ffn_mem = B * n * 4 * d * dtype_bytes
    return qkv_mem + attn_mem + ffn_mem

# 例: GPT-2 スケール
d, n, B, L = 1024, 1024, 8, 24
fwd_flops = transformer_layer_flops(d, n, B)
total_flops = 3 * L * fwd_flops  # 順伝播 + 逆伝播（約2倍）で合計3倍
act_mem = L * transformer_layer_memory(d, n, B)
param_count = L * (12 * d * d + 13 * d)  # 概算

print(f"モデル: d={d}, n={n}, B={B}, L={L}")
print(f"パラメータ数: {param_count / 1e6:.0f}M")
print(f"1ステップあたりの FLOPs: {total_flops / 1e12:.2f} TFLOPs")
print(f"活性化値メモリ: {act_mem / 1e9:.2f} GB (BF16)")
print(f"パラメータメモリ (FP32): {param_count * 4 / 1e9:.2f} GB")
print(f"Adam オプティマイザメモリ: {param_count * 8 / 1e9:.2f} GB")
print(f"総訓練メモリ: {(param_count * 16 + act_mem) / 1e9:.2f} GB")
```

2. データ並列訓練をシミュレートしてください。データセットを複数の「仮想 GPU」に分割し、それぞれ勾配を計算して平均化し、その結果が単一 GPU での訓練と一致することを確認してください。

```python
import jax
import jax.numpy as jnp

# 単純な線形モデル: y = wx + b
key = jax.random.PRNGKey(0)
X = jax.random.normal(key, (64, 4))
w_true = jnp.array([1.0, -2.0, 3.0, 0.5])
y = X @ w_true + 0.1 * jax.random.normal(key, (64,))

def loss_fn(w, X, y):
    return jnp.mean((X @ w - y) ** 2)

grad_fn = jax.grad(loss_fn)

# 単一 GPU: フルバッチでの勾配
w = jnp.zeros(4)
grad_single = grad_fn(w, X, y)

# データ並列: 4つの「GPU」に分割
n_gpus = 4
chunk_size = len(X) // n_gpus
grads = []
for i in range(n_gpus):
    X_chunk = X[i*chunk_size:(i+1)*chunk_size]
    y_chunk = y[i*chunk_size:(i+1)*chunk_size]
    grads.append(grad_fn(w, X_chunk, y_chunk))

# All-reduce: 勾配の平均化
grad_parallel = jnp.mean(jnp.stack(grads), axis=0)

print("単一 GPU 勾配:", grad_single)
print("データ並列勾配 (平均):", grad_parallel)
print(f"一致するか: {jnp.allclose(grad_single, grad_parallel, atol=1e-5)}")

# 両方で訓練して比較
w_single, w_parallel = jnp.zeros(4), jnp.zeros(4)
lr = 0.1
for step in range(100):
    w_single = w_single - lr * grad_fn(w_single, X, y)

    grads = [grad_fn(w_parallel, X[i*chunk_size:(i+1)*chunk_size],
                     y[i*chunk_size:(i+1)*chunk_size]) for i in range(n_gpus)]
    avg_grad = jnp.mean(jnp.stack(grads), axis=0)
    w_parallel = w_parallel - lr * avg_grad

print(f"\n100ステップ後:")
print(f"単一 GPU の重み: {w_single}")
print(f"データ並列の重み: {w_parallel}")
print(f"最大の差: {jnp.max(jnp.abs(w_single - w_parallel)):.2e}")
```

3. シンプルな Mixture of Experts 層を実装してください。トークンを上位 K 個のエキスパートにルーティングするゲートネットワークを作成し、出力を組み合わせてください。

```python
import jax
import jax.numpy as jnp

def expert_fn(x, W1, b1, W2, b2):
    """シンプルな2層 FFN エキスパート"""
    h = jnp.maximum(0, x @ W1 + b1)  # ReLU
    return h @ W2 + b2

def moe_layer(x, gate_W, experts_params, top_k=2):
    """
    MoE 順伝播
    x: (batch, d_model)
    gate_W: (d_model, n_experts)
    experts_params: 各エキスパートの (W1, b1, W2, b2) リスト
    """
    n_experts = len(experts_params)

    # ゲート: ルーティングスコアの計算
    gate_logits = x @ gate_W  # (batch, n_experts)
    gate_probs = jax.nn.softmax(gate_logits, axis=-1)

    # Top-K 選択
    top_k_indices = jnp.argsort(-gate_probs, axis=-1)[:, :top_k]
    top_k_probs = jnp.take_along_axis(gate_probs, top_k_indices, axis=-1)
    # 再正規化
    top_k_probs = top_k_probs / jnp.sum(top_k_probs, axis=-1, keepdims=True)

    # 全エキスパートの出力を計算（簡略化のため全員計算し、後でマスク）
    expert_outputs = jnp.stack([
        expert_fn(x, *experts_params[i]) for i in range(n_experts)
    ], axis=1)  # (batch, n_experts, d_model)

    # Top-K に選ばれたエキスパートの出力を取得し、重み付け結合
    batch_idx = jnp.arange(x.shape[0])[:, None]
    selected_outputs = expert_outputs[batch_idx, top_k_indices]  # (batch, top_k, d_model)
    output = jnp.sum(selected_outputs * top_k_probs[:, :, None], axis=1)

    return output, gate_probs

# セットアップ
key = jax.random.PRNGKey(42)
batch, d_model, d_ff, n_experts = 8, 16, 32, 4

# エキスパートの初期化
experts_params = []
for i in range(n_experts):
    subkeys = jax.random.split(key, 3)
    k1, k2, key = subkeys[0], subkeys[1], subkeys[2]
    experts_params.append((
        jax.random.normal(k1, (d_model, d_ff)) * 0.1,
        jnp.zeros(d_ff),
        jax.random.normal(k2, (d_ff, d_model)) * 0.1,
        jnp.zeros(d_model),
    ))

key, subkey = jax.random.split(key)
gate_W = jax.random.normal(subkey, (d_model, n_experts)) * 0.1
x = jax.random.normal(key, (batch, d_model))

output, gate_probs = moe_layer(x, gate_W, experts_params, top_k=2)

print(f"入力形状: {x.shape}")
print(f"出力形状: {output.shape}")
print(f"ゲート確率 (最初のサンプル): {gate_probs[0]}")
print(f"エキスパートの使用率 (バッチ平均):")
for i in range(n_experts):
    usage = jnp.mean(gate_probs[:, i])
    print(f"  エキスパート {i}: {usage:.3f}")
```
