# ディープラーニング

- 何をもってネットワークは「ディープ（深い）」と言えるのでしょうか？浅いネットワークは隠れ層が1つですが、ディープネットワークは多くの層を持ちます。深さは、ネットワークが階層的な表現を構築することを可能にします。初期の層が単純な特徴（エッジや色調）を学習し、後の層がそれらを組み合わせて複雑な概念（顔や文章）を構成します。この構成性（compositionality）こそが、ディープラーニングの力の源泉です。

- 最も単純なディープネットワークは**多層パーセプトロン (MLP)** であり、全結合ネットワーク（fully connected / dense network）とも呼ばれます。各層は次のような計算を行います。

$$h = \sigma(Wx + b)$$

- ここで $W$ は重み行列（第2章）、$b$ はバイアスベクトル、$\sigma$ は非線形な活性化関数です。ある層の出力が次の層の入力になります。非線形性がなければ、層を積み重ねる意味はありません：$W_2(W_1 x) = (W_2 W_1)x$ となり、これは単なる1つの線形変換に過ぎないからです。これは第2章で学んだ「行列の積の崩壊」そのものです。

- **活性化関数 (Activation functions)** は、深さに意味を持たせるための非線形性を導入します。

- **ReLU** (Rectified Linear Unit): $\text{ReLU}(x) = \max(0, x)$。最も広く使われている活性化関数です。計算が速く、正の入力に対して飽和せず、スパース（疎）な活性化（多くのニューロンが正確にゼロを出力する）を生み出します。欠点は、負の入力に対しては常にゼロを出力することです。もし永続的に負の領域に留まってしまうと、そのニューロンは「死んで」しまい、学習が止まってしまいます。

- **シグモイド (Sigmoid)**: $\sigma(x) = \frac{1}{1+e^{-x}}$。入力を $(0, 1)$ の範囲に押し込めます。二値分類の出力層には有用ですが、隠れ層では問題があります。入力がゼロから遠いと勾配が消失してしまう（曲線がほぼ平坦になる）ためです。

- **Tanh**: $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$。入力を $(-1, 1)$ に押し込めます。シグモイドとは異なりゼロ中心であるため勾配の流れを助けますが、依然として極端な値での勾配消失問題を抱えています。

- **GELU** (Gaussian Error Linear Unit): $\text{GELU}(x) = x \cdot \Phi(x)$（$\Phi$ は標準正規分布の累積分布関数）。ReLU の滑らかな近似であり、小さな負の値を許容します。GPT や BERT におけるデフォルトの活性化関数です。

- **Swish**: $\text{Swish}(x) = x \cdot \sigma(x)$。もう一つの滑らかなゲート関数で、実用的には GELU と似ています。

![ReLU, Sigmoid, Tanh, GELU のプロットとその主な特性の比較](../images/activation_functions.svg)

- $d_{\text{in}}$ 個の入力と $d_{\text{out}}$ 個の出力を持つ全結合層は、$d_{\text{in}} \times d_{\text{out}} + d_{\text{out}}$ 個のパラメータ（重みとバイアス）を持ちます。行列演算 $Wx$ は、第2章で学んだ行列とベクトルの積そのものです。バッチ処理の場合、入力は $(B, d_{\text{in}})$ 形状の行列 $X$ となり、出力は $(B, d_{\text{out}})$ 形状の $XW^T + b$ となります。

- **普遍性近似定理 (Universal approximation theorem)** によれば、十分な数のニューロンを持つ1つの隠れ層があれば、コンパクトな定義域上の任意の連続関数を任意の精度で近似できます。これを聞くと「深さは重要ではない」ように思えるかもしれませんが、落とし穴は「十分な数」という点にあります。実際には、ディープネットワークは浅いネットワークよりも指数関数的に少ないパラメータ数で同じ関数を表現できます。深さは単なる表現力だけでなく、「効率」をもたらすのです。

- ネットワークが深くなるにつれ、勾配に関する2つの病理現象が現れます。**勾配消失 (Vanishing gradients)**: 勾配が（第3章の連鎖律によって）多くの層を通過する際、多くの係数が掛け合わされます。もしこれらの係数が一貫して1未満（シグモイドや tanh の飽和領域などで起こる）だと、勾配はゼロに向かって指数関数的に縮小します。その結果、初期の層がほとんど学習されなくなります。**勾配爆発 (Exploding gradients)**: 逆に係数が一貫して1より大きいと、勾配が指数関数的に増大し、数値的なオーバーフローや学習の不安定化を引き起こします。

- 勾配消失・爆発への対策：
  - ReLU や GELU 活性化関数の使用（正の入力に対して勾配が1であり、飽和しない）
  - 慎重な重みの初期化
  - 正規化層（Normalization layers）の使用
  - 残差接続（Residual connections / スキップ接続）
  - 勾配クリッピング（勾配爆発に対して）：勾配のノルムに上限を設ける

- **重みの初期化 (Weight initialisation)** は、学習開始時の活性化値と勾配のスケールを決定するため非常に重要です。重みが大きすぎると活性化が爆発し、小さすぎると消失します。

- **Xavier (Glorot) 初期化**: 重みを分散が $\frac{2}{d_{\text{in}} + d_{\text{out}}}$ となる分布から設定します。これにより、線形または tanh 活性化を前提として、各層の活性化値の分散をほぼ一定に保ちます。

- **He (Kaiming) 初期化**: 分散 $\frac{2}{d_{\text{in}}}$ を使用します。これは ReLU 活性化に合わせて調整されたもので、ReLU が活性化の半分をゼロにすることを考慮し、補償のために2倍の分散を必要とします。

- **正規化層 (Normalisation layers)** は、各層への入力が一定の統計量（おおよそ平均ゼロ、分散1）を持つようにすることで、学習を安定させます。

- **パッチ正規化 (Batch Normalisation: BatchNorm)** は、バッチ次元に沿って正規化します。ミニバッチ内の全サンプルについて、各チャネル/特徴量ごとに平均と分散を計算して正規化します。ネットワークが必要に応じて正規化を解除できるよう、学習可能なスケール ($\gamma$) とシフト ($\beta$) パラメータが追加されます。

$$\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}, \quad y = \gamma \hat{x} + \beta$$

- BatchNorm の問題点：バッチサイズに依存することです。非常に小さなバッチでは統計量が不安定になります。また、推論時にはバッチ統計量の代わりに移動平均を使用するため、訓練時と推論時で差異が生じます。

- **レイヤー正規化 (Layer Normalisation: LayerNorm)** は、個々のサンプルごとに特徴次元に沿って正規化します。バッチ内の他のサンプルに依存しないため、Transformer や回帰型ネットワーク（RNN）における標準的な選択肢となっています。

- **インスタンス正規化 (Instance Normalisation)** は、各サンプルおよび各チャネルごとに空間次元に沿って独立して正規化します。スタイル変換の分野で人気があります。

- **グループ正規化 (Group Normalisation)** は、チャネルをいくつかのグループに分割し、各グループ内で正規化します。LayerNorm と InstanceNorm の中間的な性質を持ちます。

![BatchNorm, LayerNorm, InstanceNorm がどの次元に沿って正規化を行うかを示す3Dテンソルの図](../images/normalization_types.svg)

- **ドロップアウト (Dropout)** は、訓練中にランダムにニューロンの $p$ の割合をゼロにする正則化手法です。これにより、ネットワークが特定のニューロンだけに頼らないようになり、冗長な表現の獲得を促します。テスト時にはすべてのニューロンが動作します。**インバート・ドロップアウト (Inverted dropout)** は、テスト時のスケーリングを不要にするため、訓練時に活性化値を $\frac{1}{1-p}$ 倍します。これが標準的な実装です。

- **畳み込みニューラルネットワーク (Convolutional Neural Networks: CNNs)** は、空間構造を活用します。全結合層のようにすべての入力をすべての出力に接続するのではなく、畳み込み層は小さなフィルタ（カーネル）を入力上でスライドさせ、各位置でドット積（内積）を計算します。同じフィルタの重みがすべての位置で共有されるため、パラメータ数を大幅に削減でき、平移不変性（translation invariance：位置がずれても同じものを認識できる性質）が組み込まれます。

- サイズ $k \times k$ のフィルタ $K$ を用いた2D入力に対する**畳み込み演算**:

$$(\text{input} * K)[i,j] = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} \text{input}[i+m, j+n] \cdot K[m, n]$$

![3x3のフィルタが入力グリッド上をスライドし、各位置で要素ごとの積和演算を行って出力特徴マップを作成する様子](../images/cnn_convolution.svg)

- 出力のサイズは3つのハイパーパラメータに依存します。**ストライド (Stride)** は、フィルタが一度に移動するピクセル数です（ストライド2は空間サイズを半分にします）。**パディング (Padding)** は、入力の周囲にゼロを追加します（"same" パディングはサイズを維持し、"valid" パディングはパディングを行いません）。出力サイズの公式：$\text{out} = \lfloor (\text{in} - k + 2p) / s \rfloor + 1$。

- **プーリング (Pooling)** 層は、特徴マップをダウンサンプリングします。最大プーリング (Max pooling) はウィンドウ内の最大値をとり、平均プーリング (Average pooling) は平均をとります。プーリングは、最も重要な情報を保持しつつ空間サイズを縮小します。

- **拡張畳み込み (Dilated convolutions / Atrous convolutions)** は、フィルタの要素間に隙間を挿入し、パラメータ数を増やさずに受容野（receptive field）を広げます。拡張率（dilation rate）が2の場合、3x3のフィルタで5x5の領域をカバーできます。

- **1x1 畳み込み** は 1x1 のフィルタによる畳み込みです。空間的な近傍は見ず、代わりにチャネル間で情報を混合します。これは、すべての空間位置に同じ全結合層を適用することと同じです。低コストでチャネル数を変更するために使われます。

- **スキップ接続 (Skip connections / 残差接続)** は、入力を1つ以上の層を飛び越えてバイパスさせます：$\text{output} = F(x) + x$。この層は残差 $F(x) = \text{output} - x$（差分）だけを学習すればよくなり、最適な変換が恒等写像に近い場合に学習が容易になります。ResNet (Residual Networks) はこのトリックを使って100層以上を積み重ね、深いネットワークが浅いネットワークよりも性能が悪くなる「劣化問題」を解決しました。

- CNN は**特徴の階層**を構築します。初期の層はエッジやテクスチャを検出し、中間の層はそれらを組み合わせてパーツ（目や車輪）を構成し、後半の層で物体全体を認識します。深くなるにつれて、各層の受容野（「見えている」入力の領域）は広がっていきます。

- **埋め込み (Embeddings)** は、離散的なトークン（単語、文字、アイテムIDなど）を密なベクトルにマップします。埋め込み層は単なるルックアップテーブルであり、(語彙数, 埋め込み次元) という形状の行列 $E$ です。トークン $i$ を引き出すことは、行列 $E$ の $i$ 行目を選択することと同じです。これはワンホットベクトルを掛けることと同義であり、第2章の行列とベクトルの積の特殊なケースです。埋め込みは訓練中に学習されるため、似たトークンは似たベクトルを持つようになります。

- **トークン化 (Tokenisation)** は、生のテキストをトークンのシーケンスに変換するプロセスです。単語単位のトークン化はスペースで区切りますが、未知語（語彙にない単語）を扱えません。**サブワードトークン化** (BPE, WordPiece, SentencePiece) は、テキストを頻出するサブワード単位に分解し、語彙サイズとカバー率のバランスをとります。「unhappiness」という単語は ["un", "happiness"] や ["un", "happ", "iness"] に分割されるかもしれません。

- **回帰型ニューラルネットワーク (Recurrent Neural Networks: RNNs)** は、隠れ状態（hidden state）を保持して情報を時間的に前へと運びながら、要素を一つずつ処理します。

$$h_t = \tanh(W_h h_{t-1} + W_x x_t + b)$$

- 隠れ状態 $h_t$ は、時刻 $t$ までにネットワークが見てきたすべての情報を圧縮した要約です。同じ重み $W_h$ と $W_x$ がすべてのタイムステップで共有されます（これは CNN が空間的な重みを共有するのと同様の「重み共有」です）。

- 単純な RNN は、長いシーケンスでは勾配消失問題に苦しみます。ステップ $t$ からステップ $t-k$ への勾配信号は、$W_h$ を $k$ 回掛け合わせる過程を通るため、指数関数的に縮小（または爆発）してしまいます。

- **LSTM** (Long Short-Term Memory) は、干渉を最小限に抑えて時間を流れる別の「セル状態」$c_t$ を導入することでこれを解決します。3つのゲートが情報の出入りと保持を制御します。

- **忘却ゲート (Forget gate)**: セル状態から何を消去するかを決定：$f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$
- **入力ゲート (Input gate)**: どのような新しい情報を書き込むかを決定：$i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$。候補値は $\tilde{c}_t = \tanh(W_c [h_{t-1}, x_t] + b_c)$
- セル状態の更新：$c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$
- **出力ゲート (Output gate)**: 何を外部にさらすかを決定：$o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)$。そして $h_t = o_t \odot \tanh(c_t)$

![忘却ゲート、入力ゲート、出力ゲート、およびセル状態のバイパス回路を示す LSTM セルの図](../images/rnn_lstm_cell.svg)

- セル状態は「コンベアベルト」のように機能します。忘却ゲートが1に近い限り、情報は多くのステップにわたって変化せずに流れることができ、長距離の依存関係における勾配消失問題を解決します。

- **GRU** (Gated Recurrent Unit) は、セル状態と隠れ状態を統合し、ゲートを3つから2つ（更新ゲートとリセットゲート）に減らすことで LSTM を簡略化したものです。パラメータ数が少なく、LSTM と同等の性能を示すことが多いです。

- RNN（LSTM を含む）の根本的な限界は、並列化ができない逐次処理にあります（トークン1の次にトークン2を処理しなければならない）。また、すべての文脈を固定サイズの隠れ状態に押し込めなければならないため、情報がボトルネックになります。

- **アテンション (Attention / 注意機構)** は、これら両方の問題を解決します。入力を一つの固定ベクトルに圧縮する代わりに、モデルが入力のすべての位置を振り返り、現在の出力にとってどれが関連しているかを判断できるようにします。

- 現代的な定式化では、**クエリ (Query: Q)、キー (Key: K)、バリュー (Value: V)** を使用します。これは図書館での検索に例えられます。クエリ（探しているもの）があり、各本にはキー（ラベル）が付いていて、中身（バリュー）があります。クエリと全キーを比較して、どのバリューを取り出すべきかを決めます。

- **スケールド・ドットプロダクト・アテンション (Scaled dot-product attention)**:

$$\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

- $QK^T$ ですべてのクエリとすべてのキーの間の類似度を計算します。これは行列積（第2章）であり、各要素はドット積、つまりコサイン類似度（第1章）を測定しています。$\sqrt{d_k}$ で割ることで、ドット積が大きくなりすぎるのを防ぎます（大きすぎると、softmax が飽和してしまい勾配が消失します）。softmax は類似度を確率分布に変換します。最後に $V$ を掛けることで、バリューの加重結合が出力されます。

- **マルチヘッド・アテンション (Multi-head attention)** は、$h$ 個の並列したアテンション操作を実行します。それぞれが Q, K, V の異なる学習済み投影を持ちます。これにより、モデルは異なる表現サブスペースからの情報を同時に捉えることができます。あるヘッドは構文的な関係に注目し、別のヘッドは意味的な関係に注目する、といったことが可能です。各ヘッドの出力は結合（concat）され、最後に投影されます。

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O$$

- **Transformer** アーキテクチャ (Vaswani et al., 2017) は、再帰を使わず、アテンション層とフィードフォワード層のみで構築されています。エンコーダブロックは「マルチヘッド自己注意、加算とレイヤー正規化、フィードフォワード、加算とレイヤー正規化」を繰り返します。デコーダブロックには、未来のトークンを見ないようにする「マスクされた自己注意」と、エンコーダの出力を見る「クロスアテンション」が追加されます。

![Transformer エンコーダブロック：マルチヘッドアテンション、加算とレイヤー正規化、フィードフォワード、加算とレイヤー正規化、および残差接続](../images/transformer_block.svg)

- **位置エンコーディング (Positional encoding)** は不可欠です。なぜならアテンションは「順列不変（置換不変）」であり、入力をシーケンスではなく単なる集合として扱うからです。位置情報がなければ、「the cat sat on the mat」と「the mat sat on the cat」を区別できません。オリジナルの Transformer は正弦波を用いた位置エンコーディングを使用しています。

$$PE_{(pos, 2i)} = \sin\!\left(\frac{pos}{10000^{2i/d}}\right), \quad PE_{(pos, 2i+1)} = \cos\!\left(\frac{pos}{10000^{2i/d}}\right)$$

- 各位置に固有のベクトルを与えることで、モデルは位置を区別できるようになります。現代のモデルでは、学習済みの位置埋め込みや相対位置エンコーディング (RoPE, ALiBi) が使われることも多いです。

- Transformer はすべてのトークンを並列に処理します（自己注意行列 $QK^T$ は一回の行列積で計算できます）。そのため、現代のハードウェアでは RNN よりもはるかに高速に訓練できます。トレードオフとして、自己注意の計算量はシーケンス長 $n$ に対して $O(n^2)$ です（全トークンが全トークンを見るため）。これに対し RNN は $O(n)$ です。長い文脈を扱うモデルでは、これを改善するために疎なアテンションや FlashAttention などの特殊なバリアントが必要になります。

- **Vision Transformers (ViT)** は、画像を固定サイズのパッチ（例：16x16）に分割し、各パッチをベクトルにフラット化し、それらをトークンのシーケンスとして扱うことで、Transformer を画像に適用します。分類には [CLS] トークンいう特別な学習可能トークンが先頭に追加され、その最終表現が使われます。畳み込み的な先天的バイアス（inductive bias）を持たないにもかかわらず、十分なデータがあれば ViT は CNN に匹敵、あるいはそれを凌駕します。

- **MLP-Mixer** はさらに単純なアーキテクチャで、アテンションも畳み込みも排除し、MLP だけで構築されています。空間位置にまたがって適用される「トークン混合 MLP」と、特徴量（チャネル）にまたがって適用される「チャネル混合 MLP」を交互に繰り返します。これが競争力のある性能を示すことは、現代的なアーキテクチャの鍵となる洞察がアテンションそのものではなく、「トークンと特徴の間で情報をいかに効率よく混合するか」にあることを示唆しています。

- **オートエンコーダ (Autoencoders)** は、入力を自分自身で再構成するように訓練することで、圧縮された表現を学習します。エンコーダが入力を低次元のボトルネック（潜在コード）にマップし、デコーダがそれを復元します。

$$z = f_{\text{enc}}(x), \quad \hat{x} = f_{\text{dec}}(z), \quad \mathcal{L} = \|x - \hat{x}\|^2$$

- ボトルネックを設けることで、ネットワークは最も重要な特徴だけを抽出せざるを得なくなります。次元削減、デノイジング（ノイズ入りの入力からクリーンな出力を復元）、異常検知（高い再構成誤差を異常のサインとする）などに使われます。

- **変分オートエンコーダ (Variational Autoencoders: VAEs)** はこれに確率的な要素を加えたものです。エンコーダは単一の点 $z$ を出力する代わりに、分布（ガウス分布の平均 $\mu$ と分散 $\sigma^2$）のパラメータを出力します。潜在コードは、この分布からサンプリングされます：$z = \mu + \sigma \odot \epsilon$（$\epsilon \sim \mathcal{N}(0, I)$）。この**再パラメータ化トリック (reparameterisation trick)** により、サンプリングを微分可能にし、勾配を逆伝播させることができます。

- VAE の損失関数は2つの項からなります。

$$\mathcal{L} = \underbrace{\|x - \hat{x}\|^2}_{\text{再構成誤差}} + \underbrace{D_{\text{KL}}(q(z|x) \| p(z))}_{\text{正則化項}}$$

- KLダイバージェンスの項（第5章）は、学習された事後分布 $q(z|x)$ を事前分布 $p(z) = \mathcal{N}(0, I)$ に近づけるよう強制し、潜在空間が滑らかで構造化されたものになるようにします。これにより、事前分布からサンプリングしてデコードすることで新しいデータを生成できるようになり、これが VAE が「生成モデル」と呼ばれる理由です。

## コーディングタスク (CoLab または notebook を使用)

1. JAX を使ってシンプルな MLP をスクラッチで構築してください。2Dの分類問題（同心円状のデータなど）で訓練し、その決定境界を可視化してください。

```python {cmd=true}
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles

# データの生成
X, y = make_circles(n_samples=500, noise=0.1, factor=0.5, random_state=42)
X, y = jnp.array(X), jnp.array(y, dtype=jnp.float32)

# 2層 MLP の初期化: 2 -> 16 -> 16 -> 1
def init_params(key):
    k1, k2, k3 = jax.random.split(key, 3)
    return {
        'W1': jax.random.normal(k1, (2, 16)) * 0.5,
        'b1': jnp.zeros(16),
        'W2': jax.random.normal(k2, (16, 16)) * 0.5,
        'b2': jnp.zeros(16),
        'W3': jax.random.normal(k3, (16, 1)) * 0.5,
        'b3': jnp.zeros(1),
    }

def forward(params, x):
    h = jnp.maximum(0, x @ params['W1'] + params['b1'])  # ReLU
    h = jnp.maximum(0, h @ params['W2'] + params['b2'])   # ReLU
    logit = (h @ params['W3'] + params['b3']).squeeze()
    return jax.nn.sigmoid(logit)

def loss_fn(params, X, y):
    pred = forward(params, X)
    return -jnp.mean(y * jnp.log(pred + 1e-7) + (1 - y) * jnp.log(1 - pred + 1e-7))

grad_fn = jax.jit(jax.grad(loss_fn))
params = init_params(jax.random.PRNGKey(0))
lr = 0.1

for step in range(2000):
    grads = grad_fn(params, X, y)
    params = {k: params[k] - lr * grads[k] for k in params}

# 決定境界のプロット
xx, yy = jnp.meshgrid(jnp.linspace(-2, 2, 200), jnp.linspace(-2, 2, 200))
grid = jnp.column_stack([xx.ravel(), yy.ravel()])
zz = forward(params, grid).reshape(xx.shape)

plt.figure(figsize=(7, 6))
plt.contourf(xx, yy, zz, levels=[0, 0.5, 1], alpha=0.3, colors=['#e74c3c', '#3498db'])
plt.scatter(X[y==0,0], X[y==0,1], c='#e74c3c', s=10, label='クラス 0')
plt.scatter(X[y==1,0], X[y==1,1], c='#3498db', s=10, label='クラス 1')
plt.title("同心円データセットにおける MLP の決定境界")
plt.legend(); plt.grid(alpha=0.3); plt.show()

acc = jnp.mean((forward(params, X) > 0.5) == y)
print(f"正解率 (Accuracy): {acc:.2%}")
```

2. 1D 畳み込みをスクラッチで実装してください。シンプルなエッジ検出フィルタを信号に適用し、ビルトインの `jnp.convolve` と結果を比較してください。

```python {cmd=true}
import jax.numpy as jnp
import matplotlib.pyplot as plt

def conv1d(signal, kernel):
    """1D 畳み込み (valid モード) をスクラッチで実装"""
    n, k = len(signal), len(kernel)
    output = jnp.zeros(n - k + 1)
    for i in range(n - k + 1):
        output = output.at[i].set(jnp.sum(signal[i:i+k] * kernel))
    return output

# ステップ関数を含む信号を作成
t = jnp.linspace(0, 4, 200)
signal = jnp.where(t < 1, 0.0, jnp.where(t < 2, 1.0, jnp.where(t < 3, 0.5, 1.5)))

# エッジ検出カーネル
edge_kernel = jnp.array([-1.0, 0.0, 1.0])

# 自作の実装 vs ビルトイン
our_output = conv1d(signal, edge_kernel)
jnp_output = jnp.convolve(signal, edge_kernel, mode='valid')

fig, axes = plt.subplots(3, 1, figsize=(10, 6), sharex=True)
axes[0].plot(t, signal, color='#3498db', linewidth=1.5)
axes[0].set_title("元の信号"); axes[0].set_ylabel("値")

axes[1].plot(t[:len(our_output)], our_output, color='#e74c3c', linewidth=1.5)
axes[1].set_title("エッジ検出後 (自作 conv1d)"); axes[1].set_ylabel("値")

axes[2].plot(t[:len(jnp_output)], jnp_output, color='#27ae60', linewidth=1.5, linestyle='--')
axes[2].set_title("エッジ検出後 (jnp.convolve)"); axes[2].set_ylabel("値")
axes[2].set_xlabel("t")

plt.tight_layout(); plt.show()
print(f"結果は一致するか: {jnp.allclose(our_output, jnp_output)}")
```

3. スケールド・ドットプロダクト・アテンションをスクラッチで実装してください。小規模な例に対してアテンションの重みを計算し、アテンション行列をヒートマップとして可視化してください。

```python {cmd=true}
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt

def scaled_dot_product_attention(Q, K, V):
    """スケールド・ドットプロダクト・アテンション"""
    d_k = Q.shape[-1]
    scores = Q @ K.T / jnp.sqrt(d_k)
    weights = jax.nn.softmax(scores, axis=-1)
    output = weights @ V
    return output, weights

# 例: トークン数 4, 埋め込み次元 8
key = jax.random.PRNGKey(42)
k1, k2, k3 = jax.random.split(key, 3)
seq_len, d_model = 4, 8

Q = jax.random.normal(k1, (seq_len, d_model))
K = jax.random.normal(k2, (seq_len, d_model))
V = jax.random.normal(k3, (seq_len, d_model))

output, weights = scaled_dot_product_attention(Q, K, V)

print(f"Q の形状: {Q.shape}")
print(f"アテンション重みの形状: {weights.shape}")
print(f"出力の形状: {output.shape}")
print(f"\nアテンション重み (行の合計が 1 になる):")
print(weights)
print(f"行ごとの合計: {weights.sum(axis=-1)}")

# アテンションの可視化
fig, ax = plt.subplots(figsize=(5, 4))
im = ax.imshow(weights, cmap='Blues', vmin=0, vmax=1)
ax.set_xlabel("キー (Key) の位置"); ax.set_ylabel("クエリ (Query) の位置")
ax.set_title("アテンションの重み")
tokens = ['tok 0', 'tok 1', 'tok 2', 'tok 3']
ax.set_xticks(range(4)); ax.set_xticklabels(tokens)
ax.set_yticks(range(4)); ax.set_yticklabels(tokens)
for i in range(4):
    for j in range(4):
        ax.text(j, i, f"{weights[i,j]:.2f}", ha='center', va='center', fontsize=10)
plt.colorbar(im); plt.tight_layout(); plt.show()
```

4. 2D データを 1D のボトルネック層を通して再構成するシンプルなオートエンコーダを構築してください。潜在空間（Latent space）と再構成されたデータを可視化してください。

```python {cmd=true}
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons

# データの生成
X, _ = make_moons(n_samples=500, noise=0.05, random_state=42)
X = jnp.array(X)

# オートエンコーダ: 2 -> 8 -> 1 -> 8 -> 2
def init_ae(key):
    k1, k2, k3, k4 = jax.random.split(key, 4)
    return {
        'enc_W1': jax.random.normal(k1, (2, 8)) * 0.5, 'enc_b1': jnp.zeros(8),
        'enc_W2': jax.random.normal(k2, (8, 1)) * 0.5, 'enc_b2': jnp.zeros(1),
        'dec_W1': jax.random.normal(k3, (1, 8)) * 0.5, 'dec_b1': jnp.zeros(8),
        'dec_W2': jax.random.normal(k4, (8, 2)) * 0.5, 'dec_b2': jnp.zeros(2),
    }

def encode(p, x):
    h = jnp.tanh(x @ p['enc_W1'] + p['enc_b1'])
    return h @ p['enc_W2'] + p['enc_b2']

def decode(p, z):
    h = jnp.tanh(z @ p['dec_W1'] + p['dec_b1'])
    return h @ p['dec_W2'] + p['dec_b2']

def ae_loss(p, X):
    z = encode(p, X)
    X_hat = decode(p, z)
    return jnp.mean((X - X_hat) ** 2)

grad_fn = jax.jit(jax.grad(ae_loss))
params = init_ae(jax.random.PRNGKey(0))
lr = 0.01

for step in range(3000):
    grads = grad_fn(params, X)
    params = {k: params[k] - lr * grads[k] for k in params}

z = encode(params, X)
X_hat = decode(params, z)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
axes[0].scatter(X[:,0], X[:,1], c=z.squeeze(), cmap='viridis', s=10)
axes[0].set_title("元のデータ (潜在コードで色分け)")
axes[1].scatter(X_hat[:,0], X_hat[:,1], c=z.squeeze(), cmap='viridis', s=10)
axes[1].set_title("1D ボトルネックからの再構成")
for ax in axes:
    ax.set_aspect('equal'); ax.grid(alpha=0.3)
plt.tight_layout(); plt.show()

print(f"再構成 MSE: {ae_loss(params, X):.4f}")
```
