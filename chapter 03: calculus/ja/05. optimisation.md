# 最適化 (Optimisation)

- ニューラルネットワークの学習、回帰直線のフィッティング、ハイパーパラメータの調整：ほとんどすべての機械学習アルゴリズムの核心にあるのは **最適化 (optimisation)** 問題です。

- 私たちは何らかの関数（損失、コスト、目的関数）を持っており、それをできるだけ小さく（あるいは大きく）する入力を求めています。

- 最適化を行う前に、関数の **ゼロ点 (zeros)**（または **根 (roots)**）について理解する必要があります。$f(x)$ のゼロ点とは、$f(x) = 0$ となる $x$ の値のことです。グラフ上では、これらは x 切片に相当します。

- 例えば、$f(x) = x^2 - 3x + 2 = (x-1)(x-2)$ は $x = 1$ と $x = 2$ にゼロ点を持ちます。ゼロ点の間では関数は負（$f(1.5) = -0.25$）であり、ゼロ点の外側では正になります。ゼロ点は数直線を関数が一定の符号を持つ領域に分割します。

- ゼロ点の **重複度 (multiplicity)** とは、対応する因数が何回現れるかを示します。

- 単純なゼロ点（重複度 1）では、グラフは x 軸を横切ります。二重のゼロ点（重複度 2）では、グラフは x 軸に接しますが、横切ることなく跳ね返ります。その点ではグラフが「平ら」に見えます。

- ゼロ点を見つけることが重要なのは、導関数 $f'(x)$ のゼロ点が元の関数 $f(x)$ の **臨界点 (critical points)** であり、極大値や極小値の候補になるからです。

- 極大値や極小値では接線が水平（傾き 0）になるため、$f'(x) = 0$ となります。

![臨界点：導関数がゼロになる場所。関数は山、谷、または鞍点を持つ](../images/critical_points.svg)

- しかし、すべての臨界点が極大値や極小値であるとは限りません。$f'(x) = 0$ となる点は、一時的に関数が平らになるものの方向は変わらない **変曲点 (inflection point)**（例：$f(x) = x^3$ の $x = 0$）である可能性もあります。

- **2階微分判定法 (second derivative test)** はこれを解決します。$f'(c) = 0$ となる臨界点 $x = c$ において：
  - $f''(c) > 0$: 曲線は下に凸（ボウル状）なので、$c$ は **極小値** です。
  - $f''(c) < 0$: 曲線は上に凸（丘状）なので、$c$ は **極大値** です。
  - $f''(c) = 0$: この判定法では結論が出ません。高階微分や他の方法が必要です。

- 例として $f(x) = x^3 - 3x$ を考えます。導関数は $f'(x) = 3x^2 - 3 = 3(x-1)(x+1)$ なので、臨界点は $x = -1$ と $x = 1$ です。2階導関数は $f''(x) = 6x$ です。$x = -1$ では $f''(-1) = -6 < 0$（極大）、$x = 1$ では $f''(1) = 6 > 0$（極小）となります。

- 関数のグラフ上の任意の2点を結ぶ線分が、グラフの上側（またはグラフ上）にあるとき、その関数は **凸 (convex)** であると言います。これはあらゆる場所で上向きに曲がっているボウル状の形をイメージしてください。数学的には、すべての $x$ に対して $f''(x) \geq 0$ であれば、$f$ は凸関数です。

![凸関数はユニークな最小値を持ち、非凸関数は多くの極小値を持つ可能性がある](../images/convex_nonconvex.svg)

- 凸性は非常に強力です。なぜなら、凸関数には「すべての極小値は **最小値 (global minimum)** でもある」という驚くべき性質があるからです。騙されて迷い込むような局所的な谷（ローカルミニマム）が存在しません。凸なボウルの中にボールを転がせば、必ず底に到達します。

- $-f$ が凸であるとき、その関数 $f$ は **凹 (concave)**（下向きに曲がっている）であると言います。関数が凹から凸へと切り替わる点は **変曲点** であり、$f''(x) = 0$ で発生します。

- **ニュートン法 (Newton's method)** は、接線を利用して関数のゼロ点（ひいては、導関数の臨界点）を見つけます。初期値 $x_0$ から始めて、反復的に値を更新します：

$$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$$

![ニュートン法：接線を辿って、根のより良い近似値を見つける](../images/newtons_method.svg)

- アイデア：$x_n$ において接線を引き、それが x 軸と交わる点を見つけます。その交点が $x_{n+1}$ になります。素性の良い関数に対して適切な初期値から始めれば、ニュートン法は非常に速く（2次収束、つまり正しい桁数がステップごとにほぼ倍増する）収束します。

- 例えば $\sqrt{5}$（$f(x) = x^2 - 5$ のゼロ点）を求める場合：$f'(x) = 2x$ なので、$x_{n+1} = x_n - \frac{x_n^2 - 5}{2x_n}$ となります。$x_0 = 2$ から始めると：$x_1 = 2.25, x_2 = 2.2361\ldots$ となり、すでに小数点以下4桁まで正確です。

- 初期値が根から遠すぎる場合、根の近くで $f'(x) = 0$ となる場合、または近くに変曲点がある場合、ニュートン法は失敗することがあります。また、微分を計算する必要があり、それが高コストになる場合もあります。

- 最適化（ゼロ点ではなく極小値を求める）の場合、ニュートン法を $f'(x) = 0$ に対して適用するため、更新式は次のようになります：

$$x_{n+1} = x_n - \frac{f'(x_n)}{f''(x_n)}$$

- 多次元の場合、これは $\mathbf{x}_{n+1} = \mathbf{x}_n - H^{-1} \nabla f(\mathbf{x}_n)$ となります。ここで $H$ はヘッセ行列です。これは、前のファイルで見た多変数の2次テイラー近似を実際に動かしているものです。つまり、関数を2次式で近似し、その2次式の最小値へジャンプすることを繰り返します。

- **ラグランジュの未定乗数法 (Lagrange multipliers)** は、**制約付き最適化 (constrained optimisation)** を解きます。制約 $g(x, y) = c$ の下で $f(x, y)$ の最適値を求めます。$\mathbb{R}^n$ 全体を探索する代わりに、制約が保持される集合（曲線や曲面）の上に制限されます。

- キーとなる幾何学的な洞察は、「制約付き最適点において、$f$ の勾配と $g$ の勾配は平行でなければならない」という点です。もし平行でなければ、制約に沿って移動することで $f$ をさらに改善できる方向が存在することになり、まだ最適点にはいないことになります。

- 新しい変数 $\lambda$（ラグランジュ乗数）を導入し、**ラグランジュ関数 (Lagrangian)** を定義します：

$$\mathcal{L}(x, y, \lambda) = f(x, y) - \lambda(g(x, y) - c)$$

- すべての偏微分をゼロに設定することで、制約付き最適値を解とする方程式系が得られます：

$$\frac{\partial \mathcal{L}}{\partial x} = 0, \quad \frac{\partial \mathcal{L}}{\partial y} = 0, \quad \frac{\partial \mathcal{L}}{\partial \lambda} = 0$$

![ラグランジュの未定乗数法：最適点において、f と g の勾配は平行である](../images/lagrange_multiplier.svg)

- 例として、$x^2 + y^2 = 1$ の制約下で $f(x,y) = x^2 y$ を最大化します。ラグランジュ関数は $\mathcal{L} = x^2 y - \lambda(x^2 + y^2 - 1)$ です。偏微分をとると：

$$2xy - 2\lambda x = 0, \quad x^2 - 2\lambda y = 0, \quad x^2 + y^2 = 1$$

- 第1式から（$x \neq 0$ と仮定）：$\lambda = y$。これを第2式に代入すると：$x^2 = 2y^2$。制約条件と組み合わせると：$2y^2 + y^2 = 1$ なので、$y = \frac{1}{\sqrt{3}}$。最大値は $f = \frac{2}{3\sqrt{3}}$ となります。

- 不等式制約（$= c$ ではなく $g(x,y) \leq c$）の場合、**Karush-Kuhn-Tucker (KKT) 条件** がラグランジュの未定乗数法を一般化します。制約が有効（境界上、等号が成立）か、無効（解が内部にあり制約は関係ない）かのいずれかになります。

- 実際には、手で最適化を行うことは稀です。主要なアルゴリズム群を以下に示します：
  - **1次手法 (First-order methods)**（勾配のみを使用）：勾配降下法、確率的勾配降下法 (SGD)、Adam。これらは1ステップあたりの計算コストが低いですが、特に悪条件の問題では収束が遅くなることがあります。

  - **2次手法 (Second-order methods)**（勾配とヘッセ行列を使用）：ニュートン法は高速に収束しますが、ヘッセ行列の計算と反転には非常にコストがかかります（パラメータの数 $n$ に対して $O(n^3)$）。**準ニュートン法 (Quasi-Newton methods)**（BFGS や L-BFGS など）は、勾配情報のみを使用してヘッセ行列を近似し、2次手法のフルコストをかけずに1次手法よりも速い収束を実現します。

  - **共役勾配法 (Conjugate gradient)**: 大規模な疎行列システムに対して効率的で、完全なヘッセ行列を保存する代わりに行列とベクトルの積のみを使用します。

  - **ガウス・ニュートン法 (Gauss-Newton)** および **レーベンバーグ・マルカート法 (Levenberg-Marquardt)**：最小二乗法（回帰で一般的）に特化した手法で、ヤコビ行列を介してヘッセ行列を近似します。

  - **自然勾配降下法 (Natural gradient descent)**: フィッシャー情報行列を使用してパラメータ空間の幾何学的構造を考慮します。これは、確率モデルに対してより効果的です。

- オプティマイザの選択は問題に依存します。ディープラーニングでは、パラメータ数が膨大（数百万から数千億）なため、ヘッセ行列の計算が現実的ではなく、1次手法（特に Adam）が主流です。目的関数が滑らかな小規模な問題では、2次手法が劇的に高速になる場合があります。

## コーディングタスク (CoLab または notebook を使用)

1. ニュートン法を実装して $\sqrt{7}$（$f(x) = x^2 - 7$ のゼロ点）を求めます。収束の速さを観察してください。

```python
import jax.numpy as jnp

f = lambda x: x**2 - 7
df = lambda x: 2*x

x = 3.0  # 初期値
for i in range(6):
    x = x - f(x) / df(x)
    print(f"step {i+1}: x = {x:.10f}  (誤差: {abs(x - jnp.sqrt(7.0)):.2e})")
```

2. 勾配降下法を使用して $f(x, y) = (x - 3)^2 + (y + 1)^2$ を最小化します。最小値は $(3, -1)$ です。異なる学習率で試してみてください。

```python
import jax
import jax.numpy as jnp

def f(params):
    x, y = params
    return (x - 3)**2 + (y + 1)**2

grad_f = jax.grad(f)
params = jnp.array([0.0, 0.0])
lr = 0.1

for i in range(20):
    g = grad_f(params)
    params = params - lr * g
    if i % 5 == 0 or i == 19:
        print(f"step {i:2d}: ({params[0]:.4f}, {params[1]:.4f})  loss={f(params):.6f}")
```

3. 制約付き最適化問題を数値的に解きます。$x + y = 10$ の制約の下で $f(x,y) = xy$ を最大化するため、$y = 10 - x$ とパラメータ化して1変数関数の最適値を求めます。

```python
import jax
import jax.numpy as jnp

# 制約を代入: y = 10 - x より f = x(10 - x) = 10x - x²
f = lambda x: x * (10 - x)
df = jax.grad(f)

# 勾配上昇法 (最大値を求めたいので、勾配を加算する)
x = 1.0
lr = 0.1
for i in range(20):
    x = x + lr * df(x)
print(f"x={x:.4f}, y={10-x:.4f}, f={f(x):.4f}")  # x=5, y=5, f=25.0 になるはずです
```
