# 多変数微積分学 (Multivariate Calculus)

- これまで見てきた関数は、単一の入力 $x$ を受け取り、単一の出力 $f(x)$ を生成するものでした。しかし、機械学習においては、単一の変数だけを扱うことはほとんどありません。

- 例えば $f(x, y) = x^2 + y^2$ のような2変数の関数を考えてみましょう。これは3次元空間において、ボウル状の曲面を定義します。ここで、$y$ を固定したまま $x$ を少しだけ動かした場合、$f$ はどのように変化するでしょうか？ これが **偏微分 (partial derivative)** です。

- $x$ に関する $f$ の **偏微分**（$\frac{\partial f}{\partial x}$ と書きます）は、他のすべての変数を定数として扱い、$x$ に関して通常通り微分します。

- $f(x, y) = x^2y + 3x - 2y$ の場合：

$$\frac{\partial f}{\partial x} = 2xy + 3 \qquad \frac{\partial f}{\partial y} = x^2 - 2$$

- $\frac{\partial f}{\partial x}$ を計算する際は、$y$ を定数として扱いました。そのため、$x^2y$ は $2xy$ に、$3x$ は $3$ に、$-2y$ は $0$ に微分されました。

- $\frac{\partial f}{\partial y}$ を計算する際は、$x$ を定数として扱いました。そのため、$x^2y$ は $x^2$ に、$3x$ は $0$ に、$-2y$ は $-2$ に微分されました。

- 幾何学的には、$x$ で偏微分することは、3次元の曲面を（特定の $y$ の値において）$xz$ 平面に平行な平面で切り、その断面に現れる曲線の傾きを求めることに相当します。

![偏微分：1つの変数を固定して曲面をスライスする](../images/partial_derivative.svg)

- **勾配 (gradient)** は、すべての偏微分を1つのベクトルにまとめたものです：

$$\nabla f = \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n}\right)$$

- $f(x, y) = x^2 + y^2$ の場合：$\nabla f(x, y) = (2x, 2y)$ です。点 $(1, 2)$ においては、$\nabla f(1, 2) = (2, 4)$ となります。

- 勾配には2つの重要な性質があります：
  - **方向**: 最も急激に増加する方向を指します。山の上にいる登山者を想像してください。彼らの位置における勾配は、最も急な登り坂、つまり山頂へ向かう最短の方向を指し示します。

  - **大きさ**: $\|\nabla f\|$ は、その最急上昇方向への増加率を表します。勾配が大きいということは地形が急であることを意味し、勾配が小さいということはほぼ平坦であることを意味します。

![勾配ベクトルは等高線に垂直で、山頂の方向を指す](../images/gradient_contour.svg)

- 勾配が山頂（増加）の方向を指すので、その反対方向（$-\nabla f$）に移動すると、値が小さくなる方向、つまり谷（減少）の方向へと向かいます。この単純なアイデアが、後の章で詳しく説明する最適化手法 **勾配降下法 (gradient descent)** の基礎となります。今のところ、勾配が「どちらが上か」と「どれだけ急か」を教えてくれるものだということを覚えておいてください。

- **方向微分 (directional derivative)** は偏微分を一般化したものです。「$x$ 軸に沿って $f$ がどう変化するか？」と問う代わりに、「任意の方向 $\mathbf{u}$ に沿って $f$ がどう変化するか？」を問います。これは勾配と単位ベクトルのドット積として計算されます：

$$D_{\mathbf{u}} f = \nabla f \cdot \mathbf{u}$$

- 点 $(1, 2)$ における $f(x, y) = x^2 + y^2$ について、$\mathbf{v} = (3, 4)$ の方向の変化を求める場合：まず $\mathbf{v}$ を正規化して $\mathbf{u} = (3/5, 4/5)$ を得ます。すると $D_{\mathbf{u}} f = (2, 4) \cdot (3/5, 4/5) = 6/5 + 16/5 = 22/5$ となります。

- 偏微分は、方向が座標軸に沿っている場合の方向微分の特殊なケースと言えます。ある方向への方向微分がゼロであれば、その点において関数はその方向に対して平坦であることを意味します。

- **等高線**（レベル曲線 / contour lines）は、関数が同じ値を持つ点を結んだ線です。$f(x, y) = x^2 + y^2$ の場合、等高線は原点を中心とした円になります（値 $c$ ごとに $x^2 + y^2 = c$）。

- 等高線が交差することはありません（1つの点が2つの異なる関数値を持つことはできないため）。

- 勾配は常に等高線に垂直であり、低い値から高い値の方向を指します。

- 等高線の間隔が狭いほど急峻な地形であり、間隔が広いほど緩やかな斜面であることを示します。

- これまでの関数は、単一の出力を生成していました。しかし、多くの関数は複数の出力を生成します。関数 $\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m$ は $n$ 個の入力を受け取り、$m$ 個の出力を生成します。**ヤコビ行列 (Jacobian matrix)** は、このようなベクトル値関数のすべての偏微分を整理したものです：

```math
J = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \end{bmatrix}
```

- ヤコビ行列の各行は、出力の各成分の勾配になっています。入力が 3 つ、出力が 2 つの関数の場合、ヤコビ行列は $2 \times 3$ 行列になります。

- ヤコビ行列は、微分をベクトル値関数に一般化したものです。

- スカラー関数の微分が「入力の変化に対して出力がどれだけ変化するか」を教えてくれるのと同様に、ヤコビ行列は「各入力に対して各出力がどのように変化するか」を教えてくれます。

- **ヤコビ行列式 (Determinant of the Jacobian / Jacobian)** は、変換が局所的に空間をどれだけ引き伸ばしたり、圧縮したりするかを測定します。

- 行列式が 2 であれば、微小な領域の面積は 2 倍になります。0 であれば、その変換は空間をより低い次元に押しつぶしてしまいます（行列の章で学んだ通り、行列式が 0 であることは、変換が特異であり、逆変換が存在しないことを意味します）。

- 複数の変換が合成されている場合、全体の写像のヤコビ行列は、それぞれのヤコビ行列の積になります。この考え方は、後の章で中心的な役割を果たすようになります。

- 勾配が「1次」の情報（傾き）を捉えるのに対し、**ヘッセ行列 (Hessian matrix)** は「2次」の情報（曲率 / 曲がり具合）を捉えます。

- スカラー関数 $f(x_1, \ldots, x_n)$ において、ヘッセ行列はすべての2階偏微分を並べた $n \times n$ 行列です：

```math
H = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots \\ \vdots & \vdots & \ddots \end{bmatrix}
```

- $f(x, y) = x^3 + 2xy^2 - y^3$ の場合、勾配は $(3x^2 + 2y^2,\; 4xy - 3y^2)$ であり、ヘッセ行列は次のようになります：

```math
H = \begin{bmatrix} 6x & 4y \\ 4y & 4x - 6y \end{bmatrix}
```

- 対角成分（$6x$ と $4x - 6y$）は、$x$ 方向に移動したときに $x$ 方向の傾きがどう変化するか、および $y$ についても同様の変化を教えてくれます。

- 非対角成分（$4y$）は、一方の方向に移動したときに、もう一方の方向の傾きがどう変化するかを教えてくれます。

- **クレローの定理 (Clairaut's theorem)** は、2階偏導関数が連続である関数について、混合部分微分が等しくなる（$\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}$）ことを保証します。

- つまり、ヘッセ行列は対称行列になります。これは（行列の章で見た通り）固有値が実数であり、固有ベクトルが直交することを意味します。

- ヘッセ行列は、臨界点（勾配がゼロとなる点）付近での関数の形状について教えてくれます：
  - $H$ が正定値（固有値がすべて正）であれば、その点は **極小値** です。曲面はボウルのようにあらゆる方向に上向きに曲がっています。
  - $H$ が負定値（固有値がすべて負）であれば、その点は **極大値** です。曲面は逆さまのボウルのように下向きに曲がっています。
  - $H$ が正と負の両方の固有値を持つ場合、その点は **鞍点 (saddle point)** です。地形図の峠のように、ある方向には上向きに、別の方向には下向きに曲がっています。

- **多変数の連鎖律 (multivariate chain rule)** は、連鎖律を多変数の関数に拡張したものです。$z = f(x, y)$ であり、かつ $x = g(t), y = h(t)$ であるとき：

$$\frac{dz}{dt} = \frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt}$$

- $t$ から $z$ への各経路が1つの項として寄与します：その経路に沿った偏微分に、中間変数の $t$ に関する微分を掛けたものです。

- 例えば、$z = x^2 y + 3x - y^2, x = \cos(t), y = \sin(t)$ の場合：

$$\frac{dz}{dt} = (2xy + 3)(-\sin t) + (x^2 - 2y)(\cos t)$$

- 手計算で微分を行う以外に、主に3つのアプローチがあります：
  - **数値微分 (Numerical differentiation)**: 非常に小さな $h$ を用いて $f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}$ として近似します。単純ですが、ノイズが多く精度に欠けます。
  - **数式微分 (Symbolic differentiation)**: 代数的に微分のルールを適用して厳密な数式を導き出します。数式が指数関数的に肥大化することがあります。
  - **自動微分 (Automatic differentiation / autodiff)**: 演算の連鎖を追跡し、連鎖律を用いて厳密な微分を効率的に計算します。JAX、PyTorch、TensorFlow はこれを使用しています。数式の肥大化を招くことなく、（近似ではない）正確な数値を得ることができます。

## コーディングタスク (CoLab または notebook を使用)

1. `jax.grad` を使用して、点 $(1, 2)$ における $f(x, y) = x^2 y + 3x - 2y$ の勾配を計算します。$f$ は個別の引数を取るため、`jax.grad` で `argnums` を指定します。

```python {cmd=true}
import jax
import jax.numpy as jnp

def f(x, y):
    return x**2 * y + 3*x - 2*y

df_dx = jax.grad(f, argnums=0)
df_dy = jax.grad(f, argnums=1)

x, y = 1.0, 2.0
print(f"∂f/∂x = {df_dx(x, y):.4f}  (期待値: {2*x*y + 3:.4f})")
print(f"∂f/∂y = {df_dy(x, y):.4f}  (期待値: {x**2 - 2:.4f})")
```

2. `jax.jacobian` を使用してベクトル値関数のヤコビ行列を計算し、手計算と比較します。

```python {cmd=true}
import jax
import jax.numpy as jnp

def F(x):
    return jnp.array([x[0]**2 + x[1], x[0] * x[1]**2])

J = jax.jacobian(F)
x = jnp.array([1.0, 2.0])
print(f"(1,2) におけるヤコビ行列:\n{J(x)}")
# 期待値: [[2*x[0], 1], [x[1]**2, 2*x[0]*x[1]]] = [[2, 1], [4, 4]]
```

3. `jax.hessian` を使用して $f(x, y) = x^3 + 2xy^2 - y^3$ のヘッセ行列を計算し、それが対称であることを確認します。

```python {cmd=true}
import jax
import jax.numpy as jnp

def f(xy):
    x, y = xy[0], xy[1]
    return x**3 + 2*x*y**2 - y**3

H = jax.hessian(f)
point = jnp.array([1.0, 2.0])
hess = H(point)
print(f"ヘッセ行列:\n{hess}")
print(f"対称行列か: {jnp.allclose(hess, hess.T)}")
# 期待値: [[6x, 4y], [4y, 4x-6y]] = [[6, 8], [8, -8]]
```

4. 最小限の自動微分エンジンをゼロから作成してみましょう。
   - 各 `Var` は自身の値と、連鎖律を通じて勾配を逆方向に伝播（Backpropagate）させる方法を保持します。
   - 割り算やべき乗など、さらに多くの演算を追加して拡張してみてください。
   - これは、JAX、PyTorch、Numpy がどのように設計されたかの基礎となる考え方です。

```python {cmd=true}
class Var:
    def __init__(self, val, children=(), backward_fn=None):
        self.val = val
        self.grad = 0.0
        self.children = children
        self.backward_fn = backward_fn

    def __add__(self, other):
        out = Var(self.val + other.val, children=(self, other))
        def _backward():
            self.grad += out.grad    # d(a+b)/da = 1
            other.grad += out.grad   # d(a+b)/db = 1
        out.backward_fn = _backward
        return out

    def __mul__(self, other):
        out = Var(self.val * other.val, children=(self, other))
        def _backward():
            self.grad += other.val * out.grad  # d(a*b)/da = b
            other.grad += self.val * out.grad  # d(a*b)/db = a
        out.backward_fn = _backward
        return out

    def backward(self):
        # トポロジカルソートを行ってから勾配を伝播させる
        # アルゴリズムとデータ構造のセクションで詳しく扱います
        order, visited = [], set()
        def topo(v):
            if v not in visited:
                visited.add(v)
                for c in v.children:
                    topo(c)
                order.append(v)
        topo(self)
        self.grad = 1.0
        for v in reversed(order):
            if v.backward_fn:
                v.backward_fn()

# (3, 2) における f(x, y) = x*x*y + x の計算
x = Var(3.0)
y = Var(2.0)
f = x * x * y + x       # = 3*3*2 + 3 = 21

f.backward()
print(f"f = {f.val}")           # 21.0
print(f"df/dx = {x.grad}")     # 2*x*y + 1 = 13.0
print(f"df/dy = {y.grad}")     # x*x = 9.0
```
