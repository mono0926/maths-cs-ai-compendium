# 関数近似 (Function Approximation)

- 私たちが遭遇する関数の多くは、直接扱うには複雑すぎます。紙の上で $e^{0.1}$ を計算したり、衛星の軌道を予測したりすることはすべて、単純な閉じた形の解（解析的な解）を持たない関数を扱います。

- **関数近似** は、複雑な関数を、関心のある領域において「十分に正確な」より単純な関数に置き換える手法です。

- 最も自然な近似は多項式によるものです。多項式は、係数を持つ $x$ のべき乗の合計にすぎず、評価、微分、積分が容易です。

- しかし、なぜ多項式は近似器としてこれほどまでによく機能するのでしょうか？ $x$ の各べき乗が何に寄与しているかを考えてみましょう。
  - 定数項 $a_0$ は、ベースラインの値を設定します。
  - $a_1 x$ の項は、傾きを加えます。
  - $a_2 x^2$ の項は、曲率（曲がり具合）を加えます。
  - より高次のべき乗は、関数の形状に関するより詳細な情報を捉えます。

![各多項式の項が、近似にさらなる詳細の層を追加していく](../images/polynomial_buildup.svg)

- 適切な係数を選択することで、ある点における関数の値、傾き、曲率、および高次の挙動を、1つずつ一致させることができます。

- 十分な数の項があれば、多項式はほとんどすべての滑らかな関数を模倣することができます。

- そこで問いはこうなります：どうすれば適切な係数を見つけられるでしょうか？

- **線形化 (linearisation)** は最も単純な近似です。点 $x = a$ の近くで、関数をその接線に置き換えます：

$$L(x) = f(a) + f'(a)(x - a)$$

- これは1次の **テイラー近似** です。これは次のように言っています：既知の値 $f(a)$ から始め、傾きに $a$ からの距離を掛けた分だけ調整する。

- 例えば、$\sin(x)$ を $x = 0$ で線形化してみましょう：$f(0) = 0, f'(0) = \cos(0) = 1$ なので、$L(x) = x$ となります。ゼロの近くでは $\sin(x) \approx x$ です。試してみると：$\sin(0.1) = 0.0998\ldots \approx 0.1$。

- しかし、線形化が有効なのは $a$ に非常に近い場所だけです。遠ざかると近似は崩れてしまいます。より高い精度を得るには、高次の項を含めます。

- **テイラー級数 (Taylor series)** は、関数を多項式の項の無限和として表現したもので、各項がある点 $a$ の近くでの関数の挙動のより細かい詳細を捉えます：

$$f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x - a)^n = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \cdots$$

![テイラー級数：項を増やすほど近似が良くなる](../images/taylor_approximation.svg)

- 連続する各項が補正を加えます。最初の項は値を一致させ、2番目の項は傾きを一致させ、3番目の項は曲率を一致させ、といった具合です。項を増やすほど、近似が正確になる領域も広がります。

- 分母の $n!$ （階乗）は恣意的なものではありません。$(x - a)^n$ をちょうど $n$ 回微分すると $n!$ になります。階乗がこれを相殺することで、テイラー多項式の $n$ 階微分が、$x = a$ における元の関数の $n$ 階微分と等しくなることが保証されます。

- **マクローリン級数 (Maclaurin series)** は、単に $a = 0$ を中心としたテイラー級数のことです：

$$f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!} x^n$$

- 有名なマクローリン級数の例：

$$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots$$

$$\sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots$$

$$\cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \cdots$$

- $\sin x$ は奇関数なので奇数次の項のみを持ち、$\cos x$ は偶関数なので偶数次の項のみを持つことに注目してください。符号が交互に変わることで、近似が真の値の周りで振動し、両側から収束していきます。

- $e^{0.5}$ を4つの項で近似してみましょう：$1 + 0.5 + \frac{0.25}{2} + \frac{0.125}{6} = 1 + 0.5 + 0.125 + 0.02083 \approx 1.6458$。真の値は $1.6487\ldots$ なので、わずか4つの項で小数点第2位まで正確な値が得られています。

- すべてのテイラー級数があらゆる場所で収束するわけではありません。**収束半径 (radius of convergence)** は、中心 $a$ からどれだけ離れた場所まで級数が有効な結果を与えるかを示します。その半径内であれば、項を追加することで多項式近似をいくらでも正確にすることができます。半径の外側では、級数は発散します。

- **べき級数 (power series)** はその一般的な形式です：$\sum_{n=0}^{\infty} a_n (x - c)^n$。テイラー級数は係数が微分によって決定されるべき級数です。他のべき級数は別の規則で定義されるかもしれません。**比判定法 (ratio test)** は収束を判定します：$\lim_{n \to \infty} \left|\frac{a_{n+1}}{a_n}\right|$ を計算します。この極限が $L$ であれば、収束半径は $R = 1/L$ です。

- テイラー級数を $n$ 個の項で打ち切ると（切り捨てると）、誤差が生じます。**ラグランジュの剰余項 (Lagrange remainder)** はこの誤差を束縛（境界を設定）します：

$$R_n(x) = \frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}$$

- ここで $c$ は $a$ と $x$ の間のどこかにある未知の点です。$c$ を正確に知ることはできませんが、多くの場合 $|f^{(n+1)}(c)|$ を抑える（最大値を求める）ことで、最悪ケースの誤差の見積もりを得ることができます。分母の $(n+1)!$ は非常に急速に成長するため、（収束半径内の関数の場合）項を追加するにつれて誤差は急速に減少します。

- 多変数関数の場合、テイラー展開には混合部分微分が含まれます。点 $\mathbf{a}$ の周りでの $f(\mathbf{x})$ の2次近似は次のようになります：

$$f(\mathbf{x}) \approx f(\mathbf{a}) + \nabla f(\mathbf{a})^T (\mathbf{x} - \mathbf{a}) + \frac{1}{2} (\mathbf{x} - \mathbf{a})^T H(\mathbf{a}) (\mathbf{x} - \mathbf{a})$$

- 第1項は値、第2項は勾配（多変数微積分で見たベクトル）を使用し、第3項はヘッセ行列（曲率を捉えるもの）を使用します。これにより、行列の章が微積分に直接つながります。ヘッセ行列は、関数の曲面の形状を記述する2階微分の行列なのです。

- この多変数の2次近似は、ニュートン法やその他の2次最適化手法の基礎となります。これについては次のファイルで見ていきます。

- 多項式以外にも、知っておくべき近似手法があります：
  - **スプライン補間 (Spline interpolation)**: 1つの高次多項式ではなく、多くの低次多項式を滑らかにつなぎ合わせて使用します。これにより、高次多項式で発生しがちな激しい振動（ルンゲ現象）を避けることができます。
  - **フーリエ級数 (Fourier series)**: 周期関数をサインとコサインの和として近似します。信号処理やオーディオにおいて不可欠です。
  - **ニューラルネットワーク**: ユニバーサル関数近似器（普遍性定理）。十分な数のニューロンがあれば、どのような連続関数でも任意の精度で近似できます。これがディープラーニングの理論的な正当化となっています。

- 連続性（飛躍がない）、微分可能性（鋭い角がない）、滑らかさ（すべての次数の微分が存在する）、有界性（出力が有限に留まる）などの、近似を信頼できるものにする性質を持つ関数を「素性の良い (well-behaved)」関数と呼びます。

- 多項式、指数関数、および三角関数はすべて素性の良い関数です。関数の素性が良ければ良いほど、優れた近似を得るために必要なテイラー展開の項の数は少なくなります。

## コーディングタスク (CoLab または notebook を使用)

1. 項の数を増やしながら $e^x$ を近似し、近似がどのように改善されるかを可視化します。

```python {cmd=true}
import jax.numpy as jnp
import matplotlib.pyplot as plt

x = jnp.linspace(-2, 3, 300)
plt.plot(x, jnp.exp(x), "k-", linewidth=2, label="eˣ (exact)")

colors = ["#e74c3c", "#3498db", "#27ae60", "#9b59b6"]
for n, color in zip([1, 2, 4, 8], colors):
    approx = sum(x**k / jnp.array(float(jnp.prod(jnp.arange(1, k+1)) if k > 0 else 1))
                 for k in range(n+1))
    plt.plot(x, approx, color=color, linestyle="--", label=f"{n} terms")

plt.ylim(-2, 15)
plt.legend()
plt.title("Taylor approximation of eˣ")
plt.show()
```

2. ラグランジュの剰余項を計算して、異なる項数のテイラー級数で $\sin(1)$ を近似したときの誤差の範囲を求めます。

```python {cmd=true}
import jax.numpy as jnp

x = 1.0
exact = jnp.sin(x)

taylor = 0.0
for n in range(8):
    sign = (-1)**n
    factorial = float(jnp.prod(jnp.arange(1, 2*n+2)))
    taylor += sign * x**(2*n+1) / factorial
    error = abs(exact - taylor)
    bound = x**(2*n+3) / float(jnp.prod(jnp.arange(1, 2*n+4)))
    print(f"terms={n+1}  approx={taylor:.10f}  error={error:.2e}  bound={bound:.2e}")
```

3. $x = 0$ 付近における $\cos(x)$ の線形近似と2次テイラー近似を比較します。両方の近似を真の関数と一緒にプロットし、それぞれの精度が高い範囲を観察してください。

```python {cmd=true}
import jax.numpy as jnp
import matplotlib.pyplot as plt

x = jnp.linspace(-3, 3, 300)
plt.plot(x, jnp.cos(x), "k-", linewidth=2, label="cos(x)")
plt.plot(x, jnp.ones_like(x), "--", color="#e74c3c", label="線形近似: 1")
plt.plot(x, 1 - x**2/2, "--", color="#3498db", label="2次近似: 1 - x²/2")
plt.plot(x, 1 - x**2/2 + x**4/24, "--", color="#27ae60", label="4次近似")
plt.ylim(-2, 2)
plt.legend()
plt.title("Taylor approximations of cos(x)")
plt.show()
```
