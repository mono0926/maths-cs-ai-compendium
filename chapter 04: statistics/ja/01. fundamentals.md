# 統計学の基礎 (Fundamentals of Statistics)

- 統計学はデータから学ぶための科学です。観測値を収集し、それらを要約し、結論を導き出します。多くの場合、直接測定できないものについて結論を出します。

- 例えば、ある国のすべての成人の平均身長を知りたいとします。全員を測定することはできないため、サンプル（標本）を測定し、統計学を使用して全体の母集団について情報に基づいた推測を行います。

- 主に2つの枝分かれがあります：
  - **記述統計学 (Descriptive statistics)**：すでに持っているデータを要約する（平均、チャート、表など）。
  - **推測統計学 (Inferential statistics)**：サンプルを使用して、より大きなグループについて主張を行う。

- 統計学の構成要素は **分布 (distribution)** です。これは値がどのように広がっているかを示す記述です。平均、検定、予測など、他のすべてのものは分布の理解から導き出されます。

- **度数分布 (frequency distribution)** は、データ内で各値（または値の範囲）が何回現れるかをカウントします。試験のスコアをビン（区切り）に分類し、各ビンに何人の学生が含まれるかを数えることを考えてみてください。その結果がヒストグラムです。

- **確率分布 (probability distribution)** は、生のカウントを確率に置き換えます。「70点から80点の間に12人の学生がいた」ではなく、「70点から80点の間のスコアを取る確率は 0.24 である」と言います。データが連続的な場合、ヒストグラムの棒は滑らかな曲線になります。

![ヒストグラムとしての度数分布 vs 滑らかな曲線としての確率分布](../images/distribution_types.svg)

- 左側のヒストグラムは、実際に収集したデータから構築されています。右側の滑らかな曲線は、データセットの背後にあるパターンを記述する数学的なモデルです。一方は経験的であり、もう一方は理論的です。

- 数学的に分布を扱うには、結果に数値を割り当てる方法が必要です。それがまさに **確率変数 (random variable)** が行うことです。

- 確率変数は、実験の各結果を実数に対応させる関数です。コイン投げ：結果は「表」または「裏」ですが、確率変数 $X$ はこれを $X(\text{表}) = 1$ および $X(\text{裏}) = 0$ に変換します。これで計算ができるようになります。

![結果（コイン、サイコロ）を数直線に対応付ける確率変数](../images/random_variable.svg)

- **離散 (discrete)** 確率変数は、数えられる値のセットを取ります：10回のコイン投げでの表の数、サイコロの目、1時間に受信するメールの数など。

- **連続 (continuous)** 確率変数は、ある区間内の任意の値を取ることができます：正確な身長、次のバスが来るまでの時間、正午の気温など。

- この区別は重要です。なぜなら、確率の計算方法が変わるからです。離散変数の場合は合計（加算）し、連続変数の場合は積分します（Chapter 3 の積分を思い出してください）。

- 離散確率変数の場合、**確率質量関数 (probability mass function; PMF)** が各特定の値の確率を与えます：

$$P(X = x) = p(x), \quad \text{ここで } \sum_{x} p(x) = 1$$

- 連続確率変数の場合、**確率密度関数 (probability density function; PDF)** がある範囲に収まる確率を与えます。単一の正確な値の確率はゼロであり、区間のみが正の確率を持ちます：

$$P(a \le X \le b) = \int_a^b f(x)\, dx, \quad \text{ここで } \int_{-\infty}^{\infty} f(x)\, dx = 1$$

- 結果に数値を割り当てることができるようになったので、最も自然な疑問は「平均してどのような値が期待されるか？」ということです。

- **期待値 (Expectation)**（または expected value）は、すべての可能な値の重み付き平均であり、重みはその確率です。分布の「重心」と考えてください。

- 公平なサイコロを何度も振ると、平均値は 3.5 に収束します。実際に 3.5 という目を出すことはできませんが、それが期待値です。

- 離散確率変数の場合：

$$E[X] = \sum_{x} x \cdot p(x)$$

- 連続確率変数の場合（Chapter 3 の積分を使用）：

$$E[X] = \int_{-\infty}^{\infty} x \cdot f(x)\, dx$$

- 例：公平な6面ダイスは、$x = 1, 2, 3, 4, 5, 6$ に対して $p(x) = 1/6$ です。

$$E[X] = 1 \cdot \tfrac{1}{6} + 2 \cdot \tfrac{1}{6} + 3 \cdot \tfrac{1}{6} + 4 \cdot \tfrac{1}{6} + 5 \cdot \tfrac{1}{6} + 6 \cdot \tfrac{1}{6} = \frac{21}{6} = 3.5$$

- 期待値は線形であり、$E[aX + b] = aE[X] + b$ が成り立ちます。この性質は非常に便利で、ML の損失関数などで常に現れます。

- 期待値は中心を教えてくれますが、値がどれだけ広がっているかについては何も教えてくれません。分布の完全な形状を記述するには、**モーメント (moments)** が必要です。

- モーメントは、$X$ の累乗の期待値です。第 $k$ 次 **原点まわりのモーメント (raw moment)** は次の通りです：

$$\mu_k' = E[X^k]$$

- 第1次原点まわりのモーメント（$k = 1$）は単に平均です：$\mu_1' = E[X] = \mu$。

- 原点まわりのモーメントはゼロからの測定です。多くの場合、平均からの偏差に関心があります。第 $k$ 次 **中心モーメント (central moment)** は測定値を平均中心にします：

$$\mu_k = E[(X - \mu)^k]$$

- 第1次中心モーメントは常にゼロです（平均より上と下の偏差が相殺されるため）。第2次中心モーメントは **分散 (variance)** です。

- 異なるスケールの分布を比較するために、標準偏差 $\sigma$ の適切な累乗で割ることで **標準化 (standardise)** します：

$$\tilde{\mu}_k = \frac{\mu_k}{\sigma^k}$$

- 各モーメントは分布の形状の異なる側面を捉えます：

![ベル曲線。各モーメントが何を捉えるかの注釈：平均（中心）、分散（広がり）、歪度（非対称性）、尖度（裾の重さ）](../images/moments_shape.svg)

- **第1次モーメント（平均）**：分布の重心。バランスポイント。
- **第2次モーメント（分散）**：平均の周りで値がどれだけ広がっているか。分散が大きいほど幅が広くなります。
- **第3次モーメント（歪度）**：分布が左と右のどちらに傾いているか。歪度がゼロなら対称です。
- **第4次モーメント（尖度）**：裾がどれだけ重いか。尖度が高いほど、より極端な外れ値が存在します。

- 具体的なデータセットに対して4つのモーメントすべてを計算してみましょう：$X = \{2, 4, 4, 4, 5, 5, 7, 9\}$。

- **ステップ 1：平均**（第1次原点まわりのモーメント）

$$\mu = \frac{2 + 4 + 4 + 4 + 5 + 5 + 7 + 9}{8} = \frac{40}{8} = 5$$

- **ステップ 2：分散**（第2次中心モーメント）。各値から平均を引き、2乗して平均します：

$$\sigma^2 = \frac{(2{-}5)^2 + (4{-}5)^2 + (4{-}5)^2 + (4{-}5)^2 + (5{-}5)^2 + (5{-}5)^2 + (7{-}5)^2 + (9{-}5)^2}{8}$$

$$= \frac{9 + 1 + 1 + 1 + 0 + 0 + 4 + 16}{8} = \frac{32}{8} = 4$$

- **標準偏差** は $\sigma = \sqrt{4} = 2$ です。

- **ステップ 3：歪度 (Skewness)**（標準化された第3次中心モーメント）。偏差を3乗し、平均して $\sigma^3$ で割ります：

$$\tilde{\mu}_3 = \frac{1}{8} \cdot \frac{(-3)^3 + (-1)^3 + (-1)^3 + (-1)^3 + 0^3 + 0^3 + 2^3 + 4^3}{2^3}$$

$$= \frac{1}{8} \cdot \frac{-27 -1 -1 -1 + 0 + 0 + 8 + 64}{8} = \frac{42}{64} = 0.656$$

- 正の歪度は右側の裾が長いことを意味します。9 が平均から遠く上にあるため、これは理にかなっています。

- **ステップ 4：尖度 (Kurtosis)**（標準化された第4次中心モーメント）。偏差を4乗します：

$$\tilde{\mu}_4 = \frac{1}{8} \cdot \frac{(-3)^4 + (-1)^4 + (-1)^4 + (-1)^4 + 0^4 + 0^4 + 2^4 + 4^4}{2^4}$$

$$= \frac{1}{8} \cdot \frac{81 + 1 + 1 + 1 + 0 + 0 + 16 + 256}{16} = \frac{356}{128} = 2.781$$

- 正規分布の尖度は 3 です（「中尖的」と呼ばれます）。私たちの値 2.781 はこれに近い値であり、裾がほぼ正規分布であることを示唆しています。3 より大きい場合（「急尖的」）は裾が重いことを意味し、3 より小さい場合（「緩尖的」）は裾が軽いことを意味します。一部の式では、3 を引いた **過剰尖度 (excess kurtosis)** を報告するため、その場合は $-0.219$ になります。

## コーディングタスク (CoLab または notebook を使用)

1. 目が出る確率が 0.3 で、他のすべての目が残りの確率を等しく分け合っている「イカサマのサイコロ」の期待値を計算します。100,000 回の試行をシミュレーションして検証してください。

```python
import jax
import jax.numpy as jnp

# イカサマのサイコロ: 6の目が p=0.3、その他は0.7を均等に分ける
probs = jnp.array([0.14, 0.14, 0.14, 0.14, 0.14, 0.30])
faces = jnp.array([1, 2, 3, 4, 5, 6])

# 理論的な期待値
ev = jnp.sum(faces * probs)
print(f"Expected value (formula): {ev:.4f}")

# シミュレーション
key = jax.random.PRNGKey(42)
rolls = jax.random.choice(key, faces, shape=(100_000,), p=probs)
print(f"Expected value (simulation): {rolls.mean():.4f}")
```

2. 演習例のデータセットに対して、4つのモーメントすべて（平均、分散、歪度、尖度）を計算し、データを変更して各モーメントがどのように変化するかを観察してください。

```python
import jax.numpy as jnp

x = jnp.array([2, 4, 4, 4, 5, 5, 7, 9], dtype=jnp.float32)

mean = jnp.mean(x)
variance = jnp.mean((x - mean) ** 2)
std = jnp.sqrt(variance)
skewness = jnp.mean(((x - mean) / std) ** 3)
kurtosis = jnp.mean(((x - mean) / std) ** 4)

print(f"Mean:     {mean:.3f}")
print(f"Variance: {variance:.3f}")
print(f"Std Dev:  {std:.3f}")
print(f"Skewness: {skewness:.3f}")
print(f"Kurtosis: {kurtosis:.3f}")
print(f"Excess K: {kurtosis - 3:.3f}")
```

3. 公平なサイコロの PMF と CDF を並べて可視化してください。確率を変えてみて、形状がどのように変化するかを確認してください。

```python
import jax.numpy as jnp
import matplotlib.pyplot as plt

faces = jnp.array([1, 2, 3, 4, 5, 6])
pmf = jnp.ones(6) / 6  # 公平なサイコロ。ここを変えてみてください！
cdf = jnp.cumsum(pmf)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

ax1.bar(faces, pmf, color="#3498db", alpha=0.8)
ax1.set_title("PMF")
ax1.set_xlabel("Face")
ax1.set_ylabel("P(X = x)")
ax1.set_ylim(0, 0.5)

ax2.step(faces, cdf, where="mid", color="#e74c3c", linewidth=2)
ax2.set_title("CDF")
ax2.set_xlabel("Face")
ax2.set_ylabel("P(X ≤ x)")
ax2.set_ylim(0, 1.1)

plt.tight_layout()
plt.show()
```
