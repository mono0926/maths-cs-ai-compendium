# 統計指標 (Statistical Measures)

- 前のファイルでは、集団の要約統計量としてモーメントを紹介しました。ここでは、そこから派生する実用的なツール、すなわち分散、位置、形状、および関連性の指標について詳しく見ていきます。

- **散らばり (Dispersion)** は、「データがどれくらい広がっているか？」という疑問に答えます。2つのクラスのテストの平均点が同じであっても、その広がり（分散）が大きく異なることがあります。

![平均は同じだが散らばりが異なる2つの分布](../../images/variance_spread.svg)

- 狭い（青色）分布は分散が低く、ほとんどの値が平均の近くに密集しています。広い（赤色）分布は分散が高く、値が遠くまで散らばっています。

- **分散 (Variance)** は、平均からの距離の2乗の平均です。偏差の正負が相殺されないように、2乗します。

$$\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2$$

- 母集団全体ではなくサンプル（標本）を扱う場合は、$N$ ではなく $N - 1$ で割ります。この修正（**ベッセルの補正**と呼ばれます）は、サンプルが真の変動性を低く見積もる傾向があることを考慮したものです：

$$s^2 = \frac{1}{N-1} \sum_{i=1}^{N} (x_i - \bar{x})^2$$

- **標準偏差 (Standard deviation)** は分散の平方根です：$\sigma = \sqrt{\sigma^2}$。これにより、指標が元の単位に戻ります。データがセンチメートル単位の場合、分散は cm$^2$ になりますが、標準偏差は cm に戻ります。

- **平均絶対偏差 (Mean Absolute Deviation; MAD)** は、より単純な代替案です。2乗する代わりに、各偏差の絶対値を取ります：

$$\text{MAD} = \frac{1}{N} \sum_{i=1}^{N} |x_i - \mu|$$

- MAD は分散よりも外れ値に対して **ロバスト（頑健）** です。2乗によって大きな偏差が増幅されないためです。しかし、分散は数学的に扱いやすく、証明や ML の最適化において美しく分解できるという利点があります。

- **位置 (Position)** は、「特定の数値が他のデータに対してどの位置にあるか？」という疑問に答えます。

- **四分位数 (Quartiles)** は、ソートされたデータを4つの等しい部分に分割します。第1四分位数 (Q1, 25パーセンタイル) は、データの 25% がその値以下になる値です。Q2 は中央値 (50パーセンタイル) です。Q3 は 75パーセンタイルです。

- **四分位範囲 (Interquartile Range; IQR)** は $Q3 - Q1$ です。これは極端な値を除外した、データの中央 50% の広がりを捉えます。

![Q1、中央値、Q3、IQR、ひげ、および外れ値を示す箱ひげ図](../../images/quartiles_boxplot.svg)

- **箱ひげ図 (Box plot)** は、統計学において最も有用な可視化の1つです。箱は Q1 から Q3 までをカバーし、中の線は中央値です。ひげ (whiskers) は外れ値ではない極端な値まで伸びており、ひげの外側にある点は外れ値として示されます。

- **パーセンタイル (Percentiles)** は四分位数を一般化したものです。$p$ パーセンタイルとは、全データの $p\%$ がその値以下になるような値を指します。

- **zスコア (z-score)** は、ある値が平均から標準偏差何個分だけ離れているかを示します：

$$z = \frac{x - \mu}{\sigma}$$

- zスコアが 2 ならば、その値は平均より 2 標準偏差上にあることを意味します。zスコアが $-1.5$ ならば、平均より 1.5 標準偏差下にあることを意味します。これは **標準化 (standardisation)** とも呼ばれ、あらゆる分布を平均 0、標準偏差 1 に変換するため、ML の特徴量スケーリングで多用されます。

- **形状 (Shape)** は、中心や広がり以外の分布の幾何学的特徴を表します。

- **歪度 (Skewness)**（前のファイルで見た標準化された第3次モーメント）は、非対称性を測定します。正規分布のような完全に左右対称な分布では、歪度はゼロになります。正の歪度は右側の裾が長いこと（例：所得分布）、負の歪度は左側の裾が長いこと（例：退職時の年齢）を意味します。

$$\text{Skewness} = \frac{1}{N} \sum_{i=1}^{N} \left(\frac{x_i - \mu}{\sigma}\right)^3$$

- **尖度 (Kurtosis)**（標準化された第4次モーメント）は、裾の重さを測定します。正規分布の尖度は 3 です。裾が重い（外れ値が出やすい）分布では、尖度は 3 より大きくなります。

$$\text{Kurtosis} = \frac{1}{N} \sum_{i=1}^{N} \left(\frac{x_i - \mu}{\sigma}\right)^4$$

- **相関 (Correlation)** は、2つの変数間の関係の強さと方向を測定します。「一方が上がると、もう一方は上がる傾向があるか、下がる傾向があるか、あるいは何もしないか？」という疑問に答えます。

![正の相関、無相関、負の相関を示す3つの散布図](../../images/correlation_scatter.svg)

- **ピアソンの相関係数 (Pearson correlation)** ($r$) は **線形的** な関連性を測定します。範囲は $-1$（完全な負の相関）から $0$（無相関）を経て $+1$（完全な正の相関）までです。

$$r = \frac{\sum_{i=1}^{N} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2} \cdot \sqrt{\sum (y_i - \bar{y})^2}}$$

- Chapter 1 のドット積を思い出すと、ピアソンの相関は本質的に、平均を引いて中心化した $\mathbf{x}$ と $\mathbf{y}$ の間の **コサイン類似度** です。

- **スピアマンの順位相関係数 (Spearman correlation)** ($\rho$) は **単調性**（一貫して増加または減少するか）を測定します。生の数値そのものではなく、まず順位に変換してから、その順位に対してピアソンの相関を計算します。これにより、外れ値に強く、関係が非線形であっても単調であれば正確に捉えることができます。

- **幾何平均 (Geometric mean)** は、成長率のように値が掛け合わされる場合の平均として適切です。投資が 10%、20%、30% と成長した場合、平均成長率はそれらの算術平均ではありません。代わりに次のように計算します：

$$\bar{x}_{\text{geo}} = \left(\prod_{i=1}^{N} x_i\right)^{1/N}$$

- 特に成長率の場合は、パーセンテージを係数（1.10, 1.20, 1.30）に変換し、幾何平均を計算してから 1 を引きます。

- **指数移動平均 (Exponential Moving Average; EMA)** は、最近の観測値に大きな重みを置きます。全期間を均等に扱う単純移動平均とは異なり、EMA の重みは指数関数的に減衰します：

$$\text{EMA}_t = \alpha \cdot x_t + (1 - \alpha) \cdot \text{EMA}_{t-1}$$

- 平滑化係数 $\alpha$（0 から 1 の間）は、古い観測値の影響がどれくらい速く失われるかを制御します。$\alpha$ が大きいほど最近の変化に敏感になり、小さいほど滑らかになります。ML では、Adam などのオプティマイザや、バッチ正規化における統計情報の更新で使用されます。

- **外れ値検知 (Outlier detection)** は、他のデータから異常に離れたデータポイントを特定します。代表的な2つの方法：
  - **IQR法**：$Q1 - 1.5 \times \text{IQR}$ 未満、または $Q3 + 1.5 \times \text{IQR}$ を超えるポイントを外れ値とする。
  - **zスコア法**：$|z| > 3$（平均から 3 標準偏差以上離れている）ポイントを外れ値とする。

- IQR法は正規分布を仮定しないため、よりロバストです。zスコア法はデータがほぼ正規分布に従う場合にはうまく機能しますが、分布が大きく歪んでいる場合には失敗することがあります。

## コーディングタスク (CoLab または notebook を使用)

1. データセットに対して分散、標準偏差、MAD を計算して比較してください。極端な外れ値を追加したときに何が起こるかを確認してください。

```python {cmd=true}
import jax.numpy as jnp

data = jnp.array([4, 8, 6, 5, 3, 7, 9, 5, 6, 7], dtype=jnp.float32)

mean = jnp.mean(data)
variance = jnp.var(data)
std = jnp.std(data)
mad = jnp.mean(jnp.abs(data - mean))

print("元のデータ:")
print(f"  分散: {variance:.3f}, 標準偏差: {std:.3f}, MAD: {mad:.3f}")

# 外れ値を追加して再計算
data_outlier = jnp.append(data, 100.0)
mean2 = jnp.mean(data_outlier)
print(f"\n外れ値(100)あり:")
print(f"  分散: {jnp.var(data_outlier):.3f}, 標準偏差: {jnp.std(data_outlier):.3f}, MAD: {jnp.mean(jnp.abs(data_outlier - mean2)):.3f}")
```

2. 2つの変数間のピアソン相関とスピアマン相関を計算してください。異なる関係性で実験してみてください。

```python {cmd=true}
import jax
import jax.numpy as jnp

# 完全な線形関係
x = jnp.array([1, 2, 3, 4, 5, 6, 7, 8], dtype=jnp.float32)
y = 2 * x + 1  # ここを変えてみてください！

def pearson(a, b):
    a_c = a - jnp.mean(a)
    b_c = b - jnp.mean(b)
    return jnp.sum(a_c * b_c) / (jnp.sqrt(jnp.sum(a_c**2)) * jnp.sqrt(jnp.sum(b_c**2)))

def spearman(a, b):
    rank_a = jnp.argsort(jnp.argsort(a)).astype(jnp.float32)
    rank_b = jnp.argsort(jnp.argsort(b)).astype(jnp.float32)
    return pearson(rank_a, rank_b)

print(f"ピアソン r:  {pearson(x, y):.4f}")
print(f"スピアマン ρ: {spearman(x, y):.4f}")
```

3. IQR法と zスコア法の両方を使用して外れ値検知を実装し、歪んだデータでの結果を比較してください。

```python {cmd=true}
import jax.numpy as jnp

data = jnp.array([2, 3, 3, 4, 5, 5, 5, 6, 6, 7, 50], dtype=jnp.float32)

# IQR法
q1, q3 = jnp.percentile(data, 25), jnp.percentile(data, 75)
iqr = q3 - q1
lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr
iqr_outliers = data[(data < lower) | (data > upper)]
print(f"IQR 境界: [{lower:.1f}, {upper:.1f}]")
print(f"IQR 外れ値: {iqr_outliers}")

# zスコア法
z_scores = (data - jnp.mean(data)) / jnp.std(data)
z_outliers = data[jnp.abs(z_scores) > 3]
print(f"\nzスコア: {z_scores}")
print(f"zスコア外れ値 (|z| > 3): {z_outliers}")
```

4. ノイズの多いデータに対して、異なる平滑化係数を用いた指数移動平均 (EMA) を計算し、プロットしてください。

```python {cmd=true}
import jax.numpy as jnp
import matplotlib.pyplot as plt

# ノイズの多いデータを生成
key = __import__("jax").random.PRNGKey(0)
noise = __import__("jax").random.normal(key, shape=(50,))
signal = jnp.linspace(0, 5, 50) + noise

def ema(data, alpha):
    result = jnp.zeros_like(data)
    result = result.at[0].set(data[0])
    for t in range(1, len(data)):
        result = result.at[t].set(alpha * data[t] + (1 - alpha) * result[t - 1])
    return result

plt.figure(figsize=(10, 4))
plt.plot(signal, "o", alpha=0.3, label="生データ", color="#999")
for alpha, color in [(0.1, "#e74c3c"), (0.3, "#3498db"), (0.7, "#27ae60")]:
    plt.plot(ema(signal, alpha), label=f"α={alpha}", color=color, linewidth=2)
plt.legend()
plt.title("異なる平滑化係数による EMA")
plt.show()
```
