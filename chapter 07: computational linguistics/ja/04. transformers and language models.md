# Transformer と言語モデル

- **自己注意 (Self-attention)**: スケーリング・ドットプロダクト・アテンション、マルチヘッド・アテンション
- **位置エンコーディング (Positional encoding)**: 正弦波（Sinusoidal）、学習ベース、RoPE、ALiBi
- **Transformer ブロック**: レイヤー正規化（Layer Norm）、残差接続、フィードフォワード・サブレイヤー
- **エンコーダのみのモデル**: BERT、マスク言語モデリング (MLM)、次文予測 (NSP)
- **デコーダのみのモデル**: GPT ファミリー、因果言語モデリング、自己回帰生成
- **エンコーダ・デコーダモデル**: T5、BART、スパン破壊、ノイズ除去目的関数
- **ファインチューニング戦略**: フルファインチューニング、アダプター (Adapters)、LoRA、Prefix Tuning
- **プロンプトエンジニアリング**と**インコンテキスト学習 (In-context learning)**
- **スケーリング則 (Scaling laws)**: Kaplan、Chinchilla、計算最適（Compute-optimal）な訓練
- **混合エキスパート (Mixture of Experts: MoE)**: ゲート関数、Top-k ルーティング、ロードバランス損失、エキスパート並列
