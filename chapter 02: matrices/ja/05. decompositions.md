# 行列の分解 (Matrix Decompositions)

- 行列の分解（または因数分解）とは、行列をより扱いやすい単純な部品に分解することです。数値を因数分解するのと似ています。例えば $12$ をそのまま扱うよりも $12 = 3 \times 4$ と分解したほうが性質を理解しやすくなります。

- 行列を分解する目的は、連立方程式を高速に解く、逆行列を安定して計算する、固有値を見つける、データを圧縮する、そして変換の幾何学的な意味を理解するためです。

- 最も基本的な手法は **ガウス消去法 (Gaussian elimination)**（行基本変形）です。考え方は単純です。システム $A\mathbf{x} = \mathbf{b}$ が与えられたとき、許容される3つの操作を使って、答えが明白になるまで $A$ を単純化します。

- 許容される操作（行基本変形）は、2つの行を入れ替える、ある行を 0 でないスカラーで倍にする、ある行に別の行の定数倍を加える、の3つです。

- 例えば、ピボット（各行の最初の非ゼロ成分）より下の第1列を消去するには、第1行の定数倍を下の行から引きます：

```math
\begin{bmatrix} 2 & 1 & 5 \\ 4 & 3 & 7 \\ 6 & 5 & 9 \end{bmatrix} \xrightarrow{R_2 - 2R_1} \begin{bmatrix} 2 & 1 & 5 \\ 0 & 1 & -3 \\ 6 & 5 & 9 \end{bmatrix} \xrightarrow{R_3 - 3R_1} \begin{bmatrix} 2 & 1 & 5 \\ 0 & 1 & -3 \\ 0 & 2 & -6 \end{bmatrix}
```

- 目標は **行階段形 (row echelon form, REF)** です。各ピボットの下はすべて 0 であり、各ピボットはその上の行のピボットよりも右側に位置します。行列は階段状になります。

![ガウス消去法：行操作によって三角形の形式を作り、下から順に解く](../images/gaussian_elimination.svg)

- さらに進めて **行簡約階段形 (reduced row echelon form, RREF)** にすると、各ピボットは 1 になり、その列の中で唯一の非ゼロ成分になります。すべての行列には一意の RREF が存在します。

- 三角形の形式になったら、**後退代入 (back substitution)** で解きます。最下行から最後の変数が直接求まり、順次上へと遡って解いていきます。

- これは他のすべての分解法の基礎となります。分解の目的は、行列を三角形の形式に落とし込み、後退代入によって変数を解けるようにすることにあります。

- **LU分解 (LU decomposition)** は、ガウス消去法を形式化したもので、正方行列を $A = LU$（行の入れ替えを含めると $A = PLU$）と分解します。ここで $L$ は下三角行列 (lower triangular)、 $U$ は上三角行列 (upper triangular) です。

![LU分解：一つの難しい行列を二つの簡単な三角形の行列に分ける](../images/lu_decomposition.svg)

- $A\mathbf{x} = \mathbf{b}$ を解くには、まず前進代入（上から下へ）で $L\mathbf{y} = \mathbf{b}$ を解き、次に後退代入（下から上へ）で $U\mathbf{x} = \mathbf{y}$ を解きます。一つの難しい一般の行列の問題を、二つの簡単な三角形の行列の問題に置き換えるのです。

- ガウス消去法をそのまま行う場合と比較した利点は、再利用性です。一度 $L$ と $U$ を求めてしまえば、右辺のベクトル $\mathbf{b}$ が変わっても、分解をやり直すことなく多くの問題に対応できます。

- 1000個の異なる右辺に対して同じシステムを解く必要がある場合（シミュレーションなどでよくあります）、一度だけ分解してそれを使い回します。

- 行列が対称かつ正定値（共分散行列など）である場合、さらに効率的な手法があります。

- **コレスキー分解 (Cholesky decomposition)** は、行列を $A = LL^T$（ $L$ は下三角行列）と分解します。例：

```math
\begin{bmatrix} 4 & 2 \\ 2 & 5 \end{bmatrix} = \begin{bmatrix} 2 & 0 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}
```

- これはLU分解の約2倍高速で、数値的にも極めて安定しています。行列の「平方根」のようなものだと考えてください。

- もし分解に失敗した場合（平方根の中に負の値が現れるなど）、その行列は正定値ではありません。したがって、コレスキー分解は正定値性の判定テストとしても利用できます。

- 正方行列 $A$ の **固有ベクトル (eigenvector)** とは、その行列による変換によって、回転することなく、単に引き伸ばされたり縮められたりするだけの特別な方向のことです。その伸縮倍率を **固有値 (eigenvalue)** と呼びます：

$$A\mathbf{x} = \lambda\mathbf{x}$$

![固有ベクトルは同じ直線上に留まり（スケーリングのみ）、通常のベクトルは回転する](../images/eigenvector.svg)

- ほとんどのベクトルは、行列を掛けると向きが変わります。しかし固有ベクトルは特別で、出力の向きは入力と同じ（あるいは真逆）であり、サイズが $\lambda$ 倍されるだけです。$\lambda = 2$ なら長さが2倍になり、$\lambda = -1$ なら向きが反転し、$\lambda = 0$ ならゼロに押しつぶされます。

- 例えば、以下の場合：

```math
A = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}
```

ベクトル $[1, 0]^T$ は固有ベクトルであり、固有値は $\lambda = 3$ です。なぜなら $A[1, 0]^T = [3, 0]^T = 3[1, 0]^T$ となるからです。

- 固有値を求めるには、**特性方程式 (characteristic polynomial)** $\det(A - \lambda I) = 0$ を解きます。その解が固有値です。その後、各 $\lambda$ を $(A - \lambda I)\mathbf{x} = \mathbf{0}$ に代入して、対応する固有ベクトルを見つけます。

- 主な性質：
  - 行列 $A$ のトレースは固有値の和に等しい。
  - 行列 $A$ の行列式は固有値の積に等しい。
  - 対称行列は常に実数の固有値を持ち、固有ベクトルは互いに直交する。
  - 正定値行列の固有値はすべて正である。
  - 共分散行列（統計学で登場）は常に半正定値である。

- 特性方程式による固有値の計算は、巨大な行列には実用的ではありません。代わりに、反復法が使われます：
  - **べき乗法 (Power iteration)**: $A$ を繰り返し掛け、正規化します。最大固有値に対応する固有ベクトルに収束します。単純ですが、一つのペアしか見つけられません。

  - **QRアルゴリズム**: 現在主流の方法です。行列が三角形に収束し、対角線上にすべての固有値が現れるまで、QR分解と再結合を繰り返します。

  - **逆反復法 (Inverse iteration)**: 指定した値に近い固有値に対応する固有ベクトルを見つけます。特定の範囲の固有値だけが必要な場合に便利です。

  - 巨大な疎行列に対しては、効率向上のために **アーノルディ法 (Arnoldi)** や **ランチョス法 (Lanczos)** が用いられます。

- 正方行列が線形独立な固有ベクトルを十分に持っている場合、**対角化 (diagonalisation)** が可能です：$A = PDP^{-1}$。ここで $D$ は固有値を並べた対角行列、 $P$ は固有ベクトルを並べた行列です。

- これがなぜ有用なのでしょうか？ 対角行列の扱いは非常に簡単だからです。$A^{100}$ を計算したい場合、 $A$ を100回掛ける代わりに $PD^{100}P^{-1}$ を計算すれば済みます。対角行列の累乗は、各成分を累乗するだけなので一瞬です。これにより、重い計算が非常に軽くなります。

- **固有基底 (eigenbasis)** とは、固有ベクトルのみで構成される基底のことです。この基底においては行列は対角行列となり、変換は各固有ベクトルの方向に沿った独立したスケーリングにすぎなくなります。これは、その変換にとって最も「自然な」座標系を見つけるようなものです。

- **QR分解 (QR decomposition)** は、任意の行列 $A$ を $A = QR$ と分解します。ここで $Q$ は直交行列（列ベクトルが正規直交）、 $R$ は上三角行列です。これは、情報の「方向」($Q$) と、「スケーリングおよび混合」($R$) を分離していると考えることができます。

- **グラム・シュミットの正規直交化法 (Gram-Schmidt process)** は、 $Q$ を一列ずつ構築します。 $A$ の第1列を取り、正規化します。第2列を取り、第1列との射影成分を引き（垂直にする）、正規化します。これを各列で繰り返します。結果として正規直交ベクトル集合が得られます。

- QR分解は固有値を求めるQRアルゴリズムのエンジンです。また、最小二乗法の解決にも直接使われます。 $A\mathbf{x} = \mathbf{b}$ に厳密な解がない（未知数より式が多い）場合、QR分解を使って最も近い近似解を見つけることができます。

- **SVD (特異値分解、Singular Value Decomposition)** は、最も一般的で、間違いなく最も重要な分解法です。あらゆる行列（任意の形状、任意のランク）に対し、SVDが存在します：$A = U\Sigma V^T$
  - $V^T$ ($n \times n$, 直交行列): 入力を回転させる
  - $\Sigma$ ($m \times n$, 対角行列): 直交する軸に沿ってスケーリング（特異値。非負で降順に並ぶ）
  - $U$ ($m \times m$, 直交行列): 出力を回転させる

![SVD：あらゆる変換 ＝ 回転 ＋ スケーリング ＋ 再回転](../images/svd.svg)

- 幾何学的には、SVDは、どんなに複雑な線形変換であっても、回転、各軸に沿った引き伸ばし、そして別の回転の組み合わせに過ぎないことを示しています。円は楕円になります。

- 特異値 ($\sigma_1 \geq \sigma_2 \geq \ldots$) は、各方向の「重要度」を表します。大きな特異値は、最も重要な方向に対応します。行列 $A$ のランクは、ゼロでない特異値の数に等しくなります。

- **低ランク近似 (Low-rank approximation)**: 最大の $k$ 個の特異値だけを残し、残りを 0 にすることで、 $A$ の最適なランク $k$ の近似が得られます。これが画像圧縮の仕組みです。$1000 \times 1000$ の画像でも、 $k = 50$ 程度の特異値だけでほぼ同じ見た目を維持でき、20倍の圧縮になります。

- SVDは擬似逆行列も提供します：$A^+ = V\Sigma^+U^T$。ここで $\Sigma^+$ は非ゼロの特異値を逆数にしたものです。

- 固有値分解は正方行列にしか使えませんが、SVDはあらゆる行列に適用可能です。これが最大の利点です。

- **PCA (主成分分析、Principal Component Analysis)** は、次元削減のために固有値分解（またはSVD）を利用します。

- 1サンプルあたり100個の特徴量を持つデータセットがあるとします（100次元のベクトルが行列に積み重なっている状態）。それらの特徴量の多くは互いに相関しており、冗長です。

- PCAは、データが実際に変化している方向を見つけ出し、重要なものだけを残せるようにします。

![PCAはデータ内の最大の分散の方向を見つける](../images/pca.svg)

- 第1主成分 (PC1) は、分散が最大となる方向です。

- 第2主成分 (PC2) は、残りの分散を最も多く捉える方向であり、第1主成分と垂直です。

- 分散の大部分が数個の方向に集中しているなら、データをそれらの次元に投影し、残りを捨てることで、情報の損失を最小限に抑えつつデータを軽量化できます。

- 手順：
  - データの標準化（平均をゼロにし、標準偏差で割る）。すべての特徴量が対等に寄与するようにします。
  - 共分散行列を計算。
  - その固有値と固有ベクトルを求める。
  - 固有値の大きい順に $k$ 個の固有ベクトル（主成分）を選択。
  - データをこれらの成分に投影。

- 標準化は極めて重要です。これを怠ると、数値の大きい特徴量（km単位など）が、数値の小さい重要な特徴量（cm単位など）を、実際の影響度とは無関係に圧倒してしまいます。

- 実際、PCAは可視化（高次元データを2Dや3Dへ投影）、ノイズ削減（ノイズと思われる低分散の方向の除去）、MLモデルの高速化（入力特徴量の削減）などに使われます。

- **カーネルPCA** は、PCAを非線形な関係に拡張したものです。カーネル関数を用いてデータを高次元空間へ写し、そこでの構造を線形にしてから標準的なPCAを適用し、再度投影します。

- **シュール分解 (Schur decomposition)** は、正方行列を $A = QTQ^\ast$ と分解します。ここで $Q$ はユニタリ行列、 $T$ は上三角行列です。すべての正方行列には、たとえ対角化不可能であっても、シュール分解が存在します。

- **非負値行列因子分解 (NMF, Non-negative Matrix Factorisation)** は、行列を2つの非負の行列に分解します：$A \approx WH$ （ $W, H$ の全成分が $\geq 0$ ）。負の値を生成しうるSVDとは異なり、NMFは加算のみを行い、減算を行いません。これにより、部品の解釈が容易になります。例えばトピックモデリングにおいて、 $W$ は文書ごとのトピックの重みを、 $H$ はトピックごとの単語の重みを表し、すべて非負であるため、「その文書にどのトピックがどれくらい含まれるか」という人間の直感に合致した結果が得られます。

- **スペクトル定理 (spectral theorem)** によれば、対称行列（エルミート行列）は常に直交行列（ユニタリ行列）で対角化可能です。その固有値は常に実数であり、固有ベクトルは常に直交します。これがPCAの理論的支柱となっています。

## コーディング課題 (CoLab または notebook を使用)

1. 対称行列の固有値と固有ベクトルを計算してください。固有ベクトルが互いに垂直であることを確認し、固有値分解から元の行列を再構成してください。

```python {cmd=true}
import jax.numpy as jnp

A = jnp.array([[4.0, 2.0],
               [2.0, 3.0]])

eigenvalues, eigenvectors = jnp.linalg.eigh(A)
print(f"Eigenvalues: {eigenvalues}")
print(f"Eigenvectors orthogonal: {jnp.dot(eigenvectors[:,0], eigenvectors[:,1]):.6f}")

# 再構成: A = P D P^T
D = jnp.diag(eigenvalues)
A_reconstructed = eigenvectors @ D @ eigenvectors.T
print(f"Reconstruction matches: {jnp.allclose(A, A_reconstructed)}")
```

2. べき乗法を実装して最大固有値を、逆反復法を実装して最小固有値を見つけてください。`jnp.linalg.eigh` の結果と比較してください。その後、QRアルゴリズムを自分で実装してみてください。

```python {cmd=true}
import jax.numpy as jnp

A = jnp.array([[4.0, 2.0],
               [2.0, 3.0]])

# べき乗法: 最大固有値を見つける
v = jnp.array([1.0, 0.0])
for _ in range(20):
    v = A @ v
    v = v / jnp.linalg.norm(v)
print(f"Largest eigenvalue:  {v @ A @ v:.4f}")

# 逆反復法: A^{-1} を掛けることで、最小固有値を見つける
v = jnp.array([1.0, 0.0])
for _ in range(20):
    v = jnp.linalg.solve(A, v)
    v = v / jnp.linalg.norm(v)
print(f"Smallest eigenvalue: {1.0 / (v @ jnp.linalg.solve(A, v)):.4f}")

print(f"jnp.linalg.eigh:    {jnp.linalg.eigh(A)[0]}")
```

3. 行列のSVDを計算し、上位 $k$ 個の特異値のみを使って再構成してください。 $k$ の値によって近似の品質がどのように変化するか観察してください。

```python {cmd=true}
import jax.numpy as jnp

A = jnp.array([[1.0, 2.0, 3.0],
               [4.0, 5.0, 6.0],
               [7.0, 8.0, 9.0]])

U, S, Vt = jnp.linalg.svd(A)

for k in [1, 2, 3]:
    approx = U[:, :k] @ jnp.diag(S[:k]) @ Vt[:k, :]
    error = jnp.linalg.norm(A - approx)
    print(f"k={k}, reconstruction error: {error:.4f}")
```
