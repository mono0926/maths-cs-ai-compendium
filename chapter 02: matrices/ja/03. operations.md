# 行列の演算 (Matrix Operations)

- 行列は、ベクトルと同じように加算やスケーリング（定数倍）が可能です。

- 加算を行うには、両方の行列の次元が一致している必要があります。要素ごとに足し合わせます：

```math
\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} + \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} = \begin{bmatrix} 6 & 8 \\ 10 & 12 \end{bmatrix}
```

- スカラー乗算（定数倍）では、すべての要素にスカラーを掛けます：

```math
3 \times \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} = \begin{bmatrix} 3 & 6 \\ 9 & 12 \end{bmatrix}
```

- 行列に対して行える最も単純な操作の一つは、ベクトルとの積です。**行列とベクトルの積** $A\mathbf{x}$ は、$\mathbf{x}$ の各成分を重みとして、$A$ の列ベクトルを線形結合したものです：

```math
\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \begin{bmatrix} 5 \\ 6 \end{bmatrix} = 5 \begin{bmatrix} 1 \\ 3 \end{bmatrix} + 6 \begin{bmatrix} 2 \\ 4 \end{bmatrix} = \begin{bmatrix} 17 \\ 39 \end{bmatrix}
```

- これは機械学習 (ML) における核心的な演算です。ニューラルネットワークの各層は $A\mathbf{x} + \mathbf{b}$、つまり「行列 $\times$ 入力ベクトル $+$ バイアス」を計算しています。

- より一般的なケースが **行列の積 (matrix multiplication)** です。$A$ ($m \times n$) と $B$ ($n \times p$) が与えられたとき、その積 $C = AB$ は $m \times p$ 行列となり、各要素はドット積（内積）として求められます：

$$C_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj}$$

- 結果の各成分は、$A$ の行と $B$ の列のドット積です。内側の次元が一致（ここでは $n$）している必要があり、結果は外側の次元（$m \times p$）になります。

- 別の見方をすれば、結果の各列は $A$ の列の **重み付き和** であり、その重みは $B$ の対応する列から供給されます。

- $B$ の列が $[2, 3]^T$ であれば、結果の対応する列は $2 \times (A \text{ の第1列}) + 3 \times (A \text{ の第2列})$ となります。

- 有用な特殊例として、行列とその転置行列の積は常に正方行列になります。$AA^T$ は $m \times m$ 行列、$A^TA$ は $n \times n$ 行列になります：

```math
\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix} = \begin{bmatrix} 14 & 32 \\ 32 & 77 \end{bmatrix}
```

- 行列の積には、重要なルールがあります：
  - **非可換 (Not commutative)**: 一般に $AB \neq BA$ です。掛ける順番が重要です。

  - **結合法則 (Associative)**: $(AB)C = A(BC)$。掛け算のグループ化は自由に行えます。

  - **分配法則 (Distributive)**: $A(B + C) = AB + AC$。

  - **単位行列 (Identity)**: $AI = IA = A$。

- **アダマール積 (Hadamard product)**（要素ごとの積）は、同じサイズの2つの行列の対応する要素どうしを掛け合わせる演算で、$A \odot B$ と表記されます：

```math
\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \odot \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} = \begin{bmatrix} 5 & 12 \\ 21 & 32 \end{bmatrix}
```

- 標準的な行列の積とは異なり、アダマール積は可換 ($A \odot B = B \odot A$) であり、両方の行列が同じ次元である必要があります。MLでは「ゲート (gating)」処理に多用されます。0から1の間の値を持つマスクを要素ごとに掛けることで、各成分がどれくらい「通過する」かを制御します。

- 2つのベクトルの **外積 (outer product)** $\mathbf{u}$ と $\mathbf{v}$ は、行列を生成します：$\mathbf{u}\mathbf{v}^T$。各要素は $\mathbf{u}$ の一つの成分と $\mathbf{v}$ の一つの成分の積になります：

```math
\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} \begin{bmatrix} 4 & 5 \end{bmatrix} = \begin{bmatrix} 4 & 5 \\ 8 & 10 \\ 12 & 15 \end{bmatrix}
```

- すべての行は $\mathbf{v}^T$ をスケーリングしたものであるため、結果は常にランク 1 になります。任意の行列はランク 1 の外積の和として記述でき、これはまさに SVD（特異値分解、分解の章で詳述）が行っていることです。

- 行列の積は計算負荷が高い操作です。2つの $n \times n$ 行列を掛けるには $O(n^3)$ の演算が必要です。$1000 \times 1000$ の行列の場合、10億回の掛け算が行われます。

- 行列が **疎 (sparse)**（ほとんどが 0）である場合、愚直な掛け算は 0 を掛けることに時間を浪費します。**CSR (Compressed Sparse Row)** 形式などは、非ゼロ要素とその位置のみを保存します：
  - **Values**: 非ゼロ要素を行の順に格納
  - **Column indices**: 各要素がどの列に属するか
  - **Row offsets**: 各行が values リストのどこから始まるか

- 例えば、以下の行列：

```math
A = \begin{bmatrix} 5 & 0 & 0 & 2 \\ 0 & 0 & 3 & 0 \\ 0 & 0 & 0 & -1 \end{bmatrix}
```

- は次のように保存されます：values = [5, 2, 3, -1], columns = [0, 3, 2, 3], row offsets = [0, 2, 3, 4]。これにより 0 の計算をスキップでき、疎行列の演算が大幅に高速化されます。

- 行列の主要な用途の一つは、**連立一次方程式 (systems of linear equations)** を解くことです。システム $A\mathbf{x} = \mathbf{b}$ は、「$A$ によって変換されたときに $\mathbf{b}$ になるようなベクトル $\mathbf{x}$ は何か？」という問いを投げかけています。

- 例えば、果物を買っているとします。リンゴ1個 $x_1$ ドル、バナナ1個 $x_2$ ドルです。リンゴ2個とバナナ1個で 5ドル、リンゴ1個とバナナ3個で 10ドルだとわかっている場合、行列形式では次のようになります：

```math
\begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 5 \\ 10 \end{bmatrix}
```

- 行列とベクトルの積を行ごとに計算する（各行と $[x_1, x_2]^T$ のドット積）と、2つの式が得られます：

$$2x_1 + 1x_2 = 5 \qquad \text{(行 1)} \qquad \qquad x_1 + 3x_2 = 10 \qquad \text{(行 2)}$$

- 行 1 から $x_2 = 5 - 2x_1$。これを行 2 に代入すると：$x_1 + 3(5 - 2x_1) = 10$ となり、$x_1 = 1$, 続いて $x_2 = 3$ が得られます。リンゴは 1ドル、バナナは 3ドルです。

- 検算してみましょう。正しく一致します：

```math
\begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} 1 \\ 3 \end{bmatrix} = \begin{bmatrix} 2 + 3 \\ 1 + 9 \end{bmatrix} = \begin{bmatrix} 5 \\ 10 \end{bmatrix}
```

- $A$ が逆行列を持つ場合、解は単純に $\mathbf{x} = A^{-1}\mathbf{b}$ となります。しかし、逆行列を直接計算することはコストが高く、数値的に不安定になることがあります。実際には、代わりに（後述の）分解法が使われます。

- すべての行列が正方行列であるわけではなく、すべての正方行列が逆行列を持つわけでもありません。**擬似逆行列 (pseudo-inverse)** $A^+$ は、逆行列の概念を任意の行列へと一般化したものです。これは常に存在し、「最善の」逆行列を提供します：

$$A^+ = (A^TA)^{-1}A^T$$

- $A$ が下三角行列である場合、$L\mathbf{x} = \mathbf{b}$ を解くのは **前進代入 (forward substitution)** によって簡単に行えます。まず $x_1$ を解き、それを使って $x_2$ を求め、順次下へと進めていきます。

- $A$ が上三角行列である場合、$U\mathbf{x} = \mathbf{b}$ の解法は **後退代入 (back substitution)** となります。最後の変数から順に解き、上へと戻っていきます。

- これが、行列を（分解の章で見るように）三角行列の因子に分解することが極めて有用である理由です。難しい一つの問題を、二つの簡単な問題へと変えることができるのです。

## コーディング課題 (CoLab または notebook を使用)

1. 2つの行列を掛け合わせ、その次元を確認してください。その後、掛ける順序を入れ替えて結果が変化すること（あるいは次元が一致せずエラーになること）を観察してください。

```python {cmd=true}
import jax.numpy as jnp

A = jnp.array([[1.0, 2.0],
               [3.0, 4.0]])
B = jnp.array([[5.0, 6.0],
               [7.0, 8.0]])

print(f"A @ B:\n{A @ B}")
print(f"B @ A:\n{B @ A}")
print(f"Equal: {jnp.allclose(A @ B, B @ A)}")
```

2. 連立一次方程式 $A\mathbf{x} = \mathbf{b}$ を解き、元の行列との積を求めて解を検証してください。$\mathbf{b}$ を変更して、解がどのように変化するか試してみてください。

```python {cmd=true}
import jax.numpy as jnp

A = jnp.array([[2.0, 1.0],
               [5.0, 3.0]])
b = jnp.array([4.0, 7.0])

x = jnp.linalg.solve(A, b)
print(f"Solution x: {x}")
print(f"A @ x: {A @ x}")
```
